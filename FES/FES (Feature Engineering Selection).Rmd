---
title: "Feature Engineering and Selection: A Practical Approach for Predictive Models"
author: "Koji Mizumura"
date: "October 29th,2018 - `r Sys.Date()`"
always_allow_html: yes
output:
  html_document:
    df_print: paged
    toc: yes
    toc_float: yes
    number_sections: yes
    theme: "readable"
  pdf_document:
    toc: yes
  word_document:
    toc: yes
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  fig.align = "center",
  message = FALSE,
  warning = FALSE,
  collapse = TRUE
)
```

# Preface

A note to readers: this text is a work in progress. It will eventually be published in this format as well as a more traditional physical medium by Chapman & Hall/CRC.

Chapters 1 through 8 have been completed (pending more comments) and the rest are being constructed as you read this.

We‚Äôve released this initial version to get more feedback beyond what our excellent reviewers and editor have already provided. Feedback can be given at the GitHub repo https://github.com/topepo/FES/issues. Copyediting has not been done yet so read at your own risk. Right now, we are primarily interested in the quality and organization of the content but are open to all of your thoughts.

Code and data will be provided but not until everything has been finalized. That might be frustrating but we‚Äôd rather wait.

Thanks for taking the time to read this.

Changes since the 2018-05-24 Release

- Numerous typos were fixed thanks to everyone who contributed to the GitHub repo.
- A small section on validation sets was added (Section 3.4.5)
- A section on supervised encodings for categorical predictors was added (Section 5.4)
- The analysis of OkCupid text data was refactored and the analysis changed slightly. This changes the results of the sections that use the OkC models (3.6, 3.7, and 5.6).
- A different package for non-negative matrix factorization was used so the results are somewhat different in Section 6.3.
- Chapters on interaction effects and missing data were added.
- The order of the chapters on missing data and profile data were switched.

(back to the book)

The goal of our previous work, Applied Predictive Modeling, was to elucidate a framework for constructing models that generate accurate predictions for future, yet-to-be-seen data. This framework includes pre-processing the data, splitting the data into training and testing sets, selecting an approach for identifying optimal tuning parameters, building models, and estimating predictive performance. This approach protects from overfitting to the training data and helps models to identify truly predictive patterns that are generalizable to future data, thus enabling good predictions for that data. Authors and modelers have successfully used this framework to derive models that have won Kaggle competitions (Raimondi 2010), have been implemented in diagnostic tools (Jahani and Mahdavi 2016,Luo (2016)), are being used as the backbone of investment algorithms (Stankoviƒ?, Markoviƒ?, and Stojanoviƒ? 2015), and are being used as a screening tool to assess the safety of new pharmaceutical products (Thomson et al. 2011).

In addition to having a good approach to the modeling process, building an effective predictive model requires other good practices. These practices include garnering expert knowledge about the process being modeled, collecting the appropriate data to answer the desired question, understanding the inherent variation in the response and taking steps, if possible, to minimize this variation, ensuring that the predictors collected are relevant for the problem, and utilizing a range of model types to have the best chance of uncovering relationships among the predictors and the response.

Despite our attempts to follow these good practices, we are sometimes frustrated to find that the best models have less-than-anticipated, below useful predictive performance. This lack of performance may be due to a simple to explain, but difficult to pinpoint, cause: relevant predictors that were collected are represented in a way that models have trouble achieving good performance. Key relationships that are not directly available as predictors may be between the response and:

- a transformation of a predictor,
- an interaction of two or more predictors such as a product or ratio,
- a functional relationship among predictors, or
- an equivalent re-representation of a predictor.

Adjusting and reworking the predictors to enable models to better uncover predictor-response relationships has been termed feature engineering. The engineering connotation implies that we know the steps to take to fix poor performance and to guide predictive improvement. However, we often do not know the best re-representation of the predictors to improve model performance. Instead, the re-working of predictors is more of an art, requiring the right tools and experience to find better predictor representations. Moreover, we may need to search many alternative predictor representations to improve model performance. This process, too, can lead to overfitting due to the vast number of alternative predictor representations. So appropriate care must be taken to avoid overfitting during the predictor creation process.

The goals of Feature Engineering and Selection are to provide tools for re-representing predictors, to place these tools in the context of a good predictive modeling framework, and to convey our experience of utilizing these tools in practice. In the end, we hope that these tools and our experience will help you generate better models. When we started writing this book, we could not find any comprehensive references that described and illustrated the types of tactics and strategies that can be used to improve models by focusing on the predictor representations (that were not solely focused on images and text).

Like in Applied Predictive Modeling, we have used R as the computational engine for this text. There are a few reasons for that. First, while not the only good option, R has been shown to be popular and effective in modern data analysis. Second, R is free and open-source. You can install it anywhere, modify the code, and have the ability to see exactly how computations are performed. Third, it has excellent community support via the canonical R mailing lists and, more importantly, with StackOverflow1 and RStudio Community2. Anyone who asks a reasonable, reproducible question has a pretty good chance of getting an answer.

Also, as in our previous effort, it is critically important to us that all the software and data are freely available. This allows everyone to reproduce our work, find bugs/errors, and to even extend our approaches. The data sets and R code are available in the GitHub repository https://github.com/topepo/FES3.

¬© 2018 by Taylor & Francis Group, LLC. Except as permitted under U.S. copyright law, no part of this book may be reprinted, reproduced, transmitted, or utilized in any form by an electronic, mechanical, or other means, now known or hereafter invented, including photocopying, microfilming, and recording, or in any information storage or retrieval system, without written permission from the publishers.

## References
Raimondi, C. 2010. ‚ÄúHow I Won the Predict Hiv Progression Data Mining Competition.‚Ä? http://blog.kaggle.com/2010/08/09/how-i-won-the-hiv-progression-prediction-data-mining-competition/.

Jahani, M, and M Mahdavi. 2016. ‚ÄúComparison of Predictive Models for the Early Diagnosis of Diabetes.‚Ä? Healthcare Informatics Research 22 (2):95‚Ä?100.

Luo, G. 2016. ‚ÄúAutomatically Explaining Machine Learning Prediction Results: A Demonstration on Type 2 Diabetes Risk Prediction.‚Ä? Health Information Science and Systems 4 (1):2.

Stankoviƒ?, J, I Markoviƒ?, and M Stojanoviƒ?. 2015. ‚ÄúInvestment Strategy Optimization Using Technical Analysis and Predictive Modeling in Emerging Markets.‚Ä? Procedia Economics and Finance 19:51‚Ä?62.

Thomson, J, Johnson K, Chapin R, Stedman D, Kumpf S, and Ozolin≈° T. 2011. ‚ÄúNot a Walk in the Park: The Ecvam Whole Embryo Culture Model Challenged with Pharmaceuticals and Attempted Improvements with Random Forest Design.‚Ä? Birth Defects Research Part B: Developmental and Reproductive Toxicology 92 (2):111‚Ä?21.

# Introduction

Statistical models have gained importance as they have become ubiquitous in modern society. They enable us by generating various types of predictions in our daily lives. For example, doctors rely on general rules derived from models that tell them which specific cohorts of patients have an increased risk of a particular ailment or event. A numeric prediction of a flight‚Äôs arrival time can help understand if our airplane is likely to be delayed. In other cases, models are effective at telling us what is important or concrete. For example, a lawyer might utilize a statistical model to quantify the likelihood that potential hiring bias is occurring by chance or whether it is likely to be a systematic problem.

In each of these cases, models are created by taking existing data and finding a mathematical representation that has acceptable fidelity to the data. From such a model,  important statistics can be estimated. In the case of airline delays, a prediction of the outcome (arrival time) is the quantity of interest. While the estimate of a possible hiring bias might be revealed through a specific model parameter. In the latter case, the hiring bias estimate is usually compared to the estimated uncertainty (i.e. noise) in the data and a determination is made based on how uncommon such a result would be relative to the noise - a concept usually referred to as ‚Äústatistical significance.‚Ä? This type of model is generally thought of as being _inferential_: a conclusion is reached for the purpose of understanding the state of nature. In contrast, the prediction of a particular value (such as arrival time) reflects an _estimation problem_ where our goal is not necessarily to understand if a trend or fact is genuine but is focused on having the most accurate determination of that value. The uncertainty in the prediction is another important quantity, especially to gauge the trustworthiness of the value generated by the model.

Whether the model will be used for inference or estimation (or in rare occasions, both), there are important characteristics to consider. _Parsimony_ is a key consideration. Simple models are generally preferable to compex models, especially when inference is the goal. For example, it is easier to specify realistic distributional assumption in models with fewer parameters. Parsimony also leads to a higher capacity to interpret a model. For example, an economist might be interested in quantifying the benefit of postgraduate education on salaries. A simple model might represent this relationship between years of education and job salary linearly. This parameterization would easily facilitate statistical inferences on the potential benefit of such education. But suppose that the relationship differs substantially between occupations and/or is not linear. A more complex model would do a better job at capturing the data patterns but would be much less interpretable.

The problem, however, is that __accuracy should not be seriously sacrificed for the sake of simplicity__. A simple model might be easy to interpret but would not succeed if it does not maintain acceptable level of faithfulness to the data; if a model is only 50% accurate, should it be used to make inferences or predictions? Complexity is usually the solution to poor accuracy. By using additional parameters or by using a model that is inherently nonlinear, we might improve accuracy but interpretability will likely suffer greatly. This trade-off is a key consideration for model building.

Thus, far the discussion has been focused on aspects of the model. However, the variables that go into the model (and how they are represented) are just as critical to success. IT is impossible to talk about modeling without discussing models, but one of goal this book is to increase the emphasis on the predictors in a model.

In terms of nomeclature, the quantity is being modeled or predicted is referred to as either: the _outcome_, response or dependent variable. The variables that are used to model the outcome are called the _predictors_, _features_, or independent variables (depending on the context). For example, when modeling the sale price of a house (the outcome), the characteristics of a property (e.g., square footage, number of bedrooms and bathrooms) could be used as predictors. 

> Chart using interactive map

As one might expect, there are good and bad ways of entering predictors into a model. In many cases, there are multiple ways that an underlying piece of information can be represented or encoded. Consider a model for the sale price of a property. The location is likely to be crucia and can be represented in different ways. Figure 1.1 shows locations for properties in and around Ames Iowa, that were sold between 2006 and 2010. 

In this image, the colors represent the reported neighborhood of residence. There are 28 neighborhoods represented here and the number of properites per neighborhoods range from a single property in Landmark, to 443 in North Ames. A second representation of location in the data is longitude and latitude. A realtor might suggest using ZIP code as a predictor in the model as a proxy for school district since this can be an important consideration for buyers with children. But from an information theory point of view, longitude and latitude offer the most specificity for measuring physical location and one might make an argument that this representation has higher degree of information content (assuming that this particular information is predictive).

The idea that there are different ways to represent predictors in a model, and that some of these representations are better than others, leads to the idea of __feature engineering__ the process of creating representations of data that increase the effectiveness of a model.

Note that model effectivenss is influenced by many things. Obviously, if the predicotr has no relationship to the outcome then its representation is irrelevant. However, it is very important to realize that there are multitude of types of models and that each has its own sensitivies and needs. For example:

- Some models cannot tolerate predictors that measure the same underlying quantity (i.e., multicollinearity or correlation between predictors)
- Many models cannot use samples with any missing values.
- Some models are severely compromised when irrelevant predictors are in the data.

Feature engineering and variables selection can help mitigate many of these issues. __"The goal of this book is to help practitionerrs build better models by focusing on the predictors"__ . "Better" depends on the context of the problem, but most likely  involves the following factors: accuracy, simplicity, and robustness. To achieve these characteristics, or to make good trade-offs between them, it is critical to understand the interplay between predictors used in a model and the type of model. Accuracy and/or simplicity can sometimes be improved by representing data in ways that are more palatable to the model or by reducing the number of variables used. To demonstrate this point, a simple example with two predictors is shown in the next section. Additionally, a more substantial example is discussed in Section 1.3 that more closely resembles the modeling process in practice.

## A simple example

As a simple example of how feature engineering can affect models, consider Figure /@{} a that shows a plot of two correlated predictor variables (labeled as A and B). The data points are colored by their outcome, a discrete variable with two possible values ("PS" and "WS"). These data originate from an experiement from Hill et al([2007][https://bookdown.org/max/FES/intro-intro.html#ref-Hill]), which includes a larger predictor set. For their task, a model would require a high degree of accuracy but would not need to be used for inference. For this illustration, only these two predictors will be considered. In this figure, there is clearly a diagonal sepration between the two classes. A simple logistic regression model (Hosmer and Lemeshow [2000](https://bookdown.org/max/FES/intro-intro.html#ref-HosmerLemeshow)) will be used here to create a prediction equation from these two variables. That model uses the following equation:

$$
log(\frac{p}{1-p}) = \beta_0 + \beta_1 A + \beta_2 B
$$

where, $p$ is the probability that a sample is the "PS" class and the $\beta$ values are the model parameters that need to be estimated from the data.

```{r fig.cap="(a) An example data set and (b) the ROC curve from a simple logistic regression model.", eval=FALSE}

```

A standard procedure (maximum likelihood estimation) is used to estimate the three regression parameters from the data. The authors used 1009 data points to estimate the parameters from the data (i.e., trailing set) and reserved 1010 samples strictly for estimating performance (i.e., a test set). Using the training set, the parameter was estimated $\beta_0 = 1.73$, $\beta_1 = 0.003$, and $\beta_2 = -0.064$. 

To evaluate the model, predictions are made on the test set. Logistic regression naturally produces __class probabilities__ that give an indication of likelihood for each class. While it is common to use a 50% cutoff to make hard class predictions, the performance derived from this default might be misleading.

To avoid applying a probability cutoff, a technique called the __receiver operating characteristic (ROC)__ curve is used here. The ROC curve evaluates the results on all possible cutoffs and plots the __true positive rate__ versus the __false positive rate__. The curve for this example is shown in Fugure 1.2b. The __best possible curve__ is one that is shifted as close to as possible to the __upper left corner__ while an ineffective of a model with no predictive ability.

For the current logistic regression model, the area under the ROC curve is $0.794$ which indicates moderate accuracy in classifying the resnpose.

Given these two predictors variables, it would make sense to try different _transformations_ and _encodings_ of these data in an attempt to increase the area under the ROC curve. Since the predictors are both greater than zero, and appear to have right-skewed distributions, one might be inclinedto take the ratio A/B and enter only this term in the model.

Alternatively, we could also evaluate if simple transformations of each predictor would be helpful. One method is the __Box-Cox transformation__ which uses a separate estimation procedure priot to the logistic regression model that can put the predictors on a new scale. Using this methodology, the Box-Cox estimation procedure recommendedthat both predictors should be used on the inverse scale (i.e., `1/A` instead of `A`). This representation of the data is shown in Figure 1.3a. When these transformed values were entered into the logistic regression model in lieu of the original values, the area under the ROC curve changed from $0.794$ to $0.848$, which is a substantial increase. 

Figure 1.3b shows both curves. In this case, the ROC curve corresponding to the transformed predictors is uniformuly bettern than the original result.

> Fig 1.3 

```{r fig.cap="(a) The transformed predictors and (b) the ROC curve from both logistic regression models"}

```


This example demonstrates how an alternation of the predicots, in this case a simple transformation, can lead to improvements to the effectivenss of the model. When comparing the data in Figure `1.2a` and `1.3a`, it is easier to visually discriminate the two groups of data. In transforming the data individually, we enabled the logistic regression model to do a better job of separating the classes. Depending on the nature of the predictors, using the inverse of the original data might make __inferential analysis__ more difficult.

However, different models have different requirements of the data. If the skewness of the original predictors was the issue affecting the logistic regression model, other models exist that do not have the same sensitivity to this characteristic. For example, a __neural network__ can also be used to fit these data _without using the inverse transformation of the predictors_. This model was able to achieve an area under the ROC curve of 0.844 which is roughly equivalent to the improved logistic model results. 

One might conclude that the neural network model is inherently always better than logistic regression since __neural network was not susceptible to the distributional aspects of the predictors__. But we should not jump to a blanket conclusion like this due to the ‚ÄúNo Free Lunch‚Ä? theorem (see Section 1.2.3). Additionally, the neural network model has its own drawbacks: it is __completely uninterpretable__ and requires extensive parameter tuning to achieve good results. Depending on how the model will be utilized, one of these models might be more favorable than the other for these data.


## Important concepts

Before proceeding to specific strategies and methods there are some key concepts that should be discussed. These concepts involve theoretical aspects of modeling as well as the practice of creating a model. A number of these aspects are discussed here and additional details are provided in later chapters and references are given throughout this work.

### Overfitting

Overfitting is the situation where a model fits very well to the current data but fails when predicing new samples. It typically occurs when the model has relied too heavily on patterns and trends in the current data set that do not occur otherwise. since the model only has access to the current data set, it has no ability to understand that such patterns anomalous. For example, in the housing data shown in Figure 1.1, one could determine that properties that had square footage between $1267.5$ and $1277$ and contained three bedrooms could have their sale prices predicted within $1207$ of the true values. 

However, the accuracy on other houses (not in this dataset) that satisfy these conditions would be much worse. This is an example of a trend that does not generalize to new data.

Often, models that are very flexible (called "low bias model" in Section 1.2.5) have a highler likelihood of overfitting the data. It is not diffcult for these models to do extremely well on the data set used to create the model and without some preventative mechanism, can easily fail to generalize to new data. As will be seen in the coming chapters, especially Section 3.5. 

While models can overfit to the _data points_, such as with the housing data shown above, feature selection techniques can overfit to the _predictors_. This occurs when a variable appears relevant in the current data set but shows no real relationship with the outcome once new data are collected. The risk of this type of overfitting is especially dangerous when the number of data points is small and the number of potential predictors is very large. As with overfitting to the data points, this problem can be mitigated using a methodology that will show a warning when this is occuring.

### Supervised and unsupervised procedures

_Supervised_ data analysis involves identifying patterns between predictors and an identified _outcome_ to be modeled or predicted, while unsupervised techniques are focused solely on identifying patterns among the predictors.

Both types of analyses would tyipically involve some amount of exploration. Exploratory data analysis (EDA) ([Turkey 1977](https://bookdown.org/max/FES/intro-intro.html#ref-tukey1977exploratory)) is used to understand the major characterics of the predictors and outcome so that any particular challgens associated with the data can be discovered prior to modeling. This can iclude investigations of correlation structures in the variables, patterns of missing data, and/or anomalous motifs in the data that might challenge the initial expectations of the modeler.

Obviously, predictive models are strictly supervised since there is a direct focus on finding relationships between the predictors and the outcome. Unsupervised analysis include methods such as _cluster analysis_, _principal component analysis_, and similar tools for discovering patterns in data.

Both supervised and unsupervised analysis are susceptible to overfitting but supervised are particularly inclined to discovering erroneous patterns in the data for predicting the outcome.

In short, we can use these techniques to create a _self fulfilling predictive prophecy_. For example, it is not uncommon for an analyst to conduct a supervised analysis of data to detect which predictors are isgnificantly associated with the outcome. These predicotrs are then used in a visualization (such as a heat map or cluster analysis) on the same data but with only significant predictors. Not surprisingly, the visualization reliably demonstrates that there are clear patterns between the outcomes and predictors and appears to provide evidence of their importance. However, since the same data are shown, the visualization is essentially _cherry pciking_ the results that are only true for this data and which are unlikely to generalize to new data.

### No free lunch
The ‚ÄúNo Free Lunch‚Ä? Theorem ([Wolpert 1996](https://bookdown.org/max/FES/intro-intro.html#ref-wolpert1996lack)) is the idea that, without any specific knowledge of the problem or data at hand, no one predictive model can be said to be the best. There are many models that are optimized for some data characteristics (such as missing values or collinear predictors). In these situations, it might be reasonable to assume that they would do better than other models (all other things being equal). In practice, things are not so simple. One model that is optimized for collinear predictors might be constrained to model linear trends in the data and is sensitive to missingness in the data. It is very difficult to predict the best model especially before the data are in hand.

There have been experiments to judge which models tend to do better than others on average, notably [Demsar (2006)] (https://bookdown.org/max/FES/intro-intro.html#ref-demvsar2006statistical) and [Fernandez-Delgado et al. (2014)](https://bookdown.org/max/FES/intro-intro.html#ref-fernandez2014we). These analyses show that some models have a tendency to produce the most accurate models but the rate of ‚Äúwinning‚Ä? is not high enough to enact a strategy of ‚Äúalways use model X.‚Ä?

In practice, __it is wise to try a number of disparate types of models to probe which ones will work well with your particular data set__.

### The model versus the modeling process {3-6}

The process of developing an effective model is both iterative and heuristic. IT is difficult to know the needs of any data set prior to working with it and it is common for many approaches to be evaluated and modified before a model can be finalized. Many books and resources solely focus on the modeling technique but this activity is often a small part of the overall process. 

Figure 1.4 shows and illusion of the overall process for creating a model for a typical problem.

```{r intro-process-1plot, echo=FALSE, fig.cap="Figure 1.4: A schematic for the typical modeling process"}
knitr::include_graphics("images/intro-process-1.png")
```

The initial activity begins at maker _(a)_ where exploratory data analysis is used to investigate the data. After initial explorations, marker _(b)_ indicates where early data analysis might take place. This could include evaluating simple summary measures or identifying predictors that have strong correlations with the outcome. The process might iterate between visualization and analysis until the modeler feels confident that the data are well understood. At milestone _(c)_, the first draft for how the predictors will be represented in the models is created based on the previous analysis.

At this point, several different modeling methods might be evaluated with the initial feature set. However, many models can contain __hyperparameters__ that require tuning. This is represented at marker _(d)_ where four clusters of models are shown as thin red marks. This represents four distinct models that are being evaluated but each one is evaluated multile times over a set of candidate hyperparameter values. 

This model tuning process is discussed in Section [3.6]() and is illustrated several times in later chapters. Once the four models have been tuned, they are numerically evaluated on the data to understand their performance characteristics _(e)_. Summary measures for each model, such as model accuracy, are used to understand the level of difficulty for the problem and to determine which models appear to best suit the data. Based on these results, more EDA can be conducted on the model results _(f)_, such as residual analysis. For the previous example of predicting the sale prices of houses, the properties that are poorly predicted can be examined to understand if there is any systematic issues with the model. As an example, there may be particular ZIP codes that are difficult to accurately assess. Consequently, another round of feature engineering _(g)_ might be used to compensate for these obstacles. By this point, it may be apparent which models tend to work best for the problem at hand and another, more extensive, round of model tuning can be conducted on fewer models _(h)_. After more tuning and modification of the predictor representation, the two candidate models (#2 and #4) have been finalized. These models can be evaluated on an external test set as a final ‚Äúbake off‚Ä? between the models _(i)_. The final model is then chosen _(j)_ and this fitted model will be used going forward to predict new samples or to make inferences.

The point of this 

### Model Bias and Variance

_Variance_ is a well understood concept. When measured in regard to data, it describes the degree in which the values can fluctuate. If the same object is measured multiple times, the observed measurements will be different to some degree. In statistics, _bias_ is generally thought of as the degree in which something deviates from its true underlying value. 

For example, when trying to estimate public opinion on a topic, a poll could be systematically biased if the oeiole surveyed over-represent a particular demographics. The bias would occur as a result of the poll incorrectly estimating the desired target.

Models can also be evaluated in terms of variance and bias. A model has high variance if small changes to the underlying data used to estimate the parameters cause a sizable change in those parameters (or in the structure of the model). For example, the sample mean of a set of data points has higher variance than the sample median. The latter uses only the values in the center of the data distribution and, for this reason, it is insensitive to moderate changes in the values. A few examples of models with low variance are linear regression, logistic regression, and partial least squares. High variance models include those that use individual data points to define their parameters such as classification or regression trees, nearest neighbor models, and neural networks. To contrast low variance and high variance models, consider linear regression and, alternatively, nearest neighbor models. Linear regression uses all of the data to estimate slope parameters and, while it can be sensitive to outliers, it is much less sensitive than a nearest neighbor model.

_Model bias_ reflects the ability of a model to confirm to the underlying theoretical structure of the data. A _low bias_ model is one that can be _highly flexibile_ and has the capacity to fit a variety of different shapes and patterns. A _high bias_ model would be unable to estimate values close to their true theoretical counterparts. Linear methods often have high bias since, without modification, cannot describe non-linear patterns in the predictor variables. __Tree-based models__, support vector machines, neural networks, and others can be very adaptable to the data and have __low bias__.

As one might expect, model bias and variance can often be in opposition to one another; in order to achieve low bias, models tend to demonstrate high variance (and vice versa).  The _variance-bias trade-off_ is a common theme in statistics. 

In many cases, models have parameters that control the flexibility of the model and thus affect the variance and bias properties of the results. Consider a simple sequence of data points such as a daily stock price. A moving average model would estimate the stock price on a given day by the average of the data points within a certain window of the day. The size of the window can modulate the variance and bias here. For a small window, the average is much more responsive to the data and has a high potential to match the underlying trend. However, it also inherits a high degree of sensitivity to those data in the window and this increases variance. Widening the window will average more points and will reduce the variance in the model but will also desensitize the model fit potential by risking over-smoothing the data (and thus increasing bias).

Consider the example in Figure 1.5a that contains a single predictor and outcome where their relationship is nonlinear. The right-hand panel (b) shows two model fits. First, a simple three-point moving average is used (in green). This trend line is bumpy but does a good jon of tracking the non-linear trend in the data. The purple line shows the results of a standard linear regression model that includes a term for the predictor value and a term for the square of the predictor value. Linear regression is a linear in the _model parameters_ and adding polynomial terms to the model can be effective way of allowing the model to identify nonlinear patterns. Since the data points start low on the y-axis, reach an apex near a predictor value of 0.3 then decrease, a quadratic regression model would be a reasonable first attempt at modeling these data. This model is very smooth (showing low variance) but does not do a very good job of fitting the nonlinear trend seen in the data (i.e., high bias)

> Figure 1.5

```{r echo=FALSE, fig.cap="Fig.1.5: A simulated data set and model fits for a 3-point moving average (green) and quadratic regression (purple)"}

library(tidyverse)

x <- seq(-1, 1, by=0.01)
y <- -(x-0.5)^2+2+rnorm(x, mean = 0.5, sd=0.05)
data <- bind_cols(x = x,
                  y = y)
  
ggplot(data, aes(x,y))+
  geom_point()+
  geom_smooth(method = "loess")
```

To accentuate this point further, the original data were "jittered" multiple times by adding small amounts of random noise to their values. this was done twenty times, and for each version of the data, the same two models were fit to the jittered data. The fitter curve are shown in Figure 1.6. The moving average shows a significant degree of noise in the regression predictions but, on average, manages to track the data patterns well. The quadratic model was not confused by the extra noise and generated very similar (although inaccurate) model fits.

The notions of model bias and variance are central to the ideas in this text. As previously described, simplicity is an important characteristic of a model. One method of creating a _low variance_, _low bias_ model is to augment a low variance model with appropriate representations of the data to decrease the bias. The previous example in Section 1.1 is a simple example of this process; a logistic regression (high bias, low variance) was improved by modifying the predictor variables and was able to show results on par with a neural networks model (low bias). As another example, the data in Figure 1.5a were generated using the following equation

$$
y = x^3 + [\beta_1 \ exp(\beta_2(x-\beta_3)^2)] + \epsilon
$$

Theoretically, if this function form could be  determined from the data, then the best possible model would be a nonlinear regression model (low variance, low bias). We revisit the variance-bias relationship in Section [3.4]() in the context of measuring performance using resampling.

In a similar manner, models can have reduced performance due to irrelevant predictors causing excess model variation. Feature selection techniques improve models by reducing the unwanted noise of extra variables. 

> Fig 1.6

```{r fig.cap="Fig.1.6: Model fits for twenty jitted versions of the data set"}

```

### Experience driven modeling and empirically driven modeling

Projects may arise where no modeling has previously applied to the data. For example, suppose that a new customer database becomes available and this database contains a large number of fields that are potential predictors. Subject matter experts may have a good sense of what features should be in the model based on previous experience. This knowledge allows experts to be prescriptive about exactly which variables are to be used and how they are represented. Their reasoning should be strongly considered given their expertise. However, since the models estimate parameters from the data, there can be a strong desire to be data driven rather than _experience driven_.

Many types of models have the ability to empirically discern which predictors should be in the model and can derive the representation of the predictors that can maximize performance (based on the available data). The perceived (and often real) danger in this approach is twofold. First, as previously discussed, data driven approaches run the risk of overfitting to false patterns in the data. Second, they might yield models that are highly complex and may not have any obvious rational explanation. In the latter case a circular argument may arise where practitioners only accept models that quantify what they already know but expect better results than what a humanÅfs manual assessment can provide. For example, if an unexpected, novel predictor is found that has a strong relationship with the outcome, this may challenge the current conventional wisdom and be viewed with suspicion.

It is common to have some conflict between experience driven modeling and empirically driven modeling. Each approach has its advantages and disadvantages. In practice, we have found that a combination of the two approaches works best as long as both sides see the value in the contrasting approaches. The subject matter expert may have more confidence in a novel model feature if they feel that the methodology used to discover the feature is rigorous enough to avoid spurious results. Also an empirical modeler might find benefit in an expertÅfs recommendations to initially whittle down a large number of predictors or at least to help prioritize them in the modeling process. Also, the process of feature engineering requires some level of expertise related to what is being modeled. It is difficult to make recommendations on how predictors should be represented in a vacuum or without the knowing the context of the project. For example, in the simple example in Section 1.1, the inverse transformation for the predictors might have seemed obvious to an experienced practitioner.

### Big data

The definition of Big Data is somewhat nebulous. Typically, this term implies a large number of data points (as opposed to variables) and it is worth noting that the effective sample size might be smaller than the actual data size. For example, if there is a severe class imbalance or rare event rate, the number of events in the data might be fairly pedestrian. Click-through rate on online ads is a good example of this. Another example is when one particular region of the predictor space is abundantly sampled. Suppose a data set had billions of records but most correspond to white males within a certain age range. The number of distinct samples might be low, resulting in a data set that is not diverse.

One situation where large datasets probably doesnÅft help is when samples are added within the mainstream of the data. This simply increases the granularity of the distribution of the variables and, after a certain point, may not help in the data analysis. More rows of data can be helpful when new areas of the population are being accrued. In other words, big data does not necessarily mean better data.

While the benefits of big data have been widely espoused, there are some potential drawbacks. First, it simply might not solve problems being encountered in the analysis. Big data cannot automatically induce a relationship between the predictors and outcome when none exists. Second, there are often computational ramifications to having large amounts of data. Many high variance/low bias models tend to be very complex and computationally demanding and the time to fit these models can increase with data size and, in some cases, the increase can be nonlinear. Adding more data allows these models to more accurately reflect the complexity of the data but would require specialized solutions to be feasible. This, in itself, is not problematic unless the solutions have the effect of restricting the types of models that can be utilized. It is better for the problem to dictate the type of model that is needed.

Additionally, not all models can exploit large data volumes. For high bias, low variance models, big data tends to simply drive down the standard errors of the parameter estimates. For example, in a linear regression created on a million data records, doubling or tripling the amount of training data is unlikely to improve the parameter estimates to any practical degree (all other things being equal).

However, there are models that can effectively leverage large data sets. In some domains, there can be large amounts of unlabeled data where the outcome is unknown but the predictors have been measured or computed. The classic examples are images and text but unlabeled data can occur in other situations. For example, pharmaceutical companies have large databases of chemical compounds that have been designed but their important characteristics have not been measured (which can be expensive). Other examples include public governmental databases where there is an abundance of data that have not been connected to a specific outcome.

Unlabeled data can be used to solve some specific modeling problems. For models that require formal probability specifications, determining multivariate distributions can be extremely difficult. Copious amounts of predictors data can help estimate or specify these distributions. Autoencoders, discussed in Section 6.3.2, are models that can denoise or smooth the predictor values. The outcome is not required to create an autoencoder so unlabeled data can potentially improve the situation.

Overall, when encountering (or being offered) large amounts of data, one might think to ask:

- What are you using it for? Does it solve some unmet need?
- Will it get in the way.

## A More Complex Example

To illustrate the interplay between models and features, we present a more realistic example. The case study discussed here involves predicting the ridership on Chicago ÅgLÅh trains (i.e., the number of people entering a particular station on a daily basis). If a sufficiently predictive model can be built, the Chicago Transit Authority could use this model to appropriately staff trains and number of cars required per line. This data set is discussed in more detail in Section [4.1]() but this section describes a series of models that were evaluated when the data were originally analyzed.

```{r fig.cap="Fig.1.7 A series of model and feature combinations for modeling train ridership in Chicago"}

```

To begin, a simple set of four predictors was considered. These initial predictors, labelled as "Set 1", were developed because they are simple to calculate and visualizations showed strong relationships with ridership (the outcome). A variety of different models were evaluated and the root mean squared error (RMSE) was estimated using resampling methods.

Figure [1.7]() shows the results for several different types of models (e.g., tree-based models, linear models, etc). RMSE values for the initial feature set ranged between 2331 and 3248 daily rides11. With the same feature set, tree???based models had the best performance while linear models had the worst results. Additionally, there is very little variation in RMSE results within a model type (i.e., the linear model results tend to be similar to each other).
 
In an effort to improve model performance, some time was spent deriving a second set of predictors that might be used to augment the original group of four. From this, 128 numeric predictors were identified that were lagged versions of the ridership at different stations. For example, to predict the ridership one week in the future, todayÅfs ridership would be used as a predictor (i.e. a seven day lag). This second set of predictors had an beneficial effect overall but were especially helpful to linear models (see the x-axis value of `{1, 2}` in Figure 1.7). However, the benefit varied between models and model types.

Since the lag variables were important for predicting the outcome, more lag variables were created using lags between 8 and 14 days. Many of these variables show a strong correlation to the other predictors. However, models with predictor sets 1, 2, and 3 did not show much meaningful improvement above and beyond the previous set of models and, for some, the results were worse. One particular linear model suffered since this expanded set had a high degree of between-variable correlation. This situation is generally known as _multicollinearity_ and can be particularly troubling for some models. Because this expanded group of lagged variables didnÅft now show much benefit overall, it was not considered further.

When brainstorming which predictors could be added next, it seemed reasonable to think that weather conditions might affect ridership. To evaluate this conjecture, a fourth set of 18 predictors was calculated and used in models with the first two sets (labeled as {1, 2, 4}). Like the third set, the weather did not show any relevance to predicting train ridership.

After conducting exploratory data analysis of residual plots associated with models with sets 1 and 2, a fifth set of 49 binary predictors were developed to address days where the current best models did poorly. These predictors resulted in a substantial drop in model error and were retained. Note that the improvement affected models differently and that, with feature sets 1, 2, and 5, the simple linear models yielded results that are on par with more complex modeling techniques.

The overall points that should be understood from this demonstration are:

1. When modeling data, there is almost never a single model fit or feature set that will immediately solve the problem. The process is more likely to be a campaign of trial and error to achieve the best results.
2. The effect of feature sets can be much larger than the effect of different models.
3. The interplay between models and features is complex and somewhat unpredictable.
4. With the right set of predictors, is it common that many different types of models can achieve the same level of performance. Initially, the linear models had the worst performance but, in the end, showed some of the best performance.

Techniques for discovering, representing, adding and subtracting are discussed in subsequent chapters. 

## Feature selection

In the previous example, new sets of features were derived sequentially to compensate to improve performance of the model. These sets were developed, added to the model, and then resampling was used to evaluate their utility. The new predictors were not prospectively filtered for statistical significance prior to adding them to the model. This would be a supervised procedure and care must be taken to make sure that overfitting is not occurring.

In that example, it was demonstrated that some of the predictors have enough underlying information to adequately predict the outcome (such as sets 1, 2, and 5). However, this collection of predictors might very well contain non-informative variables and this might impact performance to some extent. To whittle the predictor set to a smaller set that contains only the informative predictors, a supervised feature selection technique might be used. Additionally, there is the possibility that there are a small number of important predictors in sets 3 and 4 whose utility was not discovered because of all of the non-informative variables in these sets.

In other cases, all of the raw predictors are known and available at the beginning of the modeling process. In this case, a less sequential approach might be used by simply using a feature selection routine to attempt to sort out the best and worst predictors.

There are a number of different strategies for supervised feature selection that can be applied and these are discussed in Chapter 11. The main distinction between the methods are how subsets are derived:

- __Wrapper method__ used an _external_ search procedure  to choose different subsets of the whole predictor set to evaluate in a model. This approach separates the feature search process from the model fitting process. Examples of this approach would be backwards or stepwise selection as well as genetic algorithms.
- __Embedded methods__  are models where the feature selection procedure occurs naturally course of the model fitting process. Here an example would be a simple decision tree where variables are selected when the model uses them in a split. If a predictor is never used in a split, the prediction equation is functionally independent of this variable and it has been selected out.

As with model fitting, the main concern during feature selection is overfitting. This is especially true when wrapper methods are used and/or if the number of data points in the training set is small relative to the number of predictors. 

Finally, _unsupervised_ selection methods can have a very positive effect on model performance. Recall the Ames housing data discussed in Section 1. A propertyÅfs neighborhood might be a useful predictor in the model. Since most models require numbers for predictors, it is common to encode such data as _dummy_ or _indicator variables_. In this case, the single neighborhood predictor, with 28 possible values, is converted to a set of 27 binary variables that have a value of one when a property is in that neighborhood and zero otherwise. While this is a well known and common approach, here it leads to cases where 2 of the neighborhoods have only one or two properties in these data, which is less than 1% of the overall set. With such a low frequency, such a predictors might have a detrimental effect on some models (such as linear regression) and removing them prior to building the model might be advisable.

When conducting a search for a subset of variables, it is important to realize that there may not be a unique set of predictors that will produce the best performance. There is often a compensatory effect where, when one seemingly important variable is removed, the model adjusts using the remaining variables. This is especially true when there is some degree of correlation between the explanatory variables or when a low bias models is used. For this reason, feature selection should not be used as a formal method of determining feature significance. More traditional inferential statistical approaches are a better solution for appraising the contribution of a predictor to the underlying model or to the data set.

## An outline of the Book

The goal of this text is to provide effective tools for uncovering relevant and predictively useful engineering of new predictors. These tools will be the bookends of the predictive modeling process. At the beginning of the process we will explore techniques for augmenting the predictor set. Then at the end of the process we will provide methods for filtering the enhanced predictor set to ultimately produce better models. These concepts will be detailed in Chapters [2](aaa) - [11](aaa) as follows.

We begin by providing a short illustration of the interplay between the modeling and feature engineering process (Chapter [2]()) In this example, we use feature engineering and feature selection methods to improve the ability of a model to predict the risk of ischemic stroke.

In Chapter 3 we will provide a review of the process for developing predictive models, which will include an illustration of the steps of data splitting, validation approach selection, model tuning, and performance estimation for future predictions. This chapter will also include guidance on how to use feedback loops when cycling through the model building process across multiple models.

Exploratory visualizations of the data are crucial for understanding relationships among predictors and between predictors and the response, especially for high-dimensional data. In addition, visualizations can be used to assist in understanding the nature of individual predictors including predictorsÅf skewness and missing data patterns. Chapter 4 will illustrate useful visualization techniques to explore relationships within and between predictors. Graphical methods for evaluating model lack-of-fit will also be presented.

Chapter [5]() will focus on approaches for encoding discrete, or categorical, predictors. Here we will summarize standard techniques for representing categorical or ordinal (ordered categorical) predictors. Feature engineering methods for categorical predictors such as feature hashing are introduced as a method for using existing information to create new predictors that better uncover meaningful relationships. This chapter will also provide guidance on practical issues such as how to handle rare levels within a categorical predictor and the impact of creating dummy variables for tree and rule based models. Date-based predictors are present in many data sets and can be viewed as categorical predictors. Methods for encoding dates will also be demonstrated.

Engineering numeric predictors will be discussed in Chapter [6](). As mentioned above, numeric predictors as collected in the original data may not be optimal for predicting the response. Univariate and multivariate transformations are a first step to finding better forms of numeric predictors. A more advanced approach is to use basis expansions (i.e. splines) to create better representations of the original predictors. In certain situations, transforming continuous predictors to categorical or ordinal bins reduces variation and helps to improve predictive model performance. Caveats to binning numerical predictors will also be provided.

Up to this point in the book, a feature has been considered as one of the observed predictors in the data. In Chapter [7]() we will illustrate that important features for a predictive model could also be the interaction between two or more of the original predictors. Quantitative tools for determining which predictors interact with one another will be explored along with graphical methods to evaluate the importance of these types of effects. This chapter will also discuss the concept of estimability of interactions.

Working with profile data, such as time series (longitudinal), cellular-to-wellular, and image data will be addressed in Chapter [9](). These kind of data are normally collected in the fields of finance, pharmaceutical, intelligence, transportation, and weather forecasting, and this particular data structure generates unique challenges to many models. Some modern predictive modeling tools such as partial least squares can naturally handle data in this format. But many other powerful modeling techniques do not have direct ways of working with this kind of data. These models require that profile data be summarized or collapsed prior to modeling. This chapter will illustrate techniques for using this kind of information in ways that strive to preserve the predictive information while creating a format that can be used across predictive models.

Every practitioner working with real-world data will encounter missing data at some point. While some predictive models (e.g. trees) have novel ways of handling missing data, other models do not and require complete data. Chapter [8]() explores mechanisms that cause missing data and provides visualization tools for investigating missing data patterns. Traditional and modern tools for removing or imputing missing data are provided. In addition, the imputation methods are evaluated for continuous and categorical predictors.

It is tempting to take the tools provided in the previous chapters, apply them to the existing data, then build predictive models on the newly created features. However, a naive approach to these steps would lead to overfit models. Chapter 10 will describe the required steps to guard against overfitting when creating new features. This chapter will provide strategies for determining the best representation of model terms that minimize the risk of overfitting are discussed.

The feature engineering process as described in Chapters [5-9]() can lead to many more predictors than what was contained in the original data. While some of the additional predictors will likely enhance model performance, not all of the original and new predictors will likely be useful for prediction. The final chapter will discuss feature selection and feature selection tactics as an overall strategy for improving model predictive performance. Important aspects include: the goals of feature selection, consequences of irrelevant predictors, comparisons with selection via regularization, and how to avoid overfitting (in the feature selection process). The ineffectiveness of traditional stepwise methods is also discussed.

## Reference

Hill, A, P LaPan, Y Li, and S Haney. 2007. ÅgImpact of Image Segmentation on High-Content Screening Data Quality for SK-BR-3 Cells.Åh BMC Bioinformatics 8 (1):340.

Hosmer, D, and S Lemeshow. 2000. Applied Logistic Regression. 2nd ed. New York: John Wiley & Sons.

Tukey, John W. 1977. Exploratory Data Analysis. Reading, Mass.

Wolpert, D. 1996. ÅgThe Lack of a Priori Distinctions Between Learning Algorithms.Åh Neural Computation 8 (7). MIT Press:1341???90.

Demsar, J. 2006. ÅgStatistical Comparisons of Classifiers over Multiple Data Sets.Åh Journal of Machine Learning Research 7 (Jan):1???30.

Fernandez-Delgado, M, E Cernadas, S Barro, and D Amorim. 2014. ÅgDo We Need Hundreds of Classifiers to Solve Real World Classification Problems?Åh Journal of Machine Learning Research 15 (1):3133???81.

# Illustrative Example: Predicting risk of ischemic stroke

As a primer to feature engineering, an abbreviated example is presented with a modeling process similar to the owe shown in Figure 1.4. For the purpose of illustration, this example will focus on exploration, analysis fit, and feature engineering, through the lens of a single model (logistic regression).

To illustrate the value of feature engineering for enhancing model performance, consider the application of trying to better predict patient risk for ischemic stroke (in press). Historically, the degree arterial stenosis (blockage) has been used to identify patients who are at risk for stroke (Lian et al. [2012]()). To reduce the risk of stroke, patients with sufficient blockage (> 70%) are generally recommended for surgical intervention to remove the blockage (Levinson and Rodriguez [1998]()). However, historical evidence suggests that the degree of blockage alone is actually a poor predictor of future stroke (Meier et al. 2010). This is likely due to the theory that while blockages may be of the same size, the composition of the plaque blockage is also relevant to the risk of stroke outcome. Plaques that are large, yet stable and unlikely to be disrupted may pose less stroke risk than plaques that are smaller, yet less stable.

```{r fig.cap="Fig. 2.1: Three illustrations of the vascuCAP software applied to different carotid arteries and examples of the imaging predictors the software generates. (a) A carotid artery with severe stenosis, represented by the tiny opening through the middle. (b) A carotid artery with calcified plaque (green) and lipid-rich necrotic core (yellow). (c) A carotid artery with severe stenosis and positive remodeling plaque."}

# code

```

To study this hypothesis, a historical set of patients with a range of carotid artery blockages were selected. The data consisted of 126 patients, 44 of which had blockages greater than 70%. All patients had undergone Computed Tomography Angiography (CTA) to generate a detailed three-dimensional visualization and characterization of the blockage. These images were then analyzed by Elucid BioimagingÅfs vascuCAP (TM) software which generates anatomic structure estimates such as percent stenosis, arterial wall thickness, and tissue characteristics such as lipid-rich necrotic core and calcification. As an example, consider Figure 2.1 (a) which represents a carotid artery with severe stenosis as represented by the tiny tube-like opening running through the middle of the artery. Using the image, the software can calculate the maximal cross-sectional stenosis by area (MaxStenosisByArea) and by diameter (MaxStenosisByDiameter). In addition, The grey area in this figure represents macromolecules (such as collagen, elastin, glycoproteins, and proteoglycans) that provide structural support to the arterial wall. This structural region can be quantified by its area (MATXArea). Figure 2.1 (b) illustrates an artery with severe stenosis and calcified plaque (green) and lipid-rich necrotic core (yellow). Both plaque and lipid-rich necrotic core are thought to contribute to stroke risk, and these regions can be quantified by their volume (CALCVol and LRNCVol) and maximal cross-sectional area (MaxCALCArea and MaxLRNCArea). The artery presented in 2.1 (c) displays severe stenosis and outward arterial wall growth. The top arrow depicts the cross-section of greatest stenosis (MaxStenosisByDiameter) and the bottom arrow depicts the cross-section of greatest positive wall remodeling (MaxRemodelingRatio). Remodeling ratio is a measure of the arterial wall where ratios less than 1 indicate a wall shrinkage and ratios greater than 1 indicate wall growth. This metric is likely important because coronary arteries with large ratios like the one displayed here have been associated with rupture (Cilla et al. 2013, Abdeldayem et al. (2015)). The MaxRemodelingRatio metric captures the region of maximum ratio (between the two white arrows) in the three dimensional artery image. A number of other imaging predictors are generated based on physiologically meaningful characterizations of the artery.

```{r}
sample <- tibble(
  a = c("a","b")
)

knitr::kable(
  table(sample)
)
```



