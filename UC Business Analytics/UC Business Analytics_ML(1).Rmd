---
title: "UC business Analytics R programming Guide"
author: "Koji Mizumura"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```


This is a practice of [UC business analytics R programming guide](http://uc-r.github.io/).

# Predictive analytics
## Machine Learning

### Preparing for regression problems

Machine learning is a very iterative process. If performed and interpreted correctly, we can have great confidence in our outcomes. If not, the results will be useless. Approaching machine learning correctly means approaching it strategically by spending our data wisely on learning and validation procedures, properly pre-processing variables, minimizing data leakage, tuning parameters and assessing model performance. 

Before introducing specific algorithms, this tutorial introduces concepts that are commonly required in the supervised machine learning process and that you will see briskly covered in tutorieals that follow. This tutorial will prepare you with the fundamentals needed prior to applying supervised machine learning algorithms.

#### tl;dr

Before introducing specific algorithms, this tutorial introduces concepts that you will see briskly covered in each chapter and are necessary for any type of supervised machine learning model:

1. Prerequisites: what you will need to reproduce the analysis in this tutorial

##### Prerequisites

This tutorial leverages the following packages.

```{r warning=FALSE, message=FALSE} 
library(rsample)
library(caret)
library(h2o)
library(dplyr)

# turn off progress bars
h2o.no_progress()

# launch h2o
h2o.init()
```

To illustrate some of concepts we will use the Ames Housing data that has been included in the `AmesHousing` package and the employee attrition data that has been included in the `rsample` package. The housing data represents a continuous response variable (`Sale_Price`) along with 80 features (predictor variables) for 2930 homes in Ames, IA. Read more about this data [here](https://cran.r-project.org/web/packages/AmesHousing/AmesHousing.pdf). The attrition data represents a classification response variable (`Attrition`) with 30 features for 1470 employees. Read more about this data [here](https://www.ibm.com/communities/analytics/watson-analytics-blog/hr-employee-attrition/)

Throughout this tutorial, we will demonstrate approaches with the regular `df` data frame. However, since many of the supervised machine learning tutorials that we provide leverage `h2o`, we also show how to do some of the things with `h2o`. This requires your data to be in H2O object, which you can convert any data friame easily with `as.h2o`.

```{r}
# ames data
ames <- AmesHousing::make_ames()
ames.h2o <- as.h2o(ames)

# attrition data
churn <- rsample::attrition %>% 
  mutate_if(is.ordered, factor, ordered=FALSE)
churn.h2o <- as.h2o(churn)
```

#### Data splitting
##### Spending our data wisely

A major goal of the machine learning process is to find an algorithm $f(x)$ that most accurately predicts future values $(y)$ based on a set of inputs $(x)$. In other words, we want an algorithm that not only fits well to our past data, but more importantly, one that predicts a future outcome accurately. This is called the generalizability of our algorithm. How we â€œspendâ€? our data will help us understand how well our algorithm generalizes to unseen data.

To provide an accurate understanding of the generalizability of our fina optimal model, we split our data into training and test data sets.

- __Training set__: these data are used to train our algorithms and tune hyper-parameters. 
- __Test set__: having chosen a final model, these data are used to estimate its prediction error (generalization error). These data should not be used _during model training_

Given a fixed amount of data, typical recommendations for splitting your data into training-testing splits include 60% (training) - 40% (testing), 70%-30%, or 80%-20%. Generally speaking, these are appropriate guidelines to follow; however, it is good to keep in mind that as your overall data set gets smaller,

- Spending too much in training ($>80%$) won't allow us to get a good assessment of predictive performance. We may find a model that fits the training data very well, but is not generalizable (overfitting),
- sometimes too much spent in testing ($>40%$) won't allow us to get a good assessment of model parameters. 

Typically, we are lacking in the size of our data here, so a 70-30 split is often sufficinet. The two most common ways of splittin data include __simple random sampling__ and __stratified sampling__.

##### Simple random sampling

The simplest way to split the data into training and test sets is to take a simple random sample. This does not control for any data attributes, such as the percentage of data in the quantiles in your response variable ($y$). There are multiple ways to split our data. Here we show four options to produce a 70-30 split (note that setting the seed value allows you to reproduce your randomized splits):

```{r}
# base R
df <- ames
df.h2o <- ames.h2o

set.seed(123)
df
index   <- sample(1:nrow(df), round(nrow(df) * 0.7))
train_1 <- df[index, ]
test_1  <- df[-index, ]

# caret package
set.seed(123)
index2  <- createDataPartition(df$Sale_Price, p = 0.7, list = FALSE)
train_2 <- df[index2, ]
test_2  <- df[-index2, ]

# rsample package
set.seed(123)
split_1  <- initial_split(df, prop = 0.7)
train_3  <- training(split_1)
test_3   <- testing(split_1)

# h2o package
split_2 <- h2o.splitFrame(df.h2o, ratios = 0.7, seed = 123)
train_4 <- split_2[[1]]
test_4  <- split_2[[2]]
```
Since this sampling approach will randomly sample across the distribution of $y$ (`Sale_Price`), you will typically result in a similar distribution betwen your training and test sets as iilustrated below.

```{r fig.height=3}
# base R
p1 <- ggplot()+
  geom_density(data=train_1, aes(Sale_Price), show.legend = FALSE)+
  geom_density(data=test_1, aes(x=Sale_Price, col="red"),show.legend = FALSE)
 
# caret 
p2 <- ggplot()+
  geom_density(data=train_2, aes(Sale_Price),show.legend = FALSE)+
  geom_density(data=test_2, aes(x=Sale_Price, col="red"),show.legend = FALSE)

# sample
p3 <- ggplot()+
  geom_density(data=train_3, aes(Sale_Price),show.legend = FALSE)+
  geom_density(data=test_3, aes(x=Sale_Price, col="red"),show.legend = FALSE)

# h2o

p4 <- ggplot()+
  geom_density(data=as_tibble(train_4), aes(Sale_Price),show.legend = FALSE)+
  geom_density(data=as_tibble(test_4), aes(x=Sale_Price, col="red"),show.legend = FALSE)

gridExtra::grid.arrange(p1,p2,p3,p4,
                        nrow=1)
```


##### Stratified sampling

However, if we want to explictly control our sampling so that our training and test sets have similar $y$ distributions, we can use __stratified sampling__. This is more common with classification problems where the response variable may be imbalanced (90% of observations with response "Yes" and 10% with response "No"). However, we can also apply to regression problems for data sets that have a small sample size and where the response variable deviates strongly from _normality_.

With a continuous response variable, stratified sampling will break y down into quantiles and randomly sample from each quantile. Consequently, this will help ensure a balanced representation of the response distribution in both the training and test sets.

The easiest way to perform stratified sampling on a response variable is to use the `rsample` package, where you specify the response variable to `strata` fy. The following illustrates that in our original employee attrition data we have an imbalanced response (No: 84%, Yes:16%). By enforcing stratified sampling both our training and testing sets have approximiately equal response distributions.

```{r}

# original response distribution
table(churn$Attrition) %>% prop.table()

# stratified sampling with the rsample package
set.seed(123)
split_strat <- initial_split(churn, prop=0.7, strata = "Attrition")
train_strat <- training(split_strat)
test_strat <- testing(split_strat)

# consistent response ratio between train & test
table(train_strat$Attrition) %>% prop.table()
table(test_strat$Attrition) %>% prop.table()
```

#### Feature engineering
__Feature engineering__ generally refers to the process of adding, deleting and transforming the variables to be applied to your machine learning algorithms.

Feature engineering is a siginificant process and requires you to spend substantial time understanding your data... or as Leo Breiman said "live with your data before you plunge into modeling"

Although this guide is primarily concerned with machine learning algorithms, feature engineering can make or break an algorithmâ€™s predictive ability. We will not cover all the potential ways of implementing feature engineering; however, we will cover a few fundamental pre-processing tasks that can significantly improve modeling performance.

1) One-hot encoding

Many models require all variables to be numeric. Consequently, we need to transform any categorical variables into numeric representation so that these algorithms  can compute. Some packages automate this process (i.e., `h2o`, `glm`, `caret`) while others do not (i.e., `glmnet`, `keras`). Furthermore, there are many ways to encode categorical variables as numeric representation (i.e., one-hot, orinal, binary, sum, Helmert).

The most common is refered to as one-hot encoding, where we transpose our categorical variables so that each level of the feature is represented as a boolean value. For example, one-hot encoding variable `x` in the following:

```{r}
sample <- tibble::tribble(
  ~id, ~x,
  1,"a",
  2,"c",
  3,"b",
  4,"c",
  5,"c",
  6,"a",
  7,"b",
  8,"c"
)

sample %>% 
  mutate(x=as.factor(x))
```

results in the following representation:

If you need to manually implement one-hot encoding yourself, you can do that with `caret::dummyVars`. Sometimes you many have a feature level with very few observations and all these observations show up in the test set but not the training set. The benefit of using `dummyVars` on the full data set and then applying the result to both the train and test data sets is that it will guarantee that the same features are represented in both the train and test data.

```{r}
# full rank one-hot encode - recommended for generalized linear models and neural networks.

library(caret)
full_rank <- dummyVars(~., data = df, fullRank = TRUE)
train_oh <- predict(full_rank, train_1)
test_oh <- predict(full_rank, test_1)

train_1 %>% head()
train_oh %>% as_tibble() %>% head()

# less than full rank
dummy <- dummyVars(~., data = df, fullRank=FALSE)
train_oh <- predict(dummy, train_1)
test_oh <- predict(dummy, test_1)
```

Two things to note:

- Since one-hot encoding adds new features it can significantly increase the dimensionality of our data. If you have a dataset with many categorical variables and those categorical variables in turn have many unique levels, the number of features can explode. In these cases you may want to explore ordinal encoding of your data.

- if using `h2o` you do not need to explictly encode your categorical variables but you can override the default encoding. This can be considered a tuning parameter as some encoding will improve modeling accuracy over other encodings. See the encoding options for `h2o` [here](http://docs.h2o.ai/h2o/latest-stable/h2o-docs/data-science/algo-params/categorical_encoding.html).


##### Response Transformation

Although not a requirement, normalizing the distribution of the response variable by using a _transformation_ can lead to a big improvement, especially for parametric models. As we saw in the data splitting section, our response variable `Sale_Price` is right skewed.

```{r}
ggplot(train_1,aes(x=Sale_Price))+
  geom_density(trim=TRUE)+
  geom_density(data=test_1, trim=T, col="red")
```

To normalize, we have two options:

__Option 1__: normalize with a __log transformation_._ This will transform most right skewed distributions to be approximiately normal.

```{r}
# log transformation
train_log_y <- log10(train_1$Sale_Price)
test_log_y <- log10(test_1$Sale_Price)

log_transform <- ggplot(data = tibble(train_log_y), aes(train_log_y))+
  geom_density(trim=TRUE)+
  geom_density(data=tibble(test_log_y),aes(test_log_y), trim=T, col="red")
log_transform
```

__Option 2__: use a __Box Cox transformation__.

A Box Cox transformation is more flexible and will find the transformation from a family of power transformations that will transform the variable as close as possible to a normal distribution. 

__Important notes__: be sure to compute the `lambda` on the training set and apply that same `lambda` to both the training and test set to minimize data leakage.

```{r}
# Box cox transformation
lambda <- forecast::BoxCox.lambda(train_1$Sale_Price)
train_bc_y <- forecast::BoxCox(train_1$Sale_Price, lambda)
test_bc_y <- forecast::BoxCox(test_1$Sale_Price, lambda)
```

We can see that in this example, the log transformation and Box Cox transformation both do about equally well in transforming our response variable to be normally distributed.

```{r}
p1 <- ggplot(data=train_1, aes(Sale_Price))+
  geom_histogram(bins=100, col="red")+
  ggtitle("Normal")

p2 <- ggplot(data = tibble(train_log_y), aes(train_log_y))+
  geom_histogram(bins=100, col="lime green")+
  ggtitle("Log_Transform")

p3 <- ggplot(data=tibble(train_bc_y), aes(train_bc_y))+
  geom_histogram(bins=100,col="blue")+
  ggtitle("BoxCox_Transform")

gridExtra::grid.arrange(p1,p2,p3, nrow=1)
```

Note that when you model with a transformed response variable, your predictions will also be in the transformed value. You will likely want to re-transform your predicted values back to their normal state so that decision-makers can interpret the results. The following code can do this for you:

```{r}
# log transform a value
y <- log(10)

# re-transforming the log-transformed value
exp(y)

# Box Cox transform a value
y <- forecast::BoxCox(10, lambda)
y
# forecast::BoxCox

# inverse Box Cox function
inv_box_cox <- function(x,lambda){
  if (lambda==0) exp(x) else (lambda*x+1)^(1/lambda)
}

# re-transfrming the Box Cox transform value
inv_box_cox(y, lambda)
```

##### Predictor transformation

Some models such as <span style="color:red"> K-NN, SVMs, PLS, neural networks </span> require that the features have the same units. __Centering__ and __scaling__ can be used for this purpose and is often refered to as __standardizing__ the features. Standardizing numeric variables results in zero mean and unit variance, which provides a common comparable unit of measure across all the variables. 

Some packages have built^in arguments (i.e., `h20`, `caret`) to standardize and some do not (ie., `glm`, `keras`). IF you need to manually standardize your variables you can use the `preProcess` function provided by the `caret` package.

For example, here we center and scale our predictor variables. Note, it is important you standardize the test data based on the training mean and variance values of each feature. This minimizes data leakage. 

```{r}
# identify only the predictor variables
features <- setdiff(names(train_1), "Sale_Price")

# pre-process estimation based on training features 
pre_process <- caret::preProcess(
  x = train_1[,features],
  method = c("center", "scale")
)

# apply to both training & test
train_x <- predict(pre_process, train_1[, features])
test_x <- predict(pre_process, test_1[,features])
```

##### Alternative feature transformation

There are some alternative transformations that you can perform: 

- Normalizing the predictor variables with a _Box Cox transformation_ can improve parametric model performance.

- Collapsing highly correlated variables with _PCA_ can reduce the number of features and increase the stability of generalize linear models. However, this reduces the amount of information at your disposal and future tutorials show you how to use regularization as a better alternative to PCA.

- Removing _near-zero_ or _zero variance variables_. Variables with vary little variance tend to not improve model performance and can be removed.

- preProcess provides other options which you can read more about [here](https://topepo.github.io/caret/pre-processing.html).

```{r}
# identify only the predictor variables
features <- setdiff(names(train_1), "Sale_Price")

# pre-process estimation based on training features
pre_process <- preProcess(
  x      = train_1[, features],
  method = c("center", "scale", "pca", "nzv") 
  )

# apply to both training & test
train_x <- predict(pre_process, train_1[,features])
test_x <- predict(pre_process, test_1[, features])
```


#### Basic mddel formulation

There are __many__ packages to perform machine learning and there are always more than one to perform each algorithm (i.e., there are over 20 packages to perform random forests). There are pros/cons to each package; some nay be more computationally efficient while other may have more hyperparameter tuning options. 

Future tutorials will expose you to several packages; some that have become "the standard" and others that are new and may be considered "maturing". Just realize there are more ways than one to skin.

For example, these three functions will all produce the samme linear regression model output:

```{r eval=FALSE}
lm.lm <- lm(Sale_Price~., data = train_1) 
lm.glm <- glm(Sale_Price~., data=train_1, family=gaussian)
lm.caret <- caret::train(Sale_Price ~., data=train_1, method = "lm")

lm_multiple_package = tibble(
  model = c("lm", "glm", "caret"),
  outcome = list(lm.lm, lm.glm, lm.caret)
)

# test <- "abcde"
# stringr::str_sub(test, 1,3)

lm_multiple_package$outcome[[1]] %>% 
  broom::tidy() %>% 
  ggplot(aes(stringr::str_sub(term, 1, 3), estimate, col=p.value<0.01))+
  geom_point()+
  coord_flip()

# lm_multiple_package %>% 
#   mutate( outcome_tidy = purrr::map(outcome, broom::tidy))
```

One thing you will notice throughout future tutotiral s is that we can specify our model formulation in different ways. In the above examples, we use the model formulation (`Sale_Price`, which says explain `Sale_Price` based on all features) approach. Alternative approaches include the matrix formulation and variable name specification approaches. 

_Matrix formulation_ requires that we separate our response variable from our features. For example, in the regularization tutorial we will use `glmnet` which requires our features `x` and response `y` to be specified separately:

```{r eval=FALSE}
# get feature names
features <- setdiff(names(train_1), "Sale_Price")

# create feature and response set
train_x <- train_1[, features]
train_y <- train_1$Sale_Price

# example of matrix formulation
library(glmnet)
glmnet.m1 <- glmnet(x = train_x, y = train_y)
```

Alternatively, `h2o` uses _variable name specification_ where we provide all the data combined in one `training_frame` but we specify the features and response with character strings:

```{r}
# create variable names and h2o training frame
h2o.init()
y <- "Sale_Price"
x <- setdiff(names(train_1), y)
train.h2o <- as.h2o(train_1)

# example of variable name specification
h20.m1 <- h2o.glm(x=x, y=y, training_frame = train.h2o)
```


##### Model tuning

Hyperparameters control the level of model complexity. Some algorithms have many tuning parameters while others have only one or two. Tuning can be a good thing as it allows us to transform our model to better align with pattersn within our data For example the simple illustration below shows how the more flexible model aligns more closely to the data than fixed linear model.

```{r fig.cap="Fig.5: Tuning allows for more flexibile pattersn to be fit"}

```

However, highly tunable models can also be dangerous because they allow us to overfit our model to the training data, which will not generalize well to future unseen data.

```{r fig.cap="Fig.6: Highly tunable models can overfit if we are not careful"} 

```

Throughout the future tutorial, we will demonstrate how to tune the different parameters for each model. However, we bring up this point because it feeds into the next section nicely.

##### Cross validation for generalization

Our goal is to not only find a model that performs well on training data, but to find one that performs well on _future unseen data_. So although we can tune our model to reduce some error metric to near zero on our training data, this may not generalize well to future unseen data. Consequently, our goal is to find a model and its hyperparameters that will minimize error on hold-out data.

```{r fig.cap="Fig.7: Bias versus variance"}

```

The model on the left is considered rigid and consistent. If we provided it a new training sample with slightly different values, the model would not change much, if at all. Although it is consistent, the models does not accurately capture the underlying relationship. This is considered a model with high _bias_. 

The model on the right is far more inconsistent. Even with small changes to our training sample, this model would likely change significantly. This is considered a model with high _variance_.

The model in the middle balances the two and likely will minimize the error on future unseen data compared to the high bias and high variance models. This is our goal.

```{r fig.cap="Fig 8: Bias-variance tradeoff", eval=FALSE}
knitr::include_graphics("")
```

To find the model that balances the _bias-variance tradeoff_, we search for a model that minimizes a _k_-fold cross-validation error metric (you will also be introduced to what's called an _out of bag error_ which provides a similar form of evaluation). _k_-fold cross-validation is a resampling method that randomly divides the training data into _k_ groups (aka folds) of approximately same size. The model is fit on $k-1$ folds and then held-out validation fods is used to compute the error. 

This process results in _k_ estimates of the test error ($\epsilon_1,\epsilon_2,...,\epsilon_k$). Thus, the _k_-fold CV estimate is computed by averagin these values, which provides us with an approximation of the error to expect on unseen data.

```{r fig.cap="Fig 9: Illustration of the k-fold cross validation process", eval=FALSE}
knitr::include_graphics("")
```

Most algorithms and packages we cover in future tutorials have built-in cross-validation capabilities. One typically uses a 5 or 10 fold CV ( _k_=5 or _k_=10 ). For example, `h2o` implements CV with the `nfolds` argument:

```{r eval=FALSE}
# example of 10 fold CV in h2o
h2o.cv <- h2o.gbm(
  x=x,
  y=y,
  training_frame = train.h2o,
  nfolds =10
)
```

#### Model evaulation

This leads us to our final topic, error metrics to evaluate performance. There are several metrics we can choose from to assess the error of a supervised machine learning model. The most common include:

###### Regression models

- __MSE__: Mean squared error is the average of the squared eorr ($MSE = \frac{1}{n}\Sigma_{i=1}^n(y_i-\hat{y})^2$). The squared component results in larger errors having larger penalties. This (along with RMSE) is the most common error metric to use. Objective: __minimize__  

- __RMSE__: Root Root mean squared error. This simply takes the square root of the MSE metric (RMSE=$\sqrt{\frac{1}{n}\Sigma_{i=1}^n(y_i-\hat{y})^2} $) so that your error is in the same units as your response variable. If your response variable units are dollars, the units of MSE are dollars-squared, but the RMSE will be in dollars. Objective: __minimize__  

- __Deviance__:Short for mean residual deviance. In essence, it provides a measure of goodness-of-fit of the model being evaluated when compared to the null model (intercept only). If the response variable distribution is gaussian, then it is equal to MSE. When not, it usually gives a more useful estimate of error. __Objective: minimize__

- __MAE__:ean absolute error. Similar to MSE but rather than squaring, it just takes the mean absolute difference between the actual and predicted values ($MAE=\frac{1}{n}\Sigma_{i=1}^n|(y_i-\hat{y})|$) __Objective: minimize__

- __RMSLE__:Root mean squared logarithmic error. Similiar to RMSE but it performs a `log()` on the actual and predicted values prior to computing the difference ($ RMSLE = \sqrt{\frac{1}{n}\Sigma_{i=1}^n (log(y_i+1)-log())$ ) When your response variable has a wide range of values, large response values with large errors can dominate the MSE/RMSE metric. RMSLE minimizes this impact so that small response values with large errors can have just as meaningful of an impact as large response values with large errors. __Objective: minimize__

- __$R^2$__: This is a popular metric that represents the proportion of the variance in the dependent variable that is predictable from the independent variable. Unfortunately, it has several limitations. For example, two models built from two different data sets could have the exact same RMSE but if one has less variability in the response variable then it would have a lower $R^2$
 than the other. You should not place too much emphasis on this metric. 

Most models we assess in future tutorials will report most, if not all, of these metrics. We will often emphasize and RMSE but its good to realize that certain situations warrant emphasis on some more than others.

##### Classification models

- __Misclassification__: This is the overall error. For example, say you are predicting 3 classes ( high, medium, low ) and each class has 25, 30, 35 observations respectively (90 observations total). If you misclassify 3 observations of class high, 6 of class medium, and 4 of class low, then you misclassified 13 out of 90 observations resulting in a 14% misclassification rate. Objective: minimize

- __Mean per class error__: This is the average error rate for each class. For the above example, this would be the mean of $\frac{3}{25}, \frac{6}{30}, \frac{4}{35} $ , which is 12%. If your classes are balanced this will be identical to misclassification. __Objective: minimize__

- __MSE__: Mean squared error. Computes the distance from 1.0 to the probability suggested. So, say we have three classes, A, B, and C, and your model predicts a probabilty of 0.91 for A, 0.07 for B, and 0.02 for C. If the correct answer was A the $MSE=0.09^2=.0081$, if it is B $MSE=0.93^2=0.8649$, if it is C $MSE=0.98^2=0.9604$. The squared component results in large differences in probabilities for the true class having larger penalties. __Objective: minimize__

- __Cross-entropy (aka Log Loss or Deviance)__: Similar to MSE but it incorporates a log of the predicted probability multiplied by the true class. Consequently, this metric disproportionately punishes predictions where we predict a small probability for the true class, which is another way of saying having high confidence in the wrong answer is really bad. __Objective: minimize__

- __Gini index__: Mainly used with tree-based methods and commonly referred to as a measure of purity where a small value indicates that a node contains predominantly observations from a single class. __Objective: minimize__

When applying classification models, we often use a _confusion matrix_ to evaluate certain performance measures.A confusion matrix is simply a matrix that compares actual categorical levels (or events) to the predicted categorical levels. When we predict the right level, we refer to this as a true positive. However, if we predict a level or event that did not happen this is called a false positive (i.e. we predicted a customer would redeem a coupon and they did not). Alternatively, when we do not predict a level or event and it does happen that this is called a false negative (i.e. a customer that we did not predict to redeem a coupon does).

```{r fig.cap="Fig 10: Confusion matrix"}

```

We can extract different levels of performance from these measures. For example, given the classification matrix below, we can assess the following:

- __Accuracy__: Overall, how often is the classifier correct? Opposite of misclassification above. Example: $\frac{TP+TN}{total} = \frac{100+50}{165}=0.91$. __Objective: maximize__

- __Precision__: How accurately does the classifier predict events? This metric is concerned with maximizing the true positives to false positive ratio. In other words, for the number of predictions that we made, how many were correct? Example: $\frac{TP}{TP+FP} = \frac{100}{100+10}=0.91$. __Objective: maximize__

- __Sensitivity (aka recall)__: How accurately does the clasifier classify actual events? This metric is concerned with maximizing the true positives to false negative ratio. In other words, for the events that occured, how many did we predict? Example: $\frac{TP}{TP+FN} = \frac{100}{100+5}=0.95$. __Objective: maximize__

- __Specificity__: How accurately does the classifier classify actual events? This metric is concerned with maximizing the true positives to false negatives ratio. In other words, for the events that occurred, how many did we predict? Example: $\frac{TN}{TN+FP} = \frac{50}{50+10}=0.95$. __Objective: maximize__

```{r fig.cap="Fig 11: Example confusion matrix"}
```

- __AUC__: Area under the curve. A good classifier will have high precision and sensitivity. This means the classifier does well when it predicts an event will and will not occur, which minimizes false positives and false negatives. To capture this balance, we often use a ROC curve that plots the false positive rate along the x-axis and the true positive rate along the y-axis. A line that is diagonal from the lower left corner to the upper right corner represents a random guess. The higher the line is in the upper left-hand corner, the better. AUC computes the area under this curve. Objective: maximize

### Linear regression

Linear regression is a very simple approach for supervised learning.In particular, linear regression is a useful tool for predicting a quantitative response. Linear regression has been around for a long time and is the topic of innumerable textbooks. Though it may seem somewhat dull compared to some of the more modern statistical learning approaches described in later tutorials, linear regression is still a useful and widely used statistical learning method. Moreover, it serves as a good jumping-off point for newer approaches: as we will see in later tutorials, many fancy statistical learning approaches can be seen as generalizations or extensions of linear regression. Consequently, the importance of having a good understanding of linear regression before studying more complex learning methods cannot be overstated.

#### tl;dr

This tutorial serves asa an introduction to linear regression
1. [Replication requirements](#RR): What you will need to reproduce 
2. Preparing our data: Prepare our data for modeling
3. Simple linear regression: Predicting a quantitative response $Y$ with a single predictor variable $X$
4. Multiple linear regression: Predicting a quantitative response $Y$ with multiple predictor variables $X_1, X_2, ..., X_p$
5. Incorporating interactions: Removing the additive assumption
6. Additional considerations: A few other considerations to know about

##### Replication requirements {#RR}

This tutorial primarily leverages this [advertising data](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv) provided by the authors of [an Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISL/index.html). This is a simple data set that contains, in thousands of dolloars, `TV`, `Radio`, and `Newspaper` budgets for 200 different markets along with the `Sales`, in thousands of units, for each market. We will also use a few package that provide data manipulation, visualization, pipeline modeling functions, and model output tidying functions. 

```{r}
# Packages
library(tidyverse) # data manipulation and visualizations
library(modelr)# provides easy pipeline modeling functions
library(broom) # helps to tidy up model outputs

advertising <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv") %>%
   select(-X1)

advertising
```


##### Preparing our data

Initial descovery of relationships is usually done with a training set while a test set is used for evaluating whether the discovered relationships hold.More formally, a training set is a set of data used to discover potentially predictive relationships. A test set is a set of data used to assess the strength and utility of a predictive relationship. In a later tutorial we will cover more sophisticated ways for training, validating, and testing predictive models but for the time being weâ€™ll use a conventional 60% / 40% split where we training our model on 60% of the data and then test the model performance on 40% of the data that is withheld.

```{r}
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(advertising), replace=TRUE, prob=c(0.6,0.4))
train <- advertising[sample, ]
test <- advertising[!sample, ]

```

#### Simple linear regression

_Simple linear regression_ lives up to its name; it is a very straightfoward approach for prediting a quantitative response $Y$ on the basis of a single predictor variable $X$. It assumes that there is approximately a linear relationship between $X$ and $Y$. Using our advertising data, suppose we wish to model the linear relationship between the TV budget and sales. We can write this as:
$$
Y = \beta_0 + \beta_1X+\epsilon
$$

where;
- $X$ represents sale
- $X$ represents _TV adversiting budget_
- $\beta_o$ is the intercept
- $\beta_1$ is the coefficient representing the linear relationship
- $\epsilon$ is mean-zero random error term

##### Model building 

To build this model in R we use the formula notation of $Y~X$. 

```{r}
model1 <- lm(sales ~ TV, data = train)
summary(model1)
```

In the background the `lm`, which stands for â€œlinear modelâ€?, is producing the best-fit linear relationship by minimizing the least squares criterion (alternative approaches will be considered in later tutorials). This fit can be visualized in the following illustration where the â€œbest-fitâ€? line is found by minimizing the sum of squared errors (the errors are represented by the vertical black line segments).

```{r}
ggplot(data = train, aes(TV, sales))+
  geom_point()+
  geom_line(aes(y=predict(model1,train)), col="red")
```

For initial assessment of our model we can use `summary`. This provides us with a host of information about our model, which weâ€™ll walk through. Alternatively, you can also use `glance(model1)` to get a â€œtidyâ€? result output.

```{r}
library(broom)
model_results <- 
  list(
    tidy = broom::tidy(model1),
    glance = glance(model1)
  )

model_results[[1]]
model_results[[2]]
```

##### Asessing coeficcients

Our original formula in Eq. (1) includes $\Î²_0$ for our intercept coefficent and 
$/Î²_1$  for our slope coefficient. If we look at our model results (here we use tidy to just print out a tidy version of our coefficent results) we see that our model takes the form of

```{r}
#broom::tidy()
model_results[[1]]
```

In other words, our intercept estiamte is $6.76$ so when the TV advertising budget is zero, we can expect sales to be $6,760$ (remember we are operating in units of $1,000$). And for every $1,000$ increase in the TV advertising budget we expect the average increase in sales to be 50 units.

Itâ€™s also important to understand if the these coefficients are statistically significant. In other words, can we state these coefficients are statistically different then 0? To do that we can start by assessing the standard error (SE). The SE for $\beta_0$ and $\beta_1$ are computed with:

$$
SE(\beta_0)=\sigma^2[\frac{1}{n}+\frac{\hat x^2}{\Sigma_i=1^n(x_i-\bar{x})^2}],
SE(\beta_1)=\frac{\sigma^2}{\Sigma_{i=1}^n(x_i-\bar{x})^2}...(3)
$$

where $\sigma=Var(\epsilon)$. We see that our model results provide the SE (noted as Std.error). We can use the SE to compute the 95% confidence internal for the coefficients.

$$
\beta_1 = 2*SE(\beta_1)...(4)
$$

To get this information in R, we can simply use:

```{r}
confint(model1)
```

Our results show us that our 95% confidence interval for $Î²_1$ (TV) is [.043, .057]. Thus, since zero is not in this interval we can conclude that as the TV advertising budget increases by $1,000 we can expect the sales to increase by 43-57 units. This is also supported by the _t-statistic_ provided by our results, which are computed by

$$
t = \frac{\beta_1 -0}{SE(\beta_1)}...(5)
$$

which measures he number of standard deviations that $Î²_1$ is away from 0. Thus a large t-statistic such as ours will produe a small p-value (a small p-value indicates that it is unlikely to observe such a substantial association between the predictor variable and the response due to chance). Thus, we can conclude that a relationship between TV advertising budget and sales exists.

##### Assessing model accuracy

Next, we want to understand the extent to which the model fits the data. This is typically referred to as the _goodness-of-fit_. We can measure this quantitatively by assessing three things:

1. Residual standard error
2. R squared($R^2$)
3. F-statistic

The RSE is an  estimate of the standard deviation of 
$Ïµ$. Roughly speaking, it is the average amount that the response will deviate from the true regression line. It is computed by:

$$
RSE = \sqrt{\frac{1}{n-2}\Sigma_{i=1}^n(y_i-\hat{y_i})^2}...(6)
$$

We get the RSE at the bottom of `summary(model1)`, we can also get it directly with:

```{r}
sigma(model1)
```

An RSE value of $3.2$ means the actual sales in each market will deviate from the true regression line by approximately $3,200$ units, on average. Is this significant? Well, thatâ€™s subjective but when compared to the average value of sales over all markets the percentage error is $22%$:

```{r}
sigma(model1)/mean(train$sales)
```

The RSE provides an absolute measure of lack of fit of our model to the data. But since it is measured in the units of $Y$, it is not always clear what constitutes a good RSE. The $R^2$ statistic provides an alternative measure of fit. It represents the proportion of variance explained and so it always takes on a value between 0 and 1, and is independent of the scale of $Y$.$R^2$ is simply a function of residual sum of squares (RSS) and total sum of squares (TSS):

$$
R^2 = 1 - \frac{RSS}{TSS} = 1- \frac{\Sigma_{i=1}^n  (y_i-\hat{y_i})^2}{\Sigma_{i=1}^n  (y_i-\bar{y_i})^2}
$$

Similar to RSE the $R^2$ can be found at the bottom of `summary(model1)` but we can also generate it directly with `rsquare`. The result suggests that TV advertising budget can explain 64% of the variability in our sales data.

```{r}
rsquare(model1, data = train)
```

As a side note, in a simple linear regression model the $R^2$ value will equal the squared correlation between $X$ and $Y$:
```{r}
cor(train$TV, train$sales)^2
```

Lastly, the _F-statistic_ tests to see if at least one predictor variable has a non-zero coefficient. This becomes more important once we start using multiple predictors as in multiple linear regression; however, weâ€™ll introduce it here. The _F-statistic_ is computed as:

$$
F = \frac{(TSS-RSS)/p}
         {RSS/(n-p-1)}
$$

Hence, a larger F-statistic will produce a significant p-value (<0.05). In ou case, we see at the bottom of our summary statement that the F-statistic is 210.8 producing a p-value of $p<2.2e-16$. 

Combined, our RSE, $R^2$, and F-statistic results suggest that our model has an ok fit, but we could likely do better.

##### Asessing our model visually

Not only is it important to to understand quantitative measures regarding our coefficient and model accuracy but we should also understand visual approaches to assess our model. First, we should always visualize our model within our data when possible. For simple linear regression this is quite simple. Here we use `geom_smooth(method = "lm")` followed by `geom_smooth()`. This allows us to compare the linearity of our model (blue line with the 95% confidence interval in shaded region) with a non-linear __LOESS model__. Considering the LOESS smoother remains within the confidence interval we can assume the linear trend fits the essence of this relationship. However, we do note that as the TV advertising budget gets closer to 0 there is a stronger reduction in sales beyond what the linear trend follows.

```{r}
ggplot(train, aes(TV, sales))+
  geom_point()+
  geom_smooth(method = "lm")+
  geom_smooth(se=FALSE, color="red")
```

An important part of assessing regression models is visualizing residuals. If you use `plot(model1)` four residual plots will be produced that provide some insights. Here I will walk through creating each of these plots within ggplot and explain their insights.

```{r}
plot(model1)
```

First is a plot of residual versus fitter values. This will signal two important concerns:

1. Non-linearity: if a discernible pattern (blue line) exists then this suggests either non-linearity or that other attributes have not been adequately captured. Our plot indicates that the assumption of linearity is fair.

2. Heteroskedasticity: an important assumption of linear regression is that the error terms have a constant variance, $Var(\epsilon)=\sigma^2$. If there is a funnel shape with our residuals, as in our plot, then we have  violated this assumption. Sometimes this can be resolved with a log or square root transformation of $Y$ in our model.

```{r}
# add model diagnostics to our training data
model1_results <- augment(model1, train)
model1_results

ggplot(model1_results, aes(.fitted, .resid))+
  geom_ref_line(h=0)+
  geom_point()+
  geom_smooth(se=FALSE)+
  ggtitle("Residual vs Fitted")
```

We can get this same kind of information with a couple other plots which you will see when using `plot(model1)`. The first is comparing standardized residuals versus fitted values. This is the same plot as above but with the residuals standardized to show where residuals deviate by 1, 2, 3+ standard deviations. This helps us to identify outliers that exceed 3 standard deviations. The second is the scale-location plot. This plot shows if residuals are spread equally along the ranges of predictors. This is how you can check the assumption of equal variance (homoscedasticity). Itâ€™s good if you see a horizontal line with equally (randomly) spread points.

```{r}
p1 <- ggplot(model1_results, aes(.fitted, .std.resid))+
  geom_ref_line(h=0)+
  geom_point()+
   geom_smooth(se = FALSE) +
  ggtitle("Standardized Residuals vs Fitted")

p2 <- ggplot(model1_results, aes(.fitted, sqrt(.std.resid))) +
  geom_ref_line(h = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  ggtitle("Scale-Location")

gridExtra::grid.arrange(p1, p2, nrow = 1)
```

The next plot assess the normality of our residuals. A Q-Q plot plots the distribution of our residuals against the theoretical normal distribution. The closer the points are to falling directly on the diagonal line then the more we can interpret the residuals as normally distributed. If there is strong snaking or deviations from the diagonal line then we should consider our residuals non-normally distributed. In our case we have a little deviation in the bottom left-hand side which likely is the concern we mentioned earlier that as the TV advertising budget approaches 0 the relationship with sales appears to start veering away from a linear relationship.

```{r}
qq_plot <- qqnorm(model1_results$.resid)
qq_plot <- qqline(model1_results$.resid)
```

Last are the Cookâ€™s Distance and residuals versus leverage plot. These plot helps us to find influential cases (i.e., subjects) if any. Not all outliers are influential in linear regression analysis. Even though data have extreme values, they might not be influential to determine a regression line. That means, the results wouldnâ€™t be much different if we either include or exclude them from analysis. They follow the trend in the majority of cases and they donâ€™t really matter; they are not influential. On the other hand, some cases could be very influential even if they look to be within a reasonable range of the values. They could be extreme cases against a regression line and can alter the results if we exclude them from analysis. Another way to put it is that they donâ€™t get along with the trend in the majority of the cases.

Here we are looking for outlying values (we can select the top n outliers to report with `id.n`. The identified (labeled) points represent those splots where cases can be influential against a regression line. When cases have high Cookâ€™s distance scores and are to the upper or lower right of our leverage plot they have leverage meaning they are influential to the regression results. The regression results will be altered if we exclude those cases.

```{r}
par(mfrow=c(1, 2))

plot(model1, which = 4, id.n = 5)
plot(model1, which = 5, id.n = 5)
```

If you want to look at these top 5 observations with the highest Cookâ€™s distance in case you want to assess them further you can use the following.

```{r}
model1_results %>% 
  top_n(5, wt=.cooksd)
```

So, what does having patterns in residuals mean to your research? Itâ€™s not just a go-or-stop sign. It tells you about your model and data. Your current model might not be the best way to understand your data if thereâ€™s so much good stuff left in the data.

In that case, you may want to go back to your theory and hypotheses. Is it really a linear relationship between the predictors and the outcome? You may want to include a quadratic term, for example. A log transformation may better represent the phenomena that youâ€™d like to model. Or, is there any important variable that you left out from your model? Other variables you didnâ€™t include (e.g., Radio or Newspaper advertising budgets) may play an important role in your model and data. Or, maybe, your data were systematically biased when collecting data. You may want to redesign data collection methods.

Checking residuals is a way to discover new insights in your model and data!

##### Making predictions

Often the goal with regression approaches is to make predictions on new data. To assess how well our model do in this endeavor we need to assess how it does in making predictions against our test data set. This informs us how well our model generalizes to data outside our training set. We can use our model to predict Sales values for our test data by using `add_predictions`.

```{r}
test <- test %>% 
  add_predictions(model1)
test
```

The primary concern is to assess if the __out-of-sample__ mean squared error (MSE), also known as the mean squared prediction error, is substantially higher than the in-sample mean square error, as this is a sign of deficiency in the model. The MSE is computed as

$$
MSE = \frac{1}{n}\Sigma_{i=1}^n (y_i-\hat{y_i})^2
$$

We can easily compare the test sample MSE to the training sample MSE with the following.The difference is not that significant. However, this practice becomes more powerful when you are comparing multiple models. For example, if you developed a simple linear model with just the Radio advertising budget as the predictor variable, you could then compare our two different simple linear models and the one that produces the lowest test sample MSE is the preferred model.

```{r}
# test MSE
test %>% 
  add_predictions(model1) %>% 
  summarise(MSE = mean((sales-pred)^2))

# training MSE
train %>% 
  add_predictions(model1) %>%
  summarise(MSE = mean((sales - pred)^2))
## # A tibble: 1 Ã? 1
##        MSE
##      <dbl>
## 1 10.09814
```

In the next tutorial we will look at how we can extend a simple linear regression model into a multiple regression.

#### Multiple regression

Simple linear regression is a useful approach for predicting a response on the basis of a single predictor variable. However, in practice we often have more than one predictor. For example, in the Advertising data, we have examined the relationship between sales and TV advertising. We also have data for the amount of money spent advertising on the radio and in newspapers, and we may want to know whether either of these two media is associated with sales. How can we extend our analysis of the advertising data in order to accommodate these two additional predictors?

We can extend the simple linear regression model so that it can directly accommodate multiple predictors. We can do this by giving each predictor a separate slope coefficient in a single model. In general, suppose that we have p distinct predictors. Then the multiple linear regression model takes the form

$$
Y = \beta_0 + \beta_1X_1+...+\beta_pX_p+\epsilon...(10)
$$

##### Model building

If we want to run a model that uses TV, Radio, and Newspaper to predict Sales then we build this model in R using a similar approach introduced in the Simple Linear Regression tutorial.

```{r}
model2 <- lm(sales ~ TV + radio + newspaper, data=train)
```

We can also assess this model as before:

```{r}
summary(model2)
```

##### Assessing coefficients

The interpreation of our coefficients is the same as in a linear regression model. First, we see that our coefficients for TV and Radio advertising budget are statistically significant (p-value < 0.05) while the coefficient for Newspaper is not. Thus, changes in Newspaper budget do not appear to have a relationship with changes in sales. However, for TV our coefficent suggests that for every $1,000$ increase in TV advertising budget, holding all other predictors constant, we can expect an increase of 47 sales units, on average (this is similar to what we found in the simple linear regression). The Radio coefficient suggests that for every $1,000$ increase in Radio advertising budget, holding all other predictors constant, we can expect an increase of 196 sales units, on average.

```{r}
broom::tidy(model2)
```

We can also get our confidence intervals around these coefficient estimates as we did before. Here we see how the confidence interval for Newspaper includes 0 which suggests that we cannot assume the coefficient estimate of -0.0106 is different than 0.

```{r}
confint(model2)
```

##### Assessing model accuracy

Assessing model accuracy is very similar as when assessing simple linear regression models. Rather than repeat the discussion, here I will highlight a few key considerations. First, multiple regression is when the F-statistic becomes more important as this statistic is testing to see if at least one of the coefficients is non-zero. When there is no relationship between the response and predictors, we expect the F-statistic to take on a value close to 1. On the other hand, if at least predictor has a relationship then we expect $F>1$. In our summary print out above for model 2 we saw that $F=445.9$ with $p<0.05$ suggesting that at least one of the advertising media must be related to sales.

In addition, if we compare the results from our simple linea regression model (`model1`) and our multiple regression model (`model2`) we can make some important comparisons:

```{r}
list(
  model1 = broom::glance(model1),
  model2 = broom::glance(model2)
)
```

1. __$R^2$__: Model 2â€™s $R^2=.92$ is substantially higher than model 1 suggesting that model 2 does a better job explaining the variance in sales. Itâ€™s also important to consider the adjusted $R^2$. The adjusted $R^2$ is a modified version of $R^2$ that has been adjusted for the number of predictors in the model. The adjusted $R^2$
 increases only if the new term improves the model more than would be expected by chance. Thus, since model 2â€™s adjusted $R^2$ is also substantially higher than model 1 we confirm that the additional predictors are improving the modelâ€™s performance.
2. __RSE__: Model 2â€™s RSE (sigma) is lower than model 1. This shows that model 2 reduces the variance of our 
$Ïµ$ parameter which corroborates our conclusion that model 2 does a better job modeling sales.
3. __F-statistic__: the F-statistic (statistic) in model 2 is larger than model 1. Here larger is better and suggests that model 2 provides a better â€œgoodness-of-fitâ€?.
4. __Other__: We can also use other various statistics to compare the quality of our models. These include Akaike information criterion (AIC) and Bayesian information criterion (BIC), which we see in our results, among others. Weâ€™ll go into more details regarding these statistics in the Linear Model Selection tutorial but for now just know that models with lower AIC and BIC values are considered of better quality than models with higher values.

So we understand that quantitative attributes of our second model suggest it is a better fit, how about visually?

##### Assessing our model visually

Our main focus is to assess and compare residual behavior with our models. First, if we compare model 2â€™s residuals versus fitted values we see that model 2 has reduced concerns with heteroskedasticity; however, we now have discernible patter suggesting concerns of linearity. Weâ€™ll see one way to address this in the next section.

```{r}
# add model diagnostics to our training data
model1_results <- model1_results %>%
  mutate(Model = "Model 1")

model2_results <- augment(model2, train) %>%
  mutate(Model = "Model 2") %>%
  rbind(model1_results)

ggplot(model2_results, aes(.fitted, .resid)) +
  geom_ref_line(h = 0) +
  geom_point() +
  geom_smooth(se = FALSE) +
  facet_wrap(~ Model) +
  ggtitle("Residuals vs Fitted")
```

This concern with normality is supported when we compare the Q-Q plots. So although our model is performing better numerically, we now have a greater concern with normality then we did before! This is why we must always assess models numerically and visually!

```{r}
par(mfrow=c(1, 2))

# Left: model 1
qqnorm(model1_results$.resid); qqline(model1_results$.resid)

# Right: model 2
qqnorm(model2_results$.resid); qqline(model2_results$.resid)
```

##### Making predictions

To see how our models compare when making predictions on an out-of-sample data set weâ€™ll compare MSE. Here we can use `gather_predictions` to predict on our test data with both models and then, as before, compute the MSE. Here we see that model 2 drastically reduces MSE on the out-of-sample. So although we still have lingering concerns over residual normality model 2 is still the preferred model so far.

```{r}
test %>%
  gather_predictions(model1, model2) %>%
  group_by(model) %>%
  summarise(MSE = mean((sales-pred)^2))
## # A tibble: 2 Ã? 2
##    model      MSE
##    <chr>    <dbl>
## 1 model1 11.34993
## 2 model2  3.75494
```


#### Incorporating interactions

In our previous analysis of the Advertising data, we concluded that both TV and radio seem to be associated with sales. The linear models that formed the basis for this conclusion assumed that the effect on sales of increasing one advertising medium is independent of the amount spent on the other media. For example, the linear model (Eq. 10) states that the average effect on sales of a one-unit increase in TV is always $Î²_1$, regardless of the amount spent on radio.

However, this simple model may be incorrect. Suppose that spending money on radio advertising actually increases the effectiveness of TV advertising, so that the slope term for TV should increase as radio increases. In this situation, given a fixed budget of $100,000, spending half on radio and half on TV may increase sales more than allocating the entire amount to either TV or to radio. In marketing, this is known as a synergy effect, and in statistics it is referred to as an interaction effect. One way of extending our model 2 to allow for interaction effects is to include a third predictor, called an interaction term, which is constructed by computing the product of $X_1, and $X_2$
 (here weâ€™ll drop the Newspaper variable). This results in the model
 
 $$
 Y = \beta_0 + \beta_1X_1 + \beta_2X_2+\beta_3X_1X_2+\epsilon...(11)
 $$

```{r}
# Option 1
model3 <- lm(sales~TV+radio+TV*radio, data=train)

# Option 2
model3 <- lm(sales ~ TV * radio, data = train)
```

##### Asessing coefficients

We see that all our coefficients are statistically significant. Now we can interpret this as an increase in TV advertising of $1,000 is associated with increased sales of ($\beta_1$+$\beta_3$*radio) * 1000 = 21 + 1*radio. And an increase in radio advertising of $1,000 will be associated with an increase in sales of 

```{r}
broom::tidy(model3)
```

##### Assessing model accuracy

We can compare our model results across all three models. We see that our adjusted $R^2$ and F-statistic are highest with model 3 and our RSE, AIC and BIC are the lowest with model 3; suggesting the model 3 outperforms the other models.

```{r}
list(model1 = broom::glance(model1),
     model2 = broom::glance(model2),
     model3 = broom::glance(model3)) 
```

##### Assessing our model visually

Visually assessing our residuals versus fitted values we see that model three does a better job with constant variance and, with the exception of the far left side, does not have any major signs of non-normality.

```{r}
# add model diagnostics to our training data

model3_results <- augment(model3, train) %>% 
  mutate(Model = "Model 3") %>% 
  rbind(model2_results)

ggplot(model3_results, aes(.fitted, .resid))+
  geom_ref_line(h=0)+
  geom_point()+
  geom_smooth(se=FALSE)+
  facet_wrap(~Model)+
  ggtitle("Residual vs Fitted")
```

As an alternative to the Q-Q plot we can also look at residual histograms for each model. Here we see that model 3 has a couple large left tail residuals. These are related to the left tail dip we saw in the above plots.

```{r}
ggplot(model3_results, aes(.resid))+
  geom_histogram(binwidth = .25)+
  facet_wrap(~Model, scales="free_x")+
  ggtitle("Residual Histogram")
```

These residuals can be tied back to when our model is trying to predict low levels of sales (< 10,000). If we remove these sales our residuals are more normally distributed. What does this mean? Basically our linear model does a good job predicting sales over 10,000 units based on TV and Radio advertising budgets; however, the performance deteriates when trying to predict sales less than 10,000 because our linear assumption does not hold for this segment of our data.

```{r}
model3_results %>%
  filter(sales > 10) %>%
  ggplot(aes(.resid)) +
  geom_histogram(binwidth = .25) +
  facet_wrap(~ Model, scales = "free_x") +
  ggtitle("Residual Histogram")
```

This can be corroborated by looking at the Cookâ€™s Distance and Leverage plots. Both of them highlight observations 3, 5, 47, 65, and 94 as the top 5 influential observations.

```{r}
par(mfrow=c(1, 2))

plot(model3, which = 4, id.n = 5)
plot(model3, which = 5, id.n = 5)
```

If we look at these observations we see that they all have low Sales levels.
```{r}
train[c(3,5,47,65,94),]
```

##### Making prediction

Again, to see how our models compare when making predictions on an out-of-sample data set weâ€™ll compare the MSEs across all our models. Here we see that model 3 has the lowest out-of-sample MSE, further supporting the case that it is the best model and has not overfit our data.

```{r}
test %>% 
  gather_predictions(model1, model2,model3) %>% 
  group_by(model) %>% 
  summarise(MSE = mean((sales-pred)^2))
```


#### Additional consideration
##### Qualitative predictors

In our discussion so far, we have assumed that all variables in our linear regression model are _quantitative_. But in practice, this is not necessarily the case; often some predictors are _qualitative_.

For example, the [Credit data set](http://www-bcf.usc.edu/~gareth/ISL/Credit.csv) records balance (average credit card debt for a number of individuals) as well as several quantitative predictors: age, cards (number of credit cards), education (years of education), income (in thousands of dollars), limit (credit limit), and rating (credit rating).

Suppose that we wish to investigate differences in credit card balance between males and females, ignoring the other variables for the moment. If a qualitative predictor (also known as a factor) only has two levels, or possible values, then incorporating it into a regression model is very simple. We simply create an indicator or dummy variable that takes on two possible numerical values. For example, based on the gender, we can create a new variable that takes the form

and use this variable as a predictor in the regression equation. This results in the model

Now can be interpreted as the average credit card balance among males, as the average credit card balance among females, and as the average difference in credit card balance between females and males. We can produce this model in R using the same syntax as we saw earlier:

```{r}
# credit <- read_csv("http://www-bcf.usc.edu/~gareth/ISL/Credit.csv")
credit <- ISLR::Credit
credit %>% head()
model4 <- lm(Balance ~ Gender, data=credit)
```

The results below suggest that females are estimated to carry $529.54$ in credit card debt where males carry $529.54 - 19.73 = 509.81$

```{r}
broom::tidy(model4)
```

The decision to code females as 0 and makes as 1 in is arbitrary, and has no effect on the regression fit, but does alter the interpretation of the coefficients. If we want to change the reference variable (the variable coded as 0) we can change the factor levels.

```{r}
# credit$Gender <- factor(credit$Gender, levels = c("Male", "Female"))

lm(Balance ~ Gender, data = credit) %>%
  tidy()
```

A similar process ensues for qualitative predictor categories with more than two levels. For instance, if we want to assess the impact that ethnicity has on credit balance we can run the following model. Ethnicity has three levels: _African American, Asian, Caucasian_. We interpret the coefficients much the same way. In this case we see that the estimated balance for the baseline, African American, is `$531.00`. It is estimated that the Asian category will have `$18.69` less debt than the African American category, and that the Caucasian category will have $12.50 less debt than the African American category. However, the p-values associated with the coefficient estimates for the two dummy variables are very large, suggesting no statistical evidence of a real difference in credit card balance between the ethnicities.

```{r}
lm(Balance ~ Ethnicity, data = credit) %>% 
  broom::tidy()
```

The process for assessing model accuracy, both numerically and visually along with measuring predictions can follow the same process as outlined for qualitative predictor variables.

##### Transformations

Linear regression models assume a linear relationship between the response and predictors. But in some cases, the true relationship between the response and the predictors may be non-linear. We can accomodate certain non-linear relationships by transforming variables (i.e. `log(x)`, `sqrt(x)`) or using polynomial regression.

As an example consider the `Auto` data set. We can see that a linear trend does not fit the relationship between mpg and horsepower.

```{r}
auto <- ISLR::Auto
ggplot(auto, aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = "lm")
```

We can try to address the non-linear relationship with a quadratic relationship, which takes the form of:

We can fit this model in R with:
```{r}
model5 <- lm(mpg ~ horsepower + I(horsepower^2), data = auto)
broom::tidy(model5)
```

Does this fit our relationship better? We can visualize it with:

```{r}
ggplot(auto, aes(horsepower, mpg)) +
  geom_point() +
  geom_smooth(method = "lm", formula = y ~ x + I(x^2))
```

##### Correlation of error terms

An important assumption of the linear regression model is that the error terms, , are uncorrelated. Correlated residuals frequently occur in the context of time series data, which consists of observations for which measurements are obtained at discrete points in time. In many cases, observations that are obtained at adjacent time points will have positively correlated errors. This will result in biased standard errors and incorrect inference of model results.

To illustrate, weâ€™ll create a model that uses the number of unemployed to predict personal consumption expenditures (using the `economics` data frame provided by `ggplot2`). The assumption is that as more people become unemployed personal consumption is likely to reduce. However, if we look at our modelâ€™s residuals we see that adjacent residuals tend to take on similar values. In fact, these residuals have a .998 autocorrelation. This is a clear violation of our assumption. Weâ€™ll learn how to deal with correlated residuals in future tutorials.

```{r}
df <- economics %>% 
  mutate(observation = 1:n())

model6 <- lm(pce ~ unemploy, data = df)

df %>% 
  add_residuals(model6) %>% 
  ggplot(aes(observation, resid))+
  geom_line()
```

##### Collinearity

_Collinearity_ refers to the situation in which two or more predictor variables are closely related to one another. The presence of collinearity can pose problems in the regression context, since it can be difficult to separate out the individual effects of collinear variables on the response. In fact, collinearity can cause predictor variables to appear as statistically insignificant when in fact they are significant.

For example, compares the coefficient estimates obtained from two separate multiple regression models. The first is a regression of balance on age and limit, and the second is a regression of balance on rating and limit. In the first regression, both age and limit are highly significant with very small p- values. In the second, the collinearity between limit and rating has caused the standard error for the limit coefficient estimate to increase by a factor of 12 and the p-value to increase to 0.701. In other words, the importance of the limit variable has been masked due to the presence of collinearity.

```{r}
model7 <- lm(Balance ~ Age + Limit, data = credit)
model8 <- lm(Balance ~ Rating + Limit, data = credit)

list(`Model1` = broom::tidy(model7),
     `Model2` = broom::tidy(model8))
```


A simple way to detect collinearity is to look at the correlation matrix of the predictors. An element of this matrix that is large in absolute value indicates a pair of highly correlated variables, and therefore a collinearity problem in the data. Unfortunately, not all collinearity problems can be detected by inspection of the correlation matrix: it is possible for collinear- ity to exist between three or more variables even if no pair of variables has a particularly high correlation. We call this situation multicollinearity.

Instead of inspecting the correlation matrix, a better way to assess multi- collinearity is to compute the _variance inflation factor (VIF)_. The VIF is the ratio of the variance of when fitting the full model divided by the variance of if fit on its own. The smallest possible value for VIF is 1, which indicates the complete absence of collinearity. Typically in practice there is a small amount of collinearity among the predictors. As a rule of thumb, a VIF value that exceeds 5 or 10 indicates a problematic amount of collinearity. The VIF for each variable can be computed using the formula

where is the from a regression of onto all of the other predictors. We can use the `vif` function from the `car` package to compute the VIF. As we see below model 7 is near the smallest possible VIF value where model 8 has obvious concerns.

```{r}
car::vif(model7)
##      Age    Limit 
## 1.010283 1.010283
car::vif(model8)
##   Rating    Limit 
## 160.4933 160.4933
```


### Linear model selection

It is often the case that some or many of the variables used in a multiple regression model are in fact not associated with the response variable. Including such irrelevant variables leads to unnecessary complexity in the resulting model. Unfortunately, manually filtering through and comparing regression models can be tedious. Luckily, several approaches exist for automatically performing feature selection or variable selection â€? that is, for identifying those variables that result in superior regression results. This tutorial will cover a traditional approach known as model selection.

#### tl;dr

This tutorial serves as an introduction to linear model selection and covers:

1. [Replication requrements: What you will need to reproduce the analysis in this tutorial](#replication_requirements)
2. Best subset selection: Finding the best combination of the p predictors
3. Stepwise selection: Computationally efficient approach for feature selection
4. Comparing models: Determining which model is best
5. Additional resources: additional resources to help you learn more

#### Replication requirements {#replication_requirements}

This tutorial primary leverages the `Hitters` data provided by the `ISLR` package. This is a data set that contains number of hits, homeruns, RBIs, and other information for 322 major league baseball players. Weâ€™ll also use `tidyverse` for some basic data manipulation and visualization. Most importantly, weâ€™ll use the `leaps` package to illustrate subset selection methods.

```{r}
# package
library(leaps) # model selection functions

# load data and remove rows with missing data
(hitters <- na.omit(ISLR::Hitters) %>% 
  as_tibble())
```

#### Best subset selection

To perform best subset selection, we fit a separate least squares regression for each possible combination of the _p_ predictors. That is, we fit all _p_ models that contain exactly one predictor, all \begin{pmatrix}
p  \\2  \\\end{pmatrix} = p(p-1)/2  models that contain exactly two predictors, and so forth. 

We then look at all of the resulting models, with the goal of identifying the one that is best. 

The three-stage process of perfoming best subset selection includes:

__step1__: LEt $M_0$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.

__Step2__: For k = 1,2,...,p:

- Fit all (p k) models that contain exactly k predictors
- Pick the best among these (p k) models, and call it $M_k$. Here best is defined as having the smallest RSS, or equivalently largest $R^2$

__Step3__: Select a single best model from among $<M_0, ,...,M_p$ using cross-validated prediction error, $C_p$, AIC, BIC or adjusted $R^2$. 

Let's illustrate with our data. We can perform a best subset search using `regsubsets` (part of the `leaps` library), which identifies the best model for a given number of _k_ predictors, where best is quantified using RSS. 

The syntax is the same as the lm function. By default, `regsubsets` only reports results up to the best eight-variable model. But the `nvmax` option can be used in order to return as many variables as are desired. Here we fit up to a 19-variable model.

```{r}
best_subset <- regsubsets(Salary ~ ., hitters, nvmax = 19)
```

The resubsets function returns a list-object with lots of information. Initially, we can use the `summary` command to assess the best set of variables for each model size. So, for a model with 1 variable we see that CRBI has an asterisk signalling that a regression model with _Salary ~ CRBI_ is the best single variable model. The best 2 variable model is _Salary ~ CRBI + Hits_. The best 3 variable model is _Salary ~ CRBI + Hits + PutOuts_. An so forth.

```{r}
summary(best_subset)
```

We can also get the RSS, $R^2$, adjusted $R^2$, $C_p$ and BIC from the results which help us to assess the best overall model; however, we will illustrate this in the [comparing models] section. First, let's look at how to perform stepwise selection.

#### Stepwise selection

For computational reasons, best subset selection cannot be applied when the number of _p_ predictor variable is large. est subset selection may also suffer from statistical problems when p is large. The larger the search space, the higher the chance of finding models that look good on the training data, even though they might not have any predictive power on future data. Thus an enormous search space can lead to overfitting and high variance of the coefficient estimates. For both of these reasons, stepwise methods, which explore a far more restricted set of models, are attractive alternatives to best subset selection.

##### Foward stepwise

_Forward stepwise_ selection begins with a model containing no predictors, and then adds predictors to the model, one-at-a-time, until all of the predictors are in the model. In particular, at each step the variable that gives the greatest additional improvement to the fit is added to the model.

The three-state process of performing foward stepwise selection includes:

__Step1__:Let $M_0$ denote the null model, which contains no predictors. This model simply predicts the sample mean for each observation.
 
__step2__: For $k=0,...,p-1$
- Consider all _p-k_ models that augument the predictors in $M_k$ with one additional predictor
- Choose the best among these _p-k_ models, and call it $M_{k+1}$. Here best is defined as having smallest RSS or highest $R^2$

__Step3:__ Select a single best model from among $M_0, ..., M_p$ using cross-validated prediction error $C_p$, AIC, BIC or adjusted $R^2$

We can perform backward stepwise using `regsubsets` by setting `method = "backward"`:

```{r}
backward <- regsubsets(Salary ~ ., hitters,
                       nvmax = 19, method = "backward")
```


#### Comparing models

So far, I've illustrated how to perform the best subset and stepwise procedures. Now let's discuss how to compare all the models that these approaches output in order to identify the _best_ model. That is, let's perform step 3 discussed in each of the 3-state processes outlined approaches.

1. We can indirectly estimate test error by making an adjustment to the training error to account for the bias due to ovefitting.

2. We can directly estimate the test error, using either a validation set approach or a cross-validation approach.

We consider both of these approaches below.

##### Indirectly estimating test error with $C_p$, AIC, BIC and adjusted $R^2$

When performing the best subset or stepwise approaches, the $M_0,...,M_p$ models are selected based on the fact that they minimize the training set mean square error (MSE). Because of this and the fact that using the training MSE and $R^2$ will bias our results we should not use these statstics to determine which of the $M_0,...,M_p$ models is "the best".

However, a number of technique for adjusting the training error for the model size are available. These approaches can be used to select among a set of models with different number of variables These include:

__Objective: Minimize__
- $C_p$: $C_p = \frac{1}{n}(RSS+2d\hat{\sigma})$
- $Akaike information criterion$: $AIC = \frac{1}{n\hat{\sigma}}(RSS+2d\hat{\sigma^2})$
- $Bayesian information criterion(BIC): BIC = \frac{1}{n}(RSS + log(n)d\hat{\sigma)}$

__Objective: Maximize__
- adjusted $R^2$: adj $R^2$ = 1 - $\frac{RSS/(n-d-1)}{TSS/(n-1)}$

where, $d$ is the number of predictors and $\sigma^2$ is an estimate of the variance of the error $\epsilon$ associated with each response measurement in a regression model. Each of these statistics adds a penalty to the training RSS in order to adjust for the fact that the training error tends to underestimate the test error. Clearly, the penalty increases as the number of predictors in the model increases.

Therefore, these statistics provide an unbiased estimate of test MSE. If we perform our model using a training vs. testing validation approach we can use these statistics to determine the preferred model. These statistics are contained in the output provided by the `regsubsets` function. Letâ€™s extract this information and plot them.

```{r}
# create training - testing data
set.seed(1)
sample <- sample(c(T,F), nrow(hitters), replace=T, prob = c(0.6, 0.4))
train <- hitters[sample,]
test <- hitters[!sample,]

# perform best subset selection
best_subset <- regsubsets(Salary~., train, nvmax = 19)
results <- summary(best_subset)

# extract and plot results
tibble(predictors = 1:19,
       adj_R2 = results$adjr2,
       Cp = results$cp,
       BIC = results$bic) %>% 
  gather(statistic, value, -predictors) %>% 
  ggplot(aes(predictors, value, color=statistic))+
  geom_line(show.legend = F)+
  geom_point(show.legend = F)+
  facet_wrap(~statistic, scales="free")
```

Here we see that our results identify slightly different models that are considered the best. The adjusted $R^2$ statistic suggests the 10 variable model is preferred, the BIC statistic suggests the 4 variable model, and the $C_p$ suggests the 8 variable model.

```{r}
which.max(results$adjr2)
which.min(results$bic)
min(results$bic)
which.min(results$cp)
```

We can compare the variables and coefficinets that these models include using the `coef` function.

```{r}
# 10 variable model
coef(best_subset, 10)

# 4 variable model
coef(best_subset, 4)

# 8 variable model
coef(best_subset, 8)
```

We could perform the same process using foward and backward stepwise selection and obtain even more options for optimal models. For example, if I assess the optimal $C_p$ for foward/backward stepwise, we see that they suggest that an 8 variable model minimizes the $C_p$ statsitic, similar to the best subset approach above.

```{r}
forward <- regsubsets(Salary ~ ., train, nvmax = 19, method = "forward")
backward <- regsubsets(Salary ~ ., train, nvmax = 19, method = "backward")

# which models minimize Cp?
which.min(summary(forward)$cp)
## [1] 8
which.min(summary(backward)$cp)
## [1] 8
```

However, when we assess these models we see that the 8 variable models include different predictors. Although, all models include AtBat, Hits, Walks, CWalks, and PutOuts, there are unique variables in each model.

```{r}
coef(best_subset, 8)
## (Intercept)       AtBat        Hits       Walks      CAtBat       CHits 
## -59.2371674  -1.4744877   6.6802515   4.4777879  -0.3203862   1.5160882 
##      CHmRun      CWalks     PutOuts 
##   1.1861142  -0.4714870   0.2748103
coef(forward, 8)
##  (Intercept)        AtBat         Hits        Walks        CRuns 
## -112.7724200   -2.1421859    8.8914064    5.4283843    0.8555089 
##         CRBI       CWalks      LeagueN      PutOuts 
##    0.4866528   -0.9672115   64.1628445    0.2767328
coef(backward, 8)
## (Intercept)       AtBat        Hits       Walks      CAtBat       CHits 
## -59.2371674  -1.4744877   6.6802515   4.4777879  -0.3203862   1.5160882 
##      CHmRun      CWalks     PutOuts 
##   1.1861142  -0.4714870   0.2748103
```

This highlights two important findings:

1. Different subsetting procedures (best subset vs. forward stepwise vs. backward stepwise) will likely identify different â€œbestâ€? models.
2. Different indirect error test estimate statistics ($C_p, AIC, BIC and adjusted R^2$) will likely identify different best models.

This is why it is important to always perform validation; that is to always estiamte the test error directly either by using a validation set or using cross-validation.

##### Directly estimating test error

We now compute the validation set error for the best model of each model size. We first make a model matrix from the test data. The model.matrix function is used in many regression packages for build- ing an â€œXâ€? matrix from data.

```{r}
test_m <- model.matrix(Salary~., data = test)
```

Now we can loop through each moel size (i.e., 1 variable, 2 variables, ... 19 variables) and extract the coefficient for the best model of that size, multiply them into the appropriate coluns of the test moel matrix to form the predictors and compute the test MSE. 

```{r}
# create empty vector to fill with erro values
validation_errors <- vector("double", length=19)

for(i in 1:19) {
  coef_x <- coef(best_subset, id = i)                     # extract coefficients for model size i
  pred_x <- test_m[ , names(coef_x)] %*% coef_x           # predict salary using matrix algebra
  validation_errors[i] <- mean((test$Salary - pred_x)^2)  # compute test error btwn actual & predicted salary
}

# plot validation errors
plot(validation_errors, type = "b")
```

Here, we actually see that the 1 variable model produced by the best subset approach produces the lowest test MSE! If we repeat this using a different random value seed, we will get a slightly different model that is the â€œbestâ€?. However, if you recall from the [Resampling Methods](http://uc-r.github.io/resampling_methods) tutorial, this is to be expected when using a training vs. testing validation approach.

```{r}
# create training - testing data
set.seed(5)
sample <- sample(c(TRUE, FALSE), nrow(hitters), replace = T, prob = c(0.6,0.4))
train <- hitters[sample, ]
test <- hitters[!sample, ]

# perform best subset selection
best_subset <- regsubsets(Salary ~ ., train, nvmax = 19)

# compute test validation errors
test_m <- model.matrix(Salary ~ ., data = test)
validation_errors <- vector("double", length = 19)

for(i in 1:19) {
  coef_x <- coef(best_subset, id = i)                     # extract coefficients for model size i
  pred_x <- test_m[ , names(coef_x)] %*% coef_x           # predict salary using matrix algebra
  validation_errors[i] <- mean((test$Salary - pred_x)^2)  # compute test error btwn actual & predicted salary
}

# plot validation errors
plot(validation_errors, type = "b")
```

A more robust approach is to perform cross validation. But before we do, letâ€™s turn our our approach above for computing test errors into a function. Our function pretty much mimics what we did above. The only complex part is how we extracted the formula used in the call to `regsubsets`. I suggest you work through this line-by-line to understand what each step is doing.

```{r}
predict.regsubsets <- function(object, newdata, id ,...) {
  form <- as.formula(object$call[[2]]) 
  mat <- model.matrix(form, newdata)
  coefi <- coef(object, id = id)
  xvars <- names(coefi)
  mat[, xvars] %*% coefi
  }
```

We now try to choose among the models of different sizes using k-fold cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the `k training sets`. First, we create a vector that allocates each observation to one of `k = 10` folds, and we create a matrix in which we will store the results.

```{r}
k <- 10
set.seed(1)
folds <- sample(1:k, nrow(hitters), replace = TRUE)
cv_errors <- matrix(NA, k, 19, dimnames = list(NULL, paste(1:19)))
cv_errors
```

Now we write a for loop that performs cross-validation. In the jth fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size, compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix `cv_errors`.

```{r}
for(j in 1:k) {
  
  # perform best subset on rows not equal to j
  best_subset <- regsubsets(Salary ~ ., hitters[folds != j, ], nvmax = 19)
  
  # perform cross-validation
  for( i in 1:19) {
    pred_x <- predict.regsubsets(best_subset, hitters[folds == j, ], id = i)
    cv_errors[j, i] <- mean((hitters$Salary[folds == j] - pred_x)^2)
    }
  }
```

This has given us a 10Ã?19 matrix, of which the (i,j)th element corresponds to the test MSE for the ith cross-validation fold for the best j-variable model. We use the `colMeans` function to average over the columns of this matrix in order to obtain a vector for which the jth element is the cross-validation error for the j-variable model.

```{r}
mean_cv_errors <- colMeans(cv_errors)
plot(mean_cv_errors, type = "b")
```

We see that our more robust cross-validation approach selects an 11-variable model. We can now perform best subset selection on the full data set in order to obtain the 11-variable model.

```{r}
final_best <- regsubsets(Salary ~ ., data = hitters , nvmax = 19)
coef(final_best, 11)
```


#### Additional resources

This will get you started with approaches for performing linear model selection; however, understand that there are other approaches for more sophisticated model selection procedures. The following resources will help you learn more:

- [An Introduction to Statistical Learning](http://www-bcf.usc.edu/~gareth/ISLR/)
- [Applied Predictive Modeling](http://appliedpredictivemodeling.com/)
- [Elements of Statistical Learning](https://statweb.stanford.edu/~tibs/ElemStatLearn/)

### Naive Bayes

The __Naive Bayes classifier__ is a simple probabilistic classifier which is based on Bayes theorem but with strong assumptions regarding independence. Historically, this technique became popular with application in emal filtering, spam detection and document categorization. 

Although it is often outperformed by other techniques, and desipite the naive design and oversimplified assumptions, this classifier can perform well in many complex real-world problems. And since it is a resource efficient algorithm that is fast and scales well, it is definitely a machine learning algorithms to have in your toolkit.

#### tl;dr
This tutorial serves as an introduction to naive Bayes classifier and covers:

1. [Replication requirment](#Naive_Bayes_RR): what you will need to reproduce the analysis in this tutorial.
2. [A naive overview](): A closer look behind the naive Bayes classifier and its pros and cons
3. [caret](): Implementing with the `caret` package.
4. [H2O](): Implementing with the `h2o` package.
5. [Learning more]() Where to go from here.

#### Replication requirements {#Naive_Bayes_RR}

This tutorial leverages the following packages.

```{r, warning=FALSE}
library(rsample) # data splitting
library(dplyr)   # data transformation
library(ggplot2) # data visualization
library(caret)   # implementing with caret
library(h2o)     # implemeting with h2o
```

To illustrate the naive Bayes classifier, we will use the attrition data that has been included in the `rsample` package. The goal is to predict employee attrition.

```{r}
# convert some numeric variables to factors

attrition <- attrition %>% 
  mutate(
    JobLevel = factor(JobLevel),
    StockOptionLevel = factor(StockOptionLevel),
    TrainingTimesLastYear = factor(TrainingTimesLastYear)
  )

# Create training (70%) and test (30%) sets for the attrition data.
# Use set.seed for reproducibility
set.seed(123)
split <- initial_split(attrition, prop = .7, strata = "Attrition")
train <- training(split)
test  <- testing(split)

# distribution of Attrition rates across train & test set
table(train$Attrition) %>% prop.table()
## 
##       No      Yes 
## 0.838835 0.161165
table(test$Attrition) %>% prop.table()
## 
##        No       Yes 
## 0.8386364 0.1613636
```

#### A Naive overview
##### The idea

The naive Bayes classifier is founded on Bayesian probability, which originated from [Reverend Thomas Bayes](https://en.wikipedia.org/wiki/Thomas_Bayes). Bayesian probability incorporates the concept of conditional probability, the probabilty of event A given that event B has occurred [denoted as $P(A|B)$. In the context of our attrition data, we are seeking the probability of an employee belonging to attrition class $C_k$ (where $C_{yes} attrition$ and $C_{no}=non-attrition$) given that its predictor values are $x_1, x_2,...,x_p$. This can be written as $P(C_k|x_1, ...,x_p)$.

The Bayesian formula for claculating the probability is 

$$
P(C_k|X) = \frac{P(C_k) P(X|C_k)}{P(X)}
$$

where:

- $P(C_k)$ is the _prior_ probability of the outcome. Essentially, based on the historical data, what is the probability of an employee attriting or not. As we saw in the above section preparing our training and test sets, our prior probability of an employee attriting was about 16% and the probability of not attriting was about 84%.
- $P(X)$ is the probability of the predictor variables (same as $P(C_k|x_1,...,x_p)$.Essentially, based on the historical data, what is the probability of each observed combination of predictor variables. When new data comes in, this becomes our _evidence_.
- $P(X|C_k)$ is the _conditional probability or likelihood_. Essentially, for each class of the response variable (i.e. attrit or not attrit), what is the probability of observing the predictor values. 
- $P(C_k|X)$ is called our _posterior probability_. By combining our observed information, we are updating our a priori information on probabilities to compute a posterior probability that an observation has class $C_k$.

We can re-write Eq.(1) in plaini english as:

$$
posterior = \frac{prior*likelihood}{evidence}
$$

Although Eq. (1) has simplistic beauty on its surface, it becomes complex and interactable as the number of predictor variables grow. In fact, to compute the posterior probability for a response variable with _m_ classes and a data set with _p_ predictors, Eq. (1) would require $m^p$ probabilities computed. Sor for our attrition data, we have 2 classes (attrition vs. non-attrition) and 31 variables, requiring 2,147,483,648 probabilities computed.

##### The simplified classifier

Consequently, the naive Bayes classifier makes a simplifying assumption (hence the name) to allow the computation to scale. With naive Bayes, we assume that the predictor variables are _conditionally independent_ of one another given the response value. This is an extremely strong assumption. We can see quickly that our attrition data violates this as we have several moderately to strongly correlated variables.

```{r}
train %>%
  filter(Attrition == "Yes") %>% 
  select_if(is.numeric) %>% 
  cor() %>% 
  corrplot::corrplot()
```

However, by making this assumption, we can simplify our calculation such that the posterior probability is simply the product of the probability distribution for each individual variable  conditioned on the response category (Eq. 2). Now we are only required to compute $m Ã? p$  probabilities (this equates to 62 probabilities for our data set), a far more managable task.

$$
P(C_k|X) = \prod_{i=1}^{n} P(x_i|C_k)
$$

For categorical variables, this computation is quite simple as you just use the frequencies from the data. However, when including continuous predictor variables often an assumption of normality is made so that we can use the probability from the variableâ€™s probability density function. If we pick a handful of our numeric features we quickly see assumption of normality is not always fair.

```{r}
train %>% 
  dplyr::select(Age,DailyRate,DistanceFromHome, HourlyRate, MonthlyIncome, MonthlyRate) %>% 
  gather(metric, value) %>% 
  ggplot(aes(value, fill=metric))+
  geom_density(show.legend = FALSE)+
  facet_wrap(~metric, scales="free")
```

Granted, some numeric features may be normalized with a Box-Cox transformation; however, as you will see in this tutorial we can also use non-parametric kernel density estimators to try get a more accurate representation of continuous variable probabilities. Ultimately, transforming the distributions and selecting an estimator is part of the modeling development and tuning process.

##### Laplace Smoother

One additional issue to be aware of - since naÃ¯ve Bayes uses the product of feature probabilities conditioned on each class, we run into a serious problem when new data includes a feature value that never occurs for one or more levels of a response class. What results is $P(x_i|C_k)=0$ for this individual feature adn this zero will ripple through the entire multiplication of all features and this zero will ripple through the entire multiplication of all features and will always force the posterior probability to be zero for that class.

A solution to this problem involves using the __Laplace smooother__. The Laplace smooother adds a small number to each of the counts in the frequencies for each feature, which ensures that each feature has a nonzero probability of occuring for each class. Typically, a value of one to two for the Laplace smoother is sufficient, but this is a tuning parameter to incorporate and optimize with cross validation.

##### Advantage and Shortcomins

The naÃ¯ve Bayes classifier is simple (both intuitively and computationally), fast, performs well with small amounts of training data, and scales well to large data sets. The greatest weakness of the naÃ¯ve Bayes classifier is that it relies on an often-faulty assumption of equally important and independent features which results in biased posterior probabilities. Although this assumption is rarely met, in practice, this algorithm works surprisingly well. This is primarily because what is usually needed is not a propensity (exact posterior probability) for each record that is accurate in absolute terms but just a reasonably accurate rank ordering of propensities.

For example, we may not care about the exact posterior probability of attrition, we just want to know for a given observation, is the posterior probability of attriting larger than not attriting. Even when the assumption is violated, the rank ordering of the recordsâ€? propensities is typically preserved. Consequentely, naÃ¯ve Bayes is often a surprisingly accurate algorithm; however, on average it rarely can compete with the accuracy of advanced tree-based methods (random forests & gradient boosting machines) but is definitely worth having in your toolkit.

#### Implementation

There are several packages to apply naive Bayes (i.e., `e1071`, `klaR`, `naivebayes`, `bnclassify`). This tutorial demonstrates using the `caret` and `h2o` packages. `caret` allows us to use different naive Bayes packages above but in a common framework, and also allows for easy cross validation and tuning. `h2o` allows us to perform naive Bayes in a powerful and scalable architecture.

##### `caret`

First, we apply a naive Bayes model with 10-fold cross validation, which gets 83% accuracy. Consider about 83% of our obsevations in our training set do not attrit, our overall accuracy is not better than 
if we just predicted "No" attrition for every observation.

```{r message=FALSE, warning=FALSE}
library(caret)

# create response and feature data
features <- setdiff(names(train), "Attrition")
features

x <- train[, features]
y <- train$Attrition

# set up 10-fold cross validation procedure
train_control <- trainControl(
  method = "cv",
  number = 10
)

# train model
nb.m1 <- caret::train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control
)
# results 
confusionMatrix(nb.m1)
```

We can turn the few hyperparameters that a naive Bayes model has.

- `usekernel`: parameter allows us to use a kernel density estimate for continuous variables versus a guassian density estimate,
- `adjust`:  allows us to adjust the bandwidth of the kernel density (larger numbers mean more flexible density estimate),
- `fL`: allows us to incorporate the Laplace smoother.

If we just tuned our modelwith the above parameters, we are able to lift our accuracy to 85%; however, by incorporating some preprocessing of our features (normalize with Box Cox, standardize with center-scaling, and reducing with PCA), we actually get about another 2% lift in our accuracy.

```{r eval=FALSE}
# set up tuning grid
search_grid <- expand.grid(
  usekernel = c(TRUE, FALSE),
  fL = 0:5,
  adjust = seq(0, 5, by = 1)
)

# train model
nb.m2 <- train(
  x = x,
  y = y,
  method = "nb",
  trControl = train_control,
  tuneGrid = search_grid,
  preProc = c("BoxCox", "center", "scale", "pca")
  )

# top 5 model
nb.m2$results %>% 
  top_n(5, wt=Accuracy) %>% 
  arrange(Desc(Accuracy))
##   usekernel fL adjust  Accuracy     Kappa AccuracySD   KappaSD
## 1      TRUE  1      3 0.8737864 0.4435322 0.02858175 0.1262286
## 2      TRUE  0      2 0.8689320 0.4386202 0.02903618 0.1155707
## 3      TRUE  2      3 0.8689320 0.4750282 0.02830559 0.0970368
## 4      TRUE  2      4 0.8689320 0.4008608 0.02432572 0.1234943
## 5      TRUE  4      5 0.8689320 0.4439767 0.02867321 0.1354681


# plot search grid results
plot(nb.m2)
```

```{r eval=FALSE}
# results for best model
confusionMatrix(nb.m2)

## Cross-Validated (10 fold) Confusion Matrix 
## 
## (entries are percentual average cell counts across resamples)
##  
##           Reference
## Prediction   No  Yes
##        No  80.8  9.5
##        Yes  3.1  6.6
##                             
##  Accuracy (average) : 0.8738
```


We can assess the accuracy on our final holdout test set. Its obvious that our model is not capturing a large percentage of our actual attritions (illustrated by our low specificity).

```{r eval=FALSE}
pred <- predict(nb.m2, newdata = test)
confusionMatrix(pred, test$Attrition)
## Confusion Matrix and Statistics
## 
##           Reference
## Prediction  No Yes
##        No  349  41
##        Yes  20  30
##                                           
##                Accuracy : 0.8614          
##                  95% CI : (0.8255, 0.8923)
##     No Information Rate : 0.8386          
##     P-Value [Acc > NIR] : 0.10756         
##                                           
##                   Kappa : 0.4183          
##  Mcnemar's Test P-Value : 0.01045         
##                                           
##             Sensitivity : 0.9458          
##             Specificity : 0.4225          
##          Pos Pred Value : 0.8949          
##          Neg Pred Value : 0.6000          
##              Prevalence : 0.8386          
##          Detection Rate : 0.7932          
##    Detection Prevalence : 0.8864          
##       Balanced Accuracy : 0.6842          
##                                           
##        'Positive' Class : No              
## 
```

#### `h2o`

Lets go ahead and start up `h2o`:

```{r}
# start up h2o
library(h2o)
h2o.no_progress()
h2o.init()
```

We can compute a naÃ¯ve Bayes model in `h2o` with `h2o.naiveBayes`. Here we use the default model with no Laplace smoother. We experience similar results on our training cross validation as we did using `caret`.

```{r}
# create feature names
y <- "Attrition"
x <- setdiff(names(train), y)

library(magrittr)
# h2o cannot ingest ordered factors
train.h2o <- train %>%
  mutate_if(is.factor, factor, ordered=FALSE) %>% 
  as.h2o()

# train model
nb.h2o <- h2o.naiveBayes(
  x = x,
  y = y,
  training_frame = train.h2o,
  nfolds = 10,
  laplace = 0
)

# assess results
h2o.confusionMatrix(nb.h2o)
## Confusion Matrix (vertical: actual; across: predicted)  for max f1 @ threshold = 0.751627773135859:
##         No Yes    Error       Rate
## No     812  52 0.060185    =52/864
## Yes     77  89 0.463855    =77/166
## Totals 889 141 0.125243  =129/1030
```

We can also do some feature preprocessing as we did with caret and tune the Laplace smoother using `h2o.grid`. We donâ€™t see much improvement.

```{r}
# do a little preprocessing
preprocess <- preProcess(train, method = c("BoxCox", "center", "scale", "pca"))
train_pp <- predict(preprocess, train)
test_pp <- predict(preprocess, test)

# convert to h2o objects
train_pp.h2o <- train_pp %>% 
  mutate_if(is.factor, factor, ordered=FALSE) %>% 
  as.h2o()

test_pp.h2o <- test_pp %>% 
  mutate_if(is.factor, factor, ordered=FALSE) %>% 
  as.h2o()

# get new feature names -> PCA preprocessing reduced and changed some fe
x <- setdiff(names(train_pp), "Attrition")
y <- "Attrition"

# create tuning grid
hyper_params <- list(
  laplace = seq(0,5, by = 0.5)
)

# build grid search
grid <- h2o.grid(
  algorithm = "naivebayes",
  grid_id = "nb_grid",
  x = x,
  y = y,
  training_frame = train_pp.h2o,
  nfolds = 10,
  hyper_params = hyper_params
)

# sort the grid models by mse
sorted_grid <- h2o.getGrid("nb_grid", sort_by = "accuracy",
                           decreasing = TRUE)
sorted_grid
## H2O Grid Details
## ================
## 
## Grid ID: nb_grid 
## Used hyper parameters: 
##   -  laplace 
## Number of models: 11 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by decreasing accuracy
##    laplace        model_ids           accuracy
## 1      2.5  nb_grid_model_5 0.8572815533980582
## 2      5.0 nb_grid_model_10 0.8572815533980582
## 3      3.0  nb_grid_model_6 0.8533980582524272
## 4      2.0  nb_grid_model_4 0.8504854368932039
## 5      1.5  nb_grid_model_3 0.8466019417475728
## 6      4.5  nb_grid_model_9 0.8407766990291262
## 7      0.5  nb_grid_model_1 0.8398058252427184
## 8      4.0  nb_grid_model_8 0.8339805825242719
## 9      3.5  nb_grid_model_7 0.8281553398058252
## 10     1.0  nb_grid_model_2 0.8194174757281554
## 11     0.0  nb_grid_model_0 0.8009708737864077

# grab top model id
best_h2o_model <- sorted_grid@model_ids[[1]]
best_model <- h2o.getModel(best_h2o_model)

# confusion matrix of best model
h2o.confusionMatrix(best_model)

# ROC curve
auc <- h2o.auc(best_model, xval = TRUE)
fpr <- h2o.performance(best_model, xval = TRUE) %>% h2o.fpr() %>% .[['fpr']]
tpr <- h2o.performance(best_model, xval = TRUE) %>% h2o.tpr() %>% .[['tpr']]

data.frame(fpr = fpr,
           tpr = tpr) %>% 
  ggplot(aes(fpr, tpr))+
  geom_line()+
  ggtitle(sprintf('AUC: %f', auc))
```

Once weâ€™ve identified the optimal model we can assess on our test set.

```{r}
# evaluate on test set
h2o.performance(best_model, newdata = test_pp.h2o)
## H2OBinomialMetrics: naivebayes
```

```{r}
# predict new data
h2o.predict(nb.h2o, newdata = test_pp.h2o)
##   predict        No         Yes
## 1      No 0.9851309 0.014869060
## 2      No 0.9109746 0.089025413
## 3      No 0.9947524 0.005247592
## 4      No 0.9694504 0.030549563
## 5      No 0.9683716 0.031628419
## 6      No 0.9836910 0.016308996
## 
## [440 rows x 3 columns]

# shut down h2o
h2o.shutdown(prompt = FALSE)
## [1] TRUE
```

#### Learning more
Although we did not see much improvement over the baseline response class proportions in this example, the naÃ¯ve Bayes classifier is often hard to beat in terms of CPU and memory consumption as shown by Huang, J. (2003), and in certain cases its performance can be very close to more complicated and slower techniques. Consquently, its a solid technique to have in your toolkit. If you want to dig deeper into this classifier, I would start with:

- [Andrew Mooreâ€™s tutorials](http://www.cs.cmu.edu/~./awm/tutorials/naive.html)
- [Naive Bayes classifiers by Kevin Murphy](https://datajobsboard.com/wp-content/uploads/2017/01/Naive-Bayes-Kevin-Murphy.pdf)
- [Data Mining and Predictive Analytics, Ch. 14](https://www.amazon.com/Mining-Predictive-Analytics-Daniel-Chantal/dp/8126559136/ref=sr_1_1?ie=UTF8&qid=1524231609&sr=8-1&keywords=data+mining+and+predictive+analytics+2nd+edition+%2C+by+larose)

### Logistic regression

Logistic regression (aka logit regressionor logit model) was developed by statistician David Cox in 1958 and is a regression model where the response variable Y is categorical. Logistic regression allows us to estimate the probability of a categorical response based on one or more predictor variables ($X$). It allows one to say that the presence of a predictor increases (or decreases) the probability of a given outcome by a specific percentage. This tutorial covers the case when Y is binary â€? that is, where it can take only two values, â€?0â€? and â€?1â€?, which represent outcomes such as pass/fail, win/lose, alive/dead or healthy/sick. Cases where the dependent variable has more than two outcome categories may be analysed with multinomial logistic regression, or, if the multiple categories are ordered, in ordinal logistic regression. However, _discriminant analysis_ has become a popular method for multi-class classification so our next tutorial will focus on that technique for those instances.

#### tl;dr

This turorial serves as an introudction to logistic regression and covers:

1. [Replication requirements](#log-pre-requisite): What youâ€™ll need to reproduce the analysis in this tutorial
2. [Why logistic regression](#why-logit): Why use logistic regression?
3. [Preparing our data](#logit-data): Prepare our data for modeling
4. [Simple Logistic regression](): Predicting the probability of response Y with a single predictor variable X
5. [Multiple Logistic regression](): Predicting the probability of response Y with multiple predictor variables $X_1, X_2,...X_p$
6. [Model evaluation & diagnostics](): How well does the model fit the data? Which predictors are most important? Are the predictions accurate?

#### Replication requirements {#log-pre-requisite}

This tutorial primarily leverages the `Default` data provided by the `ISLR` package. This is simulated data set containing information on ten thousand customers such as whether the customer defaulted, is a student,  the average balance carried by the customer and the income of the customer. Weâ€™ll also use a few packages that provide data manipulation, visualization, pipeline modeling functions, and model output tidying functions.

```{r logistic-prerequisite}
library(tidyverse)
library(modelr)
library(broom)

# load data
(default <- as_tibble(ISLR::Default))
```

#### Why Logistic regression {#why-logit}

Linear regression is not approproate in the case of qualitative response. Why not? Suppose that we are trying to predict the medical condition of a patient in the emergency room on the basis of her symptoms. In this simplified example, there are three possible diagnoses: _stroke_, _drug overdose_, and _epileptic seizure_. We could consider encoding these values as a quantitative response variable, `Y` , as follows:

$$
Y = 1, if stroke; \\
    2, if drug overdose; \\
    3, if epileptic seizure
$$

Using this coding, least squares could be used to fit a linear regression model to predict Y on the basis of a set of predictors $X_1,..., X_p$. . Unfortunately, this coding implies an ordering on the outcomes, putting drug overdose in between stroke and epileptic seizure, and insisting that the difference between stroke and drug overdose is the same as the difference between drug overdose and epileptic seizure. In practice there is no particular reason that this needs to be the case. For instance, one could choose an equally reasonable coding,

$$
Y = 1, if epileptic seizure;
    2, if stroke;
    3  if drug overdose 
$$

which would imply a totally different relationship among the three conditions. Each of these codings would produce fundamentally different linear models that would ultimately lead to different sets of predictions on test observations.

More relevant to our data, if we are trying to classify a customer as a high- vs. low-risk defaulter based on their balance we could use linear regression; however, the left figure below illustrates how linear regression would predict the probability of defaulting. Unfortunately, for balances close to zero we predict a negative probability of defaulting; if we were to predict for very large balances, we would get values bigger than 1. These predictions are not sensible, since of course the true probability of defaulting, regardless of credit card balance, must fall between 0 and 1.

```{r}
default %>% colnames()

lm <- lm(default ~ balance, data=default)
glm <- glm(default ~ balance, data=default, family = "binomial")
```


To avoid this problem, we must model `p(X)` using a function that gives outputs between 0 and 1 for all values of `X`. Many functions meet this description. In logistic regression, we use the logistic function, which is defined in Eq. 1 and illustrated in the right figure above.

$$
p(x) = \frac{e^{\beta_o + \beta_1X}}{1+e^{\beta_o + \beta_1X}
$$

#### Preparing our data {#logit-data}

As in the regression tutorial, we will split our data into a training (60%) and testing (40%) data sets so we can assess how well our model performs on an out-of-sample data set.

```{r}
set.seed(123)
sample <- sample(c(TRUE, FALSE), nrow(default), replace = T, prob = c(0.6,0.4))
train <- default[sample, ]
test <- default[!sample, ]
```

#### Simple logistic regression 

We will fit a logistic regression model in order to predict the probability of a customer defaulting based on the average balance carried by the customer. The `glm` function fits generalized linear models, a class of models that includes logistic regression. The syntax of the `glm` function is similar to that of `lm`, except that we must pass the argument `family = binomial` in order to tell R to run a logistic regression rather than some other type of generalized linear model.

```{r}
model1 <- glm(default ~ balance, family = "binomial",
             data = train)
```

In the background the `glm`, uses _maximum likelihood_ to fit the model. The basic intuition behind using maximum likelihood to fit a logistic regression model is as follows: we seek estimates for $\beta_0$ and $\beta_1$ such that the predicted probability $p(x_i)$ of default for each individual, using Eq.1, corresponds as closely as possible to the individual's observed default status. In other words, we try to find $\beta_0$ and $\beta_1$ 1
  such that plugging these estimates into the model for $p(X)$, given in Eq. 1, yields a number close to one for all individuals who defaulted, and a number close to zero for all individuals who did not. This intuition can be formalized using a mathematical equation called a likelihood function:

$$
l(\beta_0, \beta_1) = \Pi_{i:y_i-1} p(x_i) \Pi_{i':y_i'=0}(1-p(x_i'))
$$

The estimates $\beta_0 and \beta_1$ are chosen to _maximize_ this likelihood function. Maximum likelihood is very general approach that is used to fit many of the non-linear models that we will examine in future totorial. What resuls is an S-shaped probability curve illustrated below (note that to plot the logistic regression fit line, we need to convert our response variable to a $[0,1]$ binary coded variable)

```{r}
default %>%
  mutate(prob = ifelse(default == "Yes", 1, 0)) %>%
  ggplot(aes(balance, prob)) +
  geom_point(alpha = .15) +
  geom_smooth(method = "glm", method.args = list(family = "binomial")) +
  ggtitle("Logistic regression model fit") +
  xlab("Balance") +
  ylab("Probability of Default")
```

Similar to linear regression we can assess the model using `summary` or `glance`. Note that the coefficient output format is similar to what we saw in linear regression; however, the goodness-of-fit details at the bottom of `summary` differ. Weâ€™ll get into this more later but just note that you see the word deviance. Deviance is analogous to the sum of squares calculations in linear regression and is a measure of the lack of fit to the data in a logistic regression model. The null deviance represents the difference between a model with only the intercept (which means â€œno predictorsâ€?) and a saturated model (a model with a theoretically perfect fit). The goal is for the model deviance (noted as _Residual deviance_) to be lower; smaller values indicate better fit. In this respect, the null model provides a baseline upon which to compare predictor models.

```{r}
summary(model1)
## 
## Call:
## glm(formula = default ~ balance, family = "binomial", data = train)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2905  -0.1395  -0.0528  -0.0189   3.3346  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(>|z|)    
## (Intercept) -1.101e+01  4.887e-01  -22.52   <2e-16 ***
## balance      5.669e-03  2.949e-04   19.22   <2e-16 ***
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## (Dispersion parameter for binomial family taken to be 1)
## 
##     Null deviance: 1723.03  on 6046  degrees of freedom
## Residual deviance:  908.69  on 6045  degrees of freedom
## AIC: 912.69
## 
## Number of Fisher Scoring iterations: 8
```

##### Asessing coefficients

The below table shows the coefficient estimates and related information that result from fitting a logistic regression model in order to predict the probability of default = Yes using balance. Bear in mind that the coefficient estimates from logistic regression characterize the relationship between the predictor and response variable on a _log-odds_ scale (see Ch. 3 of ISLR1 for more details). Thus, we see that $\hatÎ²_1=0.0057$ ; this indicates that an increase in balance is associated with an increase in the probability of default. To be precise, a one-unit increase in balance is associated with an increase in the log odds of default by 0.0057 units.

```{r}
broom::tidy(model1)
##          term      estimate   std.error statistic       p.value
## 1 (Intercept) -11.006277528 0.488739437 -22.51972 2.660162e-112
## 2     balance   0.005668817 0.000294946  19.21985  2.525157e-82
```

We can further interpret the _balance_ coefficient as - for every one dollar increase in monthly balance carried, the odds of the customer defaulting increases by a factor of 1.0057.

```{r}
exp(coef(model1))
```

Many aspects of the coefficient output are similar to those discussed in the linear regression output. For example, we measure the confidence intervals and accuracy of the coefficient estimates by computing their standard errors. For instance, $\hatÎ²_1$ has a p-value < $2e-16$ and the probability of defaulting. We can also use the standard errors to get confidence intervals as we did in the linear regression tutorial:

```{r}
confint(model1)
```

##### Making predictions

Once the coefficients have benn estimated, it is a simple matter to comupte the probability of default for any given credit card balance.  Mathematically, using the coefficient estimates from our model we predict that the default probability for an individual with a balance of $1,000 is less than 0.5%

$$
\hat p(x) = \frac{e^{\hat \beta_0 + \hat \beta_1 X}}{1 +e ^{\hat \beta_0 + \hat \beta_1 X}} = 0.004785

, where \beta_o = -11.0063, 0.0057
$$

We can predict the probability of defaulting in R using the `predict` function (be sure to include `type = "response"`). Here we compare the probability of defaulting based on balances of `$1000 and $2000`. As you can see as the balance moves from `$1000 to $2000` the probability of defaulting increases signficantly, from 0.5% to 58%!

```{r}
predict(model1, 
        data.frame(balance = c(1000, 2000)), 
        type = "response")
##           1           2 
## 0.004785057 0.582089269
```

One can also use qualitative predictors with the logistic regression model. As an example, we can fit a model that uses the `student` variable.

```{r}
model2 <- glm(default ~ student, family = "binomial", data = train)

tidy(model2)
##          term   estimate  std.error statistic     p.value
## 1 (Intercept) -3.5534091 0.09336545 -38.05914 0.000000000
## 2  studentYes  0.4413379 0.14927208   2.95660 0.003110511
```

The coefficient associated with `student = Yes` is positive, and the associated p-value is statistically significant. This indicates that students tend to have higher default probabilities than non-students. In fact, this model suggests that a student has nearly twice the odds of defaulting than non-students. However, in the next section weâ€™ll see why.

$$
p(default = Yes|student = Yes) = 
\frac{e^{-3.55+0.44*1}}{1+e^{-3.55+0.44*1}}=0.0426 \\

p(default = Yes|student = No) = 
\frac{e^{-3.55+0.44*0}}{1+e^{-3.55+0.44*0}}=0.0426

$$


```{r}
predict(model2, data.frame(student = factor(c("Yes", "No"))), type = "response")
##          1          2 
## 0.04261206 0.02783019
```

#### Multiple logistic regression

We can also extend our model as seen in Eq. 1 so that we can predict a binary response using multiple predictors where $X = (X_1,..., X_p$) are p predictors:

$$
p(X) = \frac{e^{\beta_0+\beta_1X_1+...+\beta_pX_p}}{1+e^{\beta_0+\beta_1X_1+...+\beta_pX_p}}
$$

Letâ€™s go ahead and fit a model that predicts the probability of _default_ based on the balance, income (in thousands of dollars), and student status variables. There is a surprising result here. The p-values associated with `balance` and `student=Yes` status are very small, indicating that each of these variables is associated with the probability of defaulting. However, the coefficient for the student variable is negative, indicating that students are less likely to default than non-students. In contrast, the coefficient for the student variable in model 2, where we predicted the probability of default based only on student status, indicated that students have a greater probability of defaulting. What gives?

```{r}
model3 <- glm(default ~ balance + income + student, family = "binomial", data = train)
tidy(model3)
```

The right-hand panel of the figure below provides an explanation for this discrepancy. The variables student and balance are correlated. Students tend to hold higher levels of debt, which is in turn associated with higher probability of default. In other words, students are more likely to have large credit card balances, which, as we know from the left-hand panel of the below figure, tend to be associated with high default rates. Thus, even though an individual student with a given credit card balance will tend to have a lower probability of default than a non-student with the same credit card balance, the fact that students on the whole tend to have higher credit card balances means that overall, students tend to default at a higher rate than non-students. This is an important distinction for a credit card company that is trying to determine to whom they should offer credit. A student is riskier than a non-student if no information about the studentâ€™s credit card balance is available. However, that student is less risky than a non-student with the same credit card balance!

```{r include=FALSE}
model3$model

predict(model3, 
        data.frame(balance = c(1000, 2000),
                   student = c("Yes", "No"),
                   income=1000), 
        type = "response")

pred <- predict(model3, test,
        type = "response")

pred_results <- test %>% 
  bind_cols(pred = pred)

pred_results %>% 
  ggplot(aes(balance, pred, col=student, group=student))+
  geom_point(size=0.5)+
  geom_line()
```

This simple example illustrates the dangers and subtleties associated with performing regressions involving only a single predictor when other predictors may also be relevant. The results obtained using one predictor may be quite different from those obtained using multiple predictors, especially when there is correlation among the predictors. This phenomenon is known as _confounding_.

In the case of multiple predictor variables sometimes we want to understand which variable is the most influential in predicting the response (`Y`) variable. We can do this with `varImp` from the `caret` package. Here, we see that _balance_ is the most important by a large margin whereas student status is less important followed by income (which was found to be insignificanbt anyways (p = .64)).

```{r}
caret::varImp(model3)
```

As before, we can easily make predictions with this model. For example, a student with a credit card balance of $1,500$ and an income of $40,000$ has an estimated probability of default of

$$
p(X) = p(X) = \frac{e^{-10.907+0.0591*1,500-0.00001*50-0.809*1}}{1+e^{-10.907+0.0591*1,500-0.00001*50-0.809*1}} = 0.054

$$

A non-student with the same balance and income has an estimated probability of default of 


$$
p(X) = p(X) = \frac{e^{-10.907+0.0591*1,500-0.00001*50-0.809*0}}{1+e^{-10.907+0.0591*1,500-0.00001*50-0.809*}} = 0.114
$$

```{r}
new.df <- tibble(balance = 1500, income = 40, student = c("Yes", "No"))
predict(model3, new.df, type = "response")
##          1          2 
## 0.05437124 0.11440288
```

Thus, we see that for the given balance and income (although income is insignificant) a student has about half the probability of defaulting than a non-student.

#### Mode evaluation & diagnostics

So far three logistic regression models have been built and the coefficients have been examined. However, some critical questions remain. Are the models any good? How well does the model fit the data? And how accurate are the predictions on an out-of-sample data set?

##### Goodness of fit

In the linear regression tutorial we saw how the F-statistic, $R^2$ and adjusted $R^2$ and residual diagnostics inform us of how good the model fits the data. Here, we will look at a few ways to assess the goodness of fit ofr our logit models.

##### Likelihood ratio test

First, we can use a _Likelihood Ratio Test_ to assess if our models are improving the fit. Adding predictor variables to a model will almost always improve the model fit (i.e. increase the log likelihood and reduce the model deviance compared to the null deviance), but it is necessary to test whether the observed difference in model fit is statistically significant. We can use _anova_ to perform this test. The results indicate that, compared to `model1`, `model3` reduces the residual deviance by over 13 (remember, a goal of logistic regression is to find a model that minimizes deviance residuals). More imporantly, this improvement is statisticallly significant at p = 0.001. This suggests that `model3` does provide an improved model fit.

```{r}
anova(model1, model3, test="Chisq")
```

##### pseudo $R^2$

Unlike linear regression with ordinary least squares estimation, there is no $R^2$ statistic which explains the proportion of variance in the dependent variable that is explained by the predictors. However, there are a number of pseudo $R^2$ metrics that could be of value. Most notable is [McFadden's $R^2$](http://stats.stackexchange.com/questions/82105/mcfaddens-pseudo-r2-interpretation), which is defined as 

$$
1 - \frac{ln(LM_1)}{ln(LM_0)}
$$

where $ln(LM_1)$ is the log likelihood value for the fitted model and ln(LM_0) is the log likelohood for the null model with only an intercept as a predictor. The measure ranges from 0 to just under 1, with values closer to zero indicating that the model has no predictive power. However, unlike $R^2$ in linear regression, models rarely achieve a high McFadden $R^2$. In fact, in McFaddenâ€™s own words models with a McFadden pseudo $R^2 ~ 0.40$ represents a very good fit. We can assess McFadden's pseudo $R^2$ values for our models with:

```{r}
list(
  model1 = pscl::pR2(model1)["McFadden"],  
  model2 = pscl::pR2(model2)["McFadden"],  
  model3 = pscl::pR2(model3)["McFadden"])
```

##### Residual assessment
Keep in mind that logistic regression does not assume the residuals are normally distributed nor that the variance is constant. However, the deviance residual is useful for determining if individual points are not well fit by the model. Here we can fit the standardized deviance residuals to see how many exceed 3 standard deviations. First we extract several useful bits of model results with `augment` and then proceed to plot.

```{r}
model1_data <- augment(model1) %>% 
  mutate(index = 1:n())

ggplot(model1_data, aes(index, .std.resid, col=default))+
  geom_point(alpha = .5)+
  geom_ref_line(h = 3)
```

Those standardized residuals that exceed 3 represent possible outliers and may deserve closer attention. We can filter for these residuals to get a closer look. We see that all these observations represent customers who defaulted with budgets that are much lower than the normal defaulters.

```{r}
model1_data %>% 
  filter(abs(.std.resid) > 3)
```

Similar to linear regression we can also identify influential observations with Cookâ€™s distance values. Here we identify the top 5 largest values.

```{r}
plot(model1, which = 4, id.n = 5)
```


And we can investigate these further as well. Here we see that the top five influential points include:

- hose customers who defaulted with very low balances and
- two customers who did not default, yet had balances over $2,000

This means if we were to remove these observations (not recommended), the shape, location, and confidence interval of our logistic regression S-curve would likely shift.

```{r}
model1_data %>% 
  top_n(5, .cooksd)
```

#### Validation of predicted values

##### Classification rate

When developing models for prediction, the most critical metric is regarding how well the model does in predicting the target variable on out-of-sample observations. First, we need to use the estimated models to predict values on our training data set(`train`). When using `predict` be sure to include `type=response` so that the prediction returns the probability of default.

```{r}
test.predicted.m1 <- predict(model1, newdata = test, type = "response")
test.predicted.m2 <- predict(model2, newdata = test, type = "response")
test.predicted.m3 <- predict(model3, newdata = test, type = "response")
```

Now we can compare the predicted target variable versus the observed values for each model and see which performs the best. We can start by using the confusion matrix, which is a table that describes the classification performance for each model on the test data. Each quadrant of the table has an important meaning. In this case the â€œNoâ€? and â€œYesâ€? in the rows represent whether customers defaulted or not. The â€œFALSEâ€? and â€œTRUEâ€? in the columns represent whether we predicted customers to default or not.

- __true positives__ (Bottom-right quadrant): these are cases in which we predicted the customer would default and they did.
- __true negatives__ (Top-left quadrant): We predicted no default, and the customer did not default.
- __false positives__ (Top-right quadrant): We predicted yes, but they didnâ€™t actually default. (Also known as a â€œType I error.â€?)
- __false negatives__ (Bottom-left): We predicted no, but they did default. (Also known as a â€œType II error.â€?)

The results show that `model1` and `model3` are very similar. 96% of the predicted observations are true negatives and about 1% are true positives. Both models have a type II error of less than 3% in which the model predicts the customer will not default but they actually did. And both models have a type I error of less than 1% in which the models predicts the customer will default but they never did. `model2` results are notably different; this model accurately predicts the non-defaulters (a result of 97% of the data being non-defaulters) but never actually predicts those customers that default!

```{r}
list(
  model1 = table(test$default, test.predicted.m1 > 0.5) %>% prop.table() %>% round(3),
  model2 = table(test$default, test.predicted.m2 > 0.5) %>% prop.table() %>% round(3),
  model3 = table(test$default, test.predicted.m3 > 0.5) %>% prop.table() %>% round(3))
```

We also want to understand the misclassification (aka _error_) rates (or we could flip this for the accuracy rates). . We donâ€™t see much improvement between models 1 and 3 and although model 2 has a low error rate donâ€™t forget that it never accurately predicts customers that actually default.

```{r}
test %>%
  mutate(m1.pred = ifelse(test.predicted.m1 > 0.5, "Yes", "No"),
         m2.pred = ifelse(test.predicted.m2 > 0.5, "Yes", "No"),
         m3.pred = ifelse(test.predicted.m3 > 0.5, "Yes", "No")) %>%
  summarise(m1.error = mean(default != m1.pred),
            m2.error = mean(default != m2.pred),
            m3.error = mean(default != m3.pred))
## # A tibble: 1 Ã? 3
##     m1.error   m2.error   m3.error
##        <dbl>      <dbl>      <dbl>
## 1 0.02782697 0.03491019 0.02807994
```



We can gain some additional insights by looking at the raw values (not percentages) in our confusion matrix. Lets look at model 1 to illustrate. We see that there are a total of 
$98+40=138$ customers that defaulted. Of the total defaults, $98/138=71%$ were not predicted. Alternatively, we could say that only $40/138=29$ of default occurrences were predicted - this is known as the the precision (also known as sensitivity) of our model. So while the overall error rate is low, the precision rate is also low, which is not good!

```{r}
table(test$default, test.predicted.m1>0.5)
```

With classification models you will also here the terms _sensitivity_ and _specificity_ when characterizing the performance of the model. As mentioned above sensitivity is synonymous to precision. However, the specificity is the percentage of non-defaulters that are correctly identified, here $1 -12/(3803+12) = 99.6%$ (the accuracy here is largely driven by the fact that 97% of the observations in our data are non-defaulters). The importance between _sensitivityy_ and _specificity_ is dependent on context. In this case, a credit card company is likely to be more concerned with sensititivy since they want to reduce their risk. Therefore, they may be more concerned with tuning a model so that their _sensititivy_/_precision_ is improved. 

The receiving operating characteristic (ROC) is a visual measure of classifier performance. Using the proportion of positive data points that are correctly considered as positive and the proportion of negative data points that are mistakenly considered as positive, we generate a graphic that shows the trade off between the rate at which you can correctly predict something with the rate of incorrectly predicting something. Ultimately, weâ€™re concerned about the area under the ROC curve, or AUC. That metric ranges from 0.50 to 1.00, and values above 0.80 indicate that the model does a good job in discriminating between the two categories which comprise our target variable. We can compare the ROC and AUC for modelâ€™s 1 and 2, which show a strong difference in performance. We definitely want our ROC plots to look more like model 1â€™s (left) rather than model 2â€™s (right)!

```{r}
library(ROCR)

par(mfrow=c(1, 2))

prediction(test.predicted.m1, test$default) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()

prediction(test.predicted.m2, test$default) %>%
  performance(measure = "tpr", x.measure = "fpr") %>%
  plot()
```

And to compute the AUC numerically we can use the following. Remember, AUC will range from .50 - 1.00. Thus, model 2 is a very poor classifying model while model 1 is a very good classying model.

```{r}
# model1 AUC
prediction(test.predicted.m1, test$default) %>% 
  performance(measure = "auc") %>% 
  .@y.values
## [[1]]
## [1] 0.939932

# model 2 AUC
prediction(test.predicted.m2, test$default) %>%
  performance(measure = "auc") %>%
  .@y.values
## [[1]]
## [1] 0.5386955
```

We can continue to â€œtuneâ€? our models to improve these classification rates. If you can improve your AUC and ROC curves (which means you are improving the classification accuracy rates) you are creating _â€œliftâ€_, meaning you are lifting the classification accuracy.

#### Additional resources

This will get you up and running with logistic regression. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more:

An Introduction to Statistical Learning
Applied Predictive Modeling
Elements of Statistical Learning

### Regularized regression

As discussed, linear regression is a simple and fundamental approach for supervised learning. Moreover, when the assumption required by OLS are met, the coefficients produced by OLS are unbiased and, of all unbiased linear techniques, have the lowest variance. However, in todayâ€™s world, data sets being analyzed typically have a large amount of features. As the number of features grow, our OLS assumptions typically break down and our models often overfit (aka have high variance) to the training sample, causing our out of sample error to increase. Regularization methods provide a means to control our regression coefficients, which can reduce the variance and decrease our of sample error.

#### tl;dr
This tutorial serves as an introduction to regularized and covers:

1. [Replication requirements](#RR_RR): What youâ€™ll need to reproduce the analysis in this tutorial.
2. [Why regularize](#RR_Reg): A closer look at why regularization can improve upon ordinary least squares regression.
3. [Ridge regression](#RR_Ridge): Regularizing coefficients but keeping all features.
4. [Lasso regression](#RR_LASSO): Regularizing coefficients to perform feature selection.
5. [Elastic nets](#RR_EN): Combining Ridge and Lasso regularization. Predicting: Once youâ€™ve found your optimal model, predict on a new data set.
6. [Other package implementations](#RR_pck_imp): Implementing regularization with other popular packages.
7. [Learning more](#RR_Learn): Where to go from here.

#### Replication Requirements: {#RR_RR}

This tutorial leverages the following packags. Most of these packages are playing a supporting role while the main emphasis will be on the `glmnet` package.

```{r}
library(rsample)  # data splitting 
library(glmnet)   # implementing regularized regression approaches
library(dplyr)    # basic data manipulation procedures
library(ggplot2)  # plotting
```

To illustrate various regularization concepts, we will use the `Ames` Housing data that has been included in the `AmesHousing` package.

```{r}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)

ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```


#### Why regularize {#RR_Reg}

The objective of ordinary least squares regression is to find the plane that minimizes the sum of squared errors (SSE) between the observed and predicted response. In Figure 1, this means identifying the plane that minimizes the grey lines, which measure the distance between the observed (red dots) and predicted response (blue plane).

More formally, this objective function is written as:
$$
minimize [{SSE = \Sigma_{i=1}^n (y_i-\hat{}y_i)^2}]
$$

The OLS objective function performs quite well when our data align to the key assumptions of OLS regression:

- Linear relationship
- Multivariate normality
- No autocorrelation
- Homoscedastic (constant variance in residuals)
- There are more observations (n) than features 

However, for many real-life data sets we have very _wide_ data, meaning we have a large number of features (_p_) that we believe are informative in predicting some outcome. As _p_ increases, we can quickly violate some of the OLS assumptions and we require alternative approaches to provide predictive analytic solutions. Specifically, as _p_ increases there are three main issues we most commonly run into:

__1. Multicollinearty__

As _p_ increases we are more likely to capture multiple features that have some multicollinearity. When multicollinearity exists, we often see high variability in our coefficient terms. For example, in our Ames data, `Gr_Liv_Area` and `TotRms_AbvGrd` are two variables that have a correlation of 0.801 and both variables are strongly correlated to our response variable (`Sale_Price`). When we fit a model with both these variables we get a positive coefficient for `Gr_Liv_Area` but a negative coefficient for `TotRms_AbvGrd`, suggesting one has a positive impact to `Sale_Price` and the other a negative impact.

```{r}
# fit with two strongly correlated variables
lm(Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)
## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area + TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)    Gr_Liv_Area  TotRms_AbvGrd  
##       49953.6          137.3       -11788.2
```

However, if we refit the model with each variable independently, they both show a positive impact. However, the `Gr_Liv_Area` effect is now smaller and the `TotRms_AbvGrd` is positive with a much larger magnitude.

```{r}
# fit with just Gr_Liv_Area
lm(Sale_Price ~ Gr_Liv_Area, data = ames_train)
## 
## Call:
## lm(formula = Sale_Price ~ Gr_Liv_Area, data = ames_train)
## 
## Coefficients:
## (Intercept)  Gr_Liv_Area  
##       17797          108

# fit with just TotRms_Area
lm(Sale_Price ~ TotRms_AbvGrd, data = ames_train)
## 
## Call:
## lm(formula = Sale_Price ~ TotRms_AbvGrd, data = ames_train)
## 
## Coefficients:
##   (Intercept)  TotRms_AbvGrd  
##         26820          23731
```

This is a common result when collinearity exists. Coefficients for correlated features become over-inflated and can fluctuate significantly. One consequence of these large fluctuations in the coefficient terms is overfitting, which means we have high variance in the bias-variance tradeoff space. Although an analyst can use tools such as variance inflaction factors (Myers, 1994) to identify and remove those strongly correlated variables, it is not always clear which variable(s) to remove. Nor do we always wish to remove variables as this may be removing signal in our data.

__2. Insufficient solution__

When the number of features exceed the number of observations ($p>n$), the OLS solution matrix is _not_ invertible. This causes significant issues because it means: (1) The least-squares estimates are not unique. In fact, there are an infinite set of solutions available and most of these solutions overfit the data. (2) In many instances the result will be computationally infeasible.

Consequently, to resolve this issue an analyst can remove variables until $p < n$ and then fit an OLS regression model. Although an analyst can use pre-processing tools to guide this manual approach (Kuhn & Johnson, 2013, pp. 43-47), it can be cumbersome and prone to errors.

__3. Interpretability__

With a large number of features, we often would like to identify a smaller subset of these features that exhibit the strongest effects. In essence, we sometimes prefer techniques that provide feature selection. One approach to this is called _hard threshholding_ feature selection, which can be performed with linear model selection approaches. However, model selection approaches can be computationally inefficient, do not scale well, and they simply assume a feature as in or out. We may wish to use a _soft threshholding_ approach that slowly pushes a featureâ€™s effect towards zero. As will be demonstrated, this can provide additional understanding regarding predictive signals.

__Regulaized regression__

When we experience these concerns, one alternative to OLS regression is to use regularized regression (also commonly referred to as _penalized_ models or _shrinkage_ methods) to control the parameter estimates. Regularized regression puts contraints on the magnitude of the coefficients and will progressively shrink them towards zero. This constraint helps to reduce the magnitude and fluctuations of the coefficients and will reduce the variance of our model.

The objective function of regulaized regression model is very similar to OLS regression; however, we add a penalty paramter (`P`)

$$
minimize(SSE + P)
$$

There are two main penalty parameters, which we will see shortly, but they both have a similarr effect. They constrain the size of the coefficients such that the only way the coefficients can increase is if we experience a comparable decrease in the sum of squared errors (SSE). Next, weâ€™ll explore the most common approaches to incorporate regularization.

#### Ridge {#RR_Ridge}

Ridge regression [(Hoerl, 1970](https://www.tandfonline.com/doi/abs/10.1080/00401706.1970.10488634)) controls the coefficients by adding $\lambda \Sigma_{j=1}^p \beta_j^2$ to the objective function. This penalty parameter is also referred to as â€?$L_2$" as it signifies a second-order penlty being used on the coefficients.

$$ minimize (SSE + \lambda \Sigma_{j=1}^p \beta_j^2)
$$ 


This penalty parameter can take on a wide range of values, which is controlled by the tuning parameter $\lambda$. When $lambda>0$ there is no effect and our objective function equals the normal PLS regression objective function of simply minimizing SSE. 

However, as $\lambda -> \infit$, the penalty becomes large and forces our coefficients to zero. This is illustrated in Figure 2 where exemplar coefficients have been regularized with $\lambda ranging from 0 to over 8,000 (log(8103)=9).

Aothough these coefficients were scaled and centered prior to the analysis, you will notice that some are extremely large when $\lambda=0$. Furthermore, you will notice the large shrinks to zero. his is indicitive of multicollinearity and likely illustrates that constraining our coefficients with $log(\lambda)~2$ whre its then continuously shrinks to zero. This is indicative of multicollinearity and likely illustrates that constraining our coefficients with $log(\lambda)>2$ may reduce the variance, and therefore the error in our model. However, the question remains - how do we find the amount that minimizes our error? We will answer this shortly.

##### Implementation

To implement Ridge regression, we will focus on the `glmnet` package (implementation in other packages are illustrated [below](http://uc-r.github.io/regularized_regression#other)). `glmnet` does not use the formula method (`y ~ x`) so prior to modeling we need to create our feature and target set. Furthermore, we use the `model.matrix` function on our feature set, which will automatically dummy encode qualitative variables (see `Matrix::sparse.model.matrix` for increased efficiency on large dimension data). We also log transform our response variable due to its skeweness.


```{r}
# Create training and testing feature model matrices and response vectors.
# we use model.matrix(...)[, -1] to discard the intercept
ames_train_x <- model.matrix(Sale_Price ~ ., ames_train)[, -1]
ames_train_y <- log(ames_train$Sale_Price)

ames_test_x <- model.matrix(Sale_Price ~ ., ames_test)[, -1]
ames_test_y <- log(ames_test$Sale_Price)

# What is the dimension of of your feature matrix?
dim(ames_train_x)
## [1] 2054  307
```

To apply a ridge model we can use the glmnet::glmnet function. The alpha parameter tells glmnet to perform a ridge (`alpha = 0`), lasso (`alpha = 1`), or elastic net ($0 < alpha < 1$) model. Behind the scenes, `glmnet` is doing two things that you should be awre of:

1. It is essential that predictor variables are standardized when performing regularized regression. `glmnet` performs this for you.  If you standardize your predictors prior to `glmnet` you can turn this argument off with `standardize = FALSE`.

2. `glmnet` will perform ridge models across a wide range of $\lambda$ parameters, which are illustrated in the figure below.

```{r}
# apply ridge regression to ames data
library(glmnet)
ames_ridge <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge, xvar="lambda")
```

In fact, we can see the exact $\lambda$ values applied with `ames_ridge$lambda`. Although you can specify your own $\lambda$ values, by default `glmnet` applies 100 $\lambda$ values that are derived. Majority of the time  you will have little need to adjust the default $\lambda$ values.

We can also directly access the coefficients for a model using `coef. glmnet` stores all the coefficients for each model in order of largest to smallest $\lambda$. Due to the number of features, here I just peak at thebcoefficients for the `Gr_Liv_Area` and `TotRms_abvGrd` features for the largest $\lambda$ (279.1035) and smallest $\lambda$ (0.02791035). You can see how the largest $\lambda$ value has pushed these coefficients to nearly 0.

```{r}
# lambdas applied to penalty parameter
ames_ridge$lambda %>% head()
## [1] 279.1035 254.3087 231.7166 211.1316 192.3752 175.2851

# coefficients for the largest and smallest lambda parameters
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 100]
##   Gr_Liv_Area TotRms_AbvGrd 
##  0.0001004011  0.0096383231
coef(ames_ridge)[c("Gr_Liv_Area", "TotRms_AbvGrd"), 1] 
##   Gr_Liv_Area TotRms_AbvGrd 
##  5.551202e-40  1.236184e-37
```

However, at this point, we do not understand how much improvement we are experiencing in our model.

__Tuning__ 

Recall tht $\lambda$ is a tuning parameter that helps to control our model frok overfitting to the training data. However, to identify the optimal $\lambda$ value we need to perform [cross validation](http://uc-r.github.io/resampling_methods) (CV). `cv.glmnet` provides a built-in option to perform k-fold CV, and by default, performs 10-fold CV.

```{r}
# Apply CV Ridge regression to ames data
ames_ridge <- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

# plot results
plot(ames_ridge)
```


Our plot outputs above illustrates the 10-fold CV mean squared error(MSE) across the $\lambda$ values. It illustrates that we do not see substantial improvement; however, as we constrain our coefficients with $log(\lambda)>0$ penalty, the MSE rises considerably. 

The numbers at the top of the plot (299) just refer to the number of variables in the model. Ridge regression does not force any variable exactly zero so all features will remain in the model (we will see this change with `lasso` and `elastic nets`).

```{r}
ames_ridge$cvm %>% min() #minimum MSE
ames_ridge$lambda.min # lambda for this min MSE

ames_ridge$cvm[ames_ridge$lambda == ames_ridge$lambda.1se] # 1st st.error
ames_ridge$lambda.1se  # lambda for this MSE
```

The advantage of identifying the $\lambda$ with an MSE within one standard error becomes more obvious with the lasso and elastic net models.  However, for now we can assess this visually. Here we plot the coefficients across the $\lambda$ values and the dashed red line represents the largest $\lambda$ that falls within oe standard error of the minimum MSE. This shows you how much we can constrain the coefficients while still maximizing predictive accuracy.

```{r}
ames_ridge_min <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 0
)

plot(ames_ridge_min, xvar = "lambda")
abline(v = log(ames_ridge$lambda.1se), col = "red", lty = "dashed")

```

__Advantage & Disdvantage__

In essence, the ridge regression model has pushed many of the correlated features towards each other rather than allowing for one to be wildly positive and the other wildly negative. Furthermore, many of the non-important features have been pushed closer to zero. This means we have reduced the noise in our data, which provides us more clarity in identifying the true signals in our model.

```{r}
coef(ames_ridge, s = "lambda.1se") %>% 
  broom::tidy() %>% 
  filter(row != "(Intercept)") %>% 
  top_n(25, wt = abs(value)) %>% 
  ggplot(aes(value, reorder(row, value)))+
  geom_point()+
  ggtitle("Top 25 influential variables")+
  xlab("coefficient")+
  ylab(NULL)
```

However, __a ridge model will retain all variables__. Therefore, a ridge model is good if you believe there is a need to retain all features in your model yet reduce the noise that less influential variables may create and minimize multicollinearity. However, a ridge model does not perform feature selection. If greater interpretation is necessary where you need to reduce the signal in your data to a smaller subset then a lasso model may be preferable.

#### LASSO {#RR_LASSO}

The _least absolute shrinkage and selection operator (lasso)_ model ([Tibshirani, 1996](http://www.jstor.org/stable/2346178?seq=1#page_scan_tab_contents)) is an alternative to ridge regression that has a small modification to the penalty in the objective function. Rather than the $L_2$ penalty we use the following $L_1$ penalty $\lambda \Sigma_{j=1}^p|\beta_j|$ in the objective function

$$
minimize (SSE + \lambda \Sigma_{j=1}^p|\beta_j|)
$$

Whereas the ridge regression approach pushes variables to _approximately but not equal to zero_, the lasso penalty will actually push coefficients to zero as illustrated with Fig. 3. Thus the lasso model not only improves the model with regularization but it also conducts automated feature selection.

In Fig.3we see that when $log(\lambda)=1$ only 3 variables are retained. Consequently, when a data set has many features lasso can be used to identify and extract those features with the largest (and most consistent) signal.

__Implementation__

Implementing lasso follows the same logic as implementing the ridge model, we just need to switch `alpha=1` within `glmnet`.

```{r}
## apply lasso regression to ames data

ames_lasso <- glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso, xvar = "lambda")
```

Our output illustrates a quick drop in the number of features retained in the lasso model as $log(\lambda)$-> -6. In fact, we see several features that had very large coefficients for the OLS model (when $log(\lambda)=-10 -> \lambda=0$.  As before, these features are likely highly correlated with other features in the data, causing their coefficients to be excessively large. As we constrain our model, these noisy features are pushed to zero.

However, similar to the Ridge regression section, we need to perform CV to determine when the right value is for $\lambda$.

__Tuning__

To perform CV we use the same approach as we did in the ridge regression tuning section, but change our `alpha = 1`. We see that we can minimize our MSE by applying approximately $-6 < log(\lambda) < -4$ Not only does this minimize our MSE but it also reduces the number of features to $156 > p > 58$

```{r}
# Apply CV Ridge regression to ames data
ames_lasso <- cv.glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)
# plot results
plot(ames_lasso)
```

As before, we can extract our minimum and one standard error MSE and $\Î»$ values.

```{r}
min(ames_lasso$cvm)       # minimum MSE
## [1] 0.02275227
ames_lasso$lambda.min     # lambda for this min MSE
## [1] 0.003521887

ames_lasso$cvm[ames_lasso$lambda == ames_lasso$lambda.1se]  # 1 st.error of min MSE
## [1] 0.02562055
ames_lasso$lambda.1se  # lambda for this MSE
## [1] 0.01180396
```

Now the advantage of identifying the $\lambda$ with an MSE within one standard error becomes more obvious. If we use the $\lambda$ that derives the minimum MSE we can reduce our feature set from 307 down to less than 160. However, there will be some variability with this MSE and we can reasonably assume that we can achieve a similar MSE with a slightly more constrained model that uses only 63 features. If describing and interpreting the predictors is an important outcome of your analysis, this may significantly aid your endeavor.

```{r}
ames_lasso_min <-  glmnet(
  x = ames_train_x,
  y = ames_train_y,
  alpha = 1
)

plot(ames_lasso_min, xvar = "lambda")
abline(v = log(ames_lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(ames_lasso$lambda.1se), col = "red", lty = "dashed")
```

__Advantage & Disadvantage__

Similar to ridge, the lasso pushes many of the collinear features towards each other rather than allowing for one to be wildly positive and the other wildly negative. However, unlike ridge, the lasso will actually push coefficients to zero and perform feature selection. This simplifies and automates the process of identifying those feature most influential to predictive accuracy.

```{r}
coef(ames_lasso, s = "lambda.1se") %>% 
  broom::tidy() %>% 
  filter(row != "(Intercept)") %>% 
  ggplot(aes(value, reorder(row, value), color = value >0))+
  geom_point(show.legend = FALSE)+
  ggtitle("Influential variables")+
  xlab("Coefficient")+
  ylab(NULL)
```

However, often when we remove features we sacrifice accuracy. Consequently, to gain the refined clarity and simplicity that lasso provides, we sometimes reduce the level of accuracy. Typically we do not see large differences in the minimum errors between the two. So practically, this may not be significant but if you are purely competing on minimizing error (i.e. Kaggle competitions) this may make all the difference!

```{r}
# minimum Ridge MSE
min(ames_ridge$cvm)
## [1] 0.02147691

# minimum Lasso MSE
min(ames_lasso$cvm)
## [1] 0.02275227
```


#### Elastic Nets {#RR_EN}

A generalization of the ridge and lasso models is the elastic net (Zou and Hastie, 2005), which combines the two penalties.

$$
minimize(SSE + \lambda_1\Sigma_{j=1}^p \beta_j^2 + \lambda_2\Sigma_{j=1}^p |\beta_j|)
$$

Although lasso models perform feature selection, a result of their penalty parameter is that typically when two strongly correlated features are pushed towards zero, one may be pushed fully to zero while the other remains in the model. Furthermore, the process of one being in and one being out is not very systematic. In contrast, the ridge regression penalty is a little more effective in systematically reducing correlated features together. Consequently, the advantage of the elastic net model is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty.

__Implementation__

We implement an elastic net the same way as the ridge and lasso models, which are controlled by the alpha parameter. Any alpha value between 0-1 will perform an elastic net. When alpha = 0.5 we perform an equal combination of penalties whereas `alpha` $â†?0$  will have a heavier ridge penalty applied and `alpha` $â†?1$ will have a heavier lasso penalty.

```{r}
library(glmnet)
lasso    <- glmnet(ames_train_x, ames_train_y, alpha = 1.0) 
elastic1 <- glmnet(ames_train_x, ames_train_y, alpha = 0.25) 
elastic2 <- glmnet(ames_train_x, ames_train_y, alpha = 0.75) 
ridge    <- glmnet(ames_train_x, ames_train_y, alpha = 0.0)

par(mfrow = c(2, 2), mar = c(6, 4, 6, 2) + 0.1)
plot(lasso, xvar = "lambda", main = "Lasso (Alpha = 1)\n\n\n")
plot(elastic1, xvar = "lambda", main = "Elastic Net (Alpha = .25)\n\n\n")
plot(elastic2, xvar = "lambda", main = "Elastic Net (Alpha = .75)\n\n\n")
plot(ridge, xvar = "lambda", main = "Ridge (Alpha = 0)\n\n\n")
```

__Tuning__

In ridge and lasso models $\lambda$ is our primary tuning parameter. However, with elastic nets, we want to tune the $\lambda$ and the `alpha` parameters. To set up our tuning, we create a common `fold_id`, which just allows us to apply the same CV folds to each model.

We then create a tuning grid that searches across a range of `alpha`s from 0-1, and empty columns where we will dump our model results into.

```{r}
# maintain the same folds across all models
fold_id <- sample(1:10, size = length(ames_train_y), replace=TRUE)

# search across a range of alphas
tuning_grid <- tibble::tibble(
  alpha      = seq(0, 1, by = .1),
  mse_min    = NA,
  mse_1se    = NA,
  lambda_min = NA,
  lambda_1se = NA
)
```

Now we can iterate over each alpha value, apply a CV elastic net, and extract the minimum and one standard error MSE values and their respective $\lambda$ values.

```{r}
for(i in seq_along(tuning_grid$alpha)) {
  
  # fit CV model for each alpha value
  fit <- cv.glmnet(ames_train_x, ames_train_y, alpha = tuning_grid$alpha[i], foldid = fold_id)
  
  # extract MSE and lambda values
  tuning_grid$mse_min[i]    <- fit$cvm[fit$lambda == fit$lambda.min]
  tuning_grid$mse_1se[i]    <- fit$cvm[fit$lambda == fit$lambda.1se]
  tuning_grid$lambda_min[i] <- fit$lambda.min
  tuning_grid$lambda_1se[i] <- fit$lambda.1se
}

tuning_grid
## # A tibble: 11 x 5
##    alpha mse_min mse_1se lambda_min lambda_1se
##    <dbl>   <dbl>   <dbl>      <dbl>      <dbl>
##  1 0      0.0217  0.0241    0.136       0.548 
##  2 0.100  0.0215  0.0239    0.0352      0.0980
##  3 0.200  0.0217  0.0243    0.0193      0.0538
##  4 0.300  0.0218  0.0243    0.0129      0.0359
##  5 0.400  0.0219  0.0244    0.0106      0.0269
##  6 0.500  0.0220  0.0250    0.00848     0.0236
##  7 0.600  0.0220  0.0250    0.00707     0.0197
##  8 0.700  0.0221  0.0250    0.00606     0.0169
##  9 0.800  0.0221  0.0251    0.00530     0.0148
## 10 0.900  0.0221  0.0251    0.00471     0.0131
## 11 1.00   0.0223  0.0254    0.00424     0.0118
```

If we plot the MSE +- one standard erro for the optimal $\lambda$ value for each `alpha` setting, we see that they all fall within the same level of accuracy. Consequently, we could select a full lasso model with $lambda=0.02062776$, gain the benefits of its feature selection capability and reasonably assume no loss in accuracy.

```{r}
tuning_grid %>% 
  mutate(se = mse_1se - mse_min) %>% 
  ggplot(aes(alpha, mse_min))+
  geom_line(size = 2)+
  geom_ribbon(aes(ymax = mse_min + se, ymin = mse_min - se), alpha= .25)+
  ggtitle("MSE +- one standard error")
```

__Advantage & Disadvantage__

As previously stated, the advantage of the elastic net model is that it enables effective regularization via the ridge penalty with the feature selection characteristics of the lasso penalty. Effectively, elastic nets allow us to control multicollinearity concerns, perform regression when 
$p>n$, and reduce excessive noise in our data so that we can isolate the most influential variables while balancing prediction accuracy.

However, elastic nets, and regularization models in general, still assume linear relationships between the features and the target variable. And although we can incorporate non-additive models with interactions, doing this when the number of features is large is extremely tedious and difficult. When non-linear relationships exist, its beneficial to start exploring non-linear regression 

#### Predicting

Once you have identified your preferred model, you can simply use `predict` to predict the same model on a new data set. The only caveat is you need to supply `predict` an `s` parameter with the preferred models $\lambda$ value. For example, here we create a lasso model, which provides me a minimum MSE of 0.022. I use the minimum $\lambda$ value to predict on the unseen test set and obtain a slightly lower MSE of 0.015.

```{r}
# some best model
cv_lasso   <- cv.glmnet(ames_train_x, ames_train_y, alpha = 1.0)
min(cv_lasso$cvm)
## [1] 0.02279668

# predict
pred <- predict(cv_lasso, s = cv_lasso$lambda.min, ames_test_x)
mean((ames_test_y - pred)^2)
## [1] 0.01488605
```

#### Other package implementation {##RR_pck_imp}

glmnet is not the only package that can perform regularized regression. The following also shows how to implement with the popular `caret` and `h2o` packages. For brevity, I show the code but not the output.

```{r}
## caret package
library(caret)

train_control <- trainControl(method = "cv", number = 10)

caret_mod <- train(
  x = ames_train_x,
  y = ames_train_y,
  method = "glmnet",
  preProc = c("center", "scale", "zv", "nzv"),
  trControl = train_control,
  tuneLength = 10
)

caret_mod
```

```{r}
## h2o package
library(h2o)
h2o.init()

# convert data to h2o object
ames_h2o <- ames_train %>%
  mutate(Sale_Price_log = log(Sale_Price)) %>%
  as.h2o()

# set the response column to Sale_Price_log
response <- "Sale_Price_log"

# set the predictor names
predictors <- setdiff(colnames(ames_train), "Sale_Price")


# try using the `alpha` parameter:
# train your model, where you specify alpha
ames_glm <- h2o.glm(
  x = predictors, 
  y = response, 
  training_frame = ames_h2o,
  nfolds = 10,
  keep_cross_validation_predictions = TRUE,
  alpha = .25
  )

# print the mse for the validation data
print(h2o.mse(ames_glm, xval = TRUE))

# grid over `alpha`
# select the values for `alpha` to grid over
hyper_params <- list(
  alpha = seq(0, 1, by = .1),
  lambda = seq(0.0001, 10, length.out = 10)
  )

# this example uses cartesian grid search because the search space is small
# and we want to see the performance of all models. For a larger search space use
# random grid search instead: {'strategy': "RandomDiscrete"}

# build grid search with previously selected hyperparameters
grid <- h2o.grid(
  x = predictors, 
  y = response, 
  training_frame = ames_h2o, 
  nfolds = 10,
  keep_cross_validation_predictions = TRUE,
  algorithm = "glm",
  grid_id = "ames_grid", 
  hyper_params = hyper_params,
  search_criteria = list(strategy = "Cartesian")
  )

# Sort the grid models by mse
sorted_grid <- h2o.getGrid("ames_grid", sort_by = "mse", decreasing = FALSE)
sorted_grid

# grab top model id
best_h2o_model <- sorted_grid@model_ids[[1]]
best_model <- h2o.getModel(best_h2o_model)

```


#### Learning More {#RR_Learn}

This serves as an introduction to regularized regression; however, it just scrapes the surface. Regularized regression approaches have been extended to other parametric generalized linear models (i.e. logistic regression, multinomial, poisson, support vector machines). Moreover, alternative approaches to regularization exist such as `Least Angle Regression` and The `Bayesian Lasso`. The following are great resources to learn more (listed in order of complexity):

- Applied Predictive Modeling
- Introduction to Statistical Learning
- The Elements of Statistical Learning
- Statistical Learning with Sparsity

### Multivariate Adaptive regression splines (MARS) {#MARS}

Several previous tutorials (i.e., ) discussed algorithms that are intrinsically linear. Many of these models can be adapted to nonlinear patterns in the data by manually adding model terms (i.e. squared terms, interaction effects); however, to do so you must know the specific nature of the nonlinearity a priori. Alternatively, there are numerous algorithms that are inherently nonlinear. When using these models, the exact form of the nonlinearity does not need to be known explicitly or specified prior to model training. Rather, these algorithms will search for, and discover, nonlinearities in the data that help maximize predictive accuracy.

This tutorial discusses multivariate adaptive regression splines (MARS), an algorithm that essentially creates a piecewise linear model which provides an intuitive stepping block into nonlinearity after grasping the concept of linear regression and other intrinsically linear models.

- [Prerequisites](#MARS_PR)
- [Basic Idea](#MARS_BI)
- [Multivariage regression splines](#MARS)
- [Fitting a basic MARS model](#MARS_Fit)
- [Tuning](#MARS_TUNE)
- [Feature interpretation](#MARS_FI)
- [Final thoughts](#MARS_SUM)
- [Learning more](#MARS_LM)


#### Prerequisites {#MARS_PR}

For this tutorial we will use the following packages:
```{r}
library(rsample)   # data splitting 
library(ggplot2)   # plotting
library(earth)     # fit MARS models
library(caret)     # automating the tuning process
library(vip)       # variable importance
library(pdp)       # variable relationships
```

To illustrate various MARS modeling concepts we will use `Ames Housing` data, which is available via the `AmesHousing` package.

```{r}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop = .7, strata = "Sale_Price")
ames_train <- training(ames_split)
ames_test  <- testing(ames_split)
```

#### The basic idea {#MARS_BI}

Some previous tutorials have focused on linear models. In those tutorials we illustrated some of the advantages of linear models such as their ease and speed of computation and also the intuitive nature of interpreting their coefficients. However, linear models make a strong assumption about linearity, and this assumption is often a poor one, which can affect predictive accuracy.

We can extend linear models to capture non-linear relationships. Typically, this is done by explicitly including polynomial parameters or step functions. Polynomial regression is a form of regression in which the relationship between the independent variable $x$ and the dependent variable y is modeled as an $n^{th}$ degree polynomial of $x$. For example, Equation 1 represents a polynomial regression function where y is modeled as a function of $x$ with $d$ degrees. Generally speaking, it is unusual to use d greater than 3 or 4 as the larger d becomes, the easier the function fit becomes overly flexible and oddly shapenedâ€¦especially near the boundaries of the range of x values

$$
y_i = \beta_0 + \beta_1x_i + \beta_2x_i^2+...+\beta_dx_i^d+\epsilon_i
$$

An alternative to polynomial regression. is step function regression. Whereas polynomial functions impose a global non-linear relationship, step functions break the range of $x$ into bins, and fit a different constant for each bin. This amounts to converting a continuous variable into an ordered categorical variable such that our linear regression function is converted to Equation 2

$$
y_i = \beta_0 + \beta_1 C_1(x_i) + \beta_2 C_2(x_i) + ... + \beta_d C_d(x_i) + \epsilon_i
$$

where $C_1(x)$ represents $x$ value ranging from $c_1 < x < c_2$, $C_2(x)$ represents $x$ value ranging from $c_2<x<c_3$, ..., $C_d(x)$ represents $x$ values ranging from $c_{d-1}<x<c_d$. Figure 1 illustrate polynomial and step function fits for `Sale_Price` asa a function of `Year_Build` in our `ames` data.

```{r fig.cap="Figure 1: Blue line represents predicted Sale_Price values as a function of Year_Built for alternative approaches to modeling explicit nonlinear regression patterns. (A) Traditional nonlinear regression approach does not capture any nonlinearity unless the predictor or response is transformed (i.e. log transformation). (B) Degree-2 polynomial, (C) Degree-3 polynomial, (D) Step function fitting cutting Year_Built into three categorical levels."}
# polynomial degree of 1
p1 <- ames_train %>% 
  ggplot(aes(Year_Built, Sale_Price))+
  geom_point()+
  geom_smooth(method = "gam", formula = y ~ x)

# polynomial degree of 2
p2 <- ames_train %>% 
  ggplot(aes(Year_Built, Sale_Price))+
  geom_point()+
  geom_smooth(method = "gam", formula = y ~ poly(x,2))

# polynomial degree of 3
p3 <- ames_train %>% 
  ggplot(aes(Year_Built, Sale_Price))+
  geom_point()+
  geom_smooth(method = "gam", formula = y ~ poly(x,3))

# step function

gridExtra::grid.arrange(p1, p2, p3, ncol=3)
```

Although useful, the typical implementation of polynomial regression and step functions require the user to explicitly identify and incorporate which variables should have what specific degree of interaction or at points of a variable $x$ should cut points to be made for the step functions. Considering many data sets today can easily contain 50, 100, or more features, this would require an enormous and unncessary time commitment from an analyst to determine these explicit non-linear settings.

### Multivariage regression splines {#MARS}

Multivariate adaptive regression splines (MARS) provide a convenient approach to capture the nonlinearity aspect of polynomial regression by assessing cutpoints ( _knots_) similar to step functions. The procedure assesses each data point for each predictor as a knot and creates a linear regression model with the candidate feature(s). For example, consider our simple model of `Sale_Price` ~ `Year_Built`. The MARS procedure will first look for the single point across the range of Year_Built values where two different linear relationships between `Sale_Price` and `Year_Built` achieve the smallest error. 

What results is known as hinge function ($h(x-a)$ where $a$ is the cutpoint value). For a single knot, our hinge function is $h(Year_Built - 1968$ such that our two linear models for `Sale_Price` are

$$
Sale\_Price = 136091.022 (for \ Year\_Built < 1968)
136091.022 + 3094.208 (Year\_Built-1968) (for Year\_Built > 1968)
$$

Once the first knot has benn found, the search continues for a second knot which is found at 2006 (Figure 2 (B)). This results in three linear models for `Sale_Price`:

```{r eval=FALSE}
mars0 <- ames_train %>% 
  earth::earth(Sale_Price ~ Year_Built, data=.) 

mars0 %>% print()
```

This procedure can continue until many knots are found, producing a highly non-linear pattern. Although including many knots may allow us to fit a really good relationship with our training data, it may not generalize very well to new, unseen data. For example, Figure 3 includes nine knots but this likley will not generalize very well to our test data.

Consequently, once the full set of knots have been created, we can sequentially remove knots that do not contribute significantly to predictive accuracy. This process is known as â€œpruningâ€? and we can use cross-validation, as we have with the previous models, to find the optimal number of knot

#### Fitting a basic MARS model {#MARS_Fit}

We can fit a MARS model with the __earth__ package. By default, `earth::earth()` will assess all potential knots across all supplied features and then will prune to the optimal number of knots on an expected change in $R^2$ (for the training data) of less than 0.001. This calculation is performed by the Generalized cross-validation procedure (GCV statistic), which is a computational shortcut for linear models that produces an error value that approximates leave-one-out cross-validation (see [here](http://w3.atomki.hu/~efo/hornyak/Tikhonov_references/Technometrics_Golub_Heath_Wahba) for technical details).

The following applies a basic MARS model to our ames data and performs a search for required knots across all features. The results show us the final models GCV statistic, generalized $R^2$ (GRSq) and more.

```{r}
# Fit a basic MARS model
mars1 <- earth::earth(
  Sale_Price ~.,
  data = ames_train
)

# Print model summary
print(mars1)
## Selected 37 of 45 terms, and 26 of 307 predictors
## Termination condition: RSq changed by less than 0.001 at 45 terms
## Importance: Gr_Liv_Area, Year_Built, Total_Bsmt_SF, ...
## Number of terms at each degree of interaction: 1 36 (additive model)
## GCV 521186626    RSS 995776275391    GRSq 0.9165386    RSq 0.92229
```

It also shows us that 38 of 41 terms were used from 27 of the 307 original predictors. But what does this mean? If we were to look at all the coefficients, we would see that there are 38 terms in our model (including the intercept). These terms include hinge functions produced from the original 307 predictors (307 predictors because the model automatically dummy encodes our categorical variables). Looking at the first 10 terms in our model, we see that `Gr_Liv_Area` is included with a knot at 2945 (the coefficients for $h(2945 - Gr_Liv_Area$ is $49.85$), `Year_Built` is included with a knot at 2003, etc.

```{r}
mars1 %>% .$coefficients %>% head()

##                              Sale_Price
## (Intercept)                301399.98345
## h(2945-Gr_Liv_Area)           -49.84518
## h(Year_Built-2003)           2698.39864
## h(2003-Year_Built)           -357.11319
## h(Total_Bsmt_SF-2171)        -265.30709
## h(2171-Total_Bsmt_SF)         -29.77024
## Overall_QualExcellent       88345.90068
## Overall_QualVery_Excellent 116330.48509
## Overall_QualVery_Good       36646.55568
## h(Bsmt_Unf_SF-278)            -21.15661
```

The plot method for MARS model objects provide convenient performance and residual plots.Figure 4 illustrates the model selection plot that graphs the GCV $R^2$  (left-hand y-axis and solid black line) based on the number of terms retained in the model (x-axis) which are constructed from a certain number of original predictors (right-hand y-axis). The vertical dashed lined at 37 tells us the optimal number of non-intercept terms retained where marginal increases in GCV 

```{r eval=FALSE, fig.cap="Figure 4: Model summary capturing GCV $R^2$ (left-hand y-axis and solid black line) based on the number of terms retained (x-axis) which is based on the number of predictors used to make those terms (right-hand side y-axis). For this model, 37 non-intercept terms were retained which are based on 26 predictors. Any additional terms retained in the model, over and above these 37, results in less than 0.001 improvement in the GCV $R^2$."}
plot(mars1, which =1)
```

In addition to pruning the number of knots, `earth::earth()` allows us to also assess potential interactions between different hinge functions. The following illustrates by including a `degree = 2` argument. You can see that now our model includes interaction terms between multiple hinge functions (i.e. `h(Year_Built-2003)*h(Gr_Liv_Area-2274)`) is an interaction effect for those houses built prior to 2003 and have less than 2,274 square feet of living space above ground).

```{r}
# Fit a basic MARS model
library(earth)
mars2 <- earth(
  Sale_Price ~ .,  
  data = ames_train,
  degree = 2
)

# check out the first 10 coefficient terms
summary(mars2) %>% .$coefficients %>% head(10)
##                                             Sale_Price
## (Intercept)                               242611.63686
## h(Gr_Liv_Area-2945)                          144.39175
## h(2945-Gr_Liv_Area)                          -57.71864
## h(Year_Built-2003)                         10909.70322
## h(2003-Year_Built)                          -780.24246
## h(Year_Built-2003)*h(Gr_Liv_Area-2274)        18.54860
## h(Year_Built-2003)*h(2274-Gr_Liv_Area)       -10.30826
## h(Total_Bsmt_SF-1035)                         62.11901
## h(1035-Total_Bsmt_SF)                        -33.03537
## h(Total_Bsmt_SF-1035)*Kitchen_QualTypical    -32.75942
```

#### Tuning {#MARS_TUNE}

Since there are two tuning parameters associated with our MARS model: the degree of interactions and the number of retained terms, we need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error (the above pruning process was based only on an approximation of cross-validated performance on the training data rather than an actual k-fold cross validation process). As in previous tutorials, we will perform a cross-validated grid search to identify the optimal mix. Here, we set up a search grid that assesses 30 different combinations of interaction effects (`degree`) and the number of terms to retain (`nprune`).

```{r}
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3,
  nprune = seq(2, 100, length.out = 10) %>% floor()
)
```

We can use __caret__ to perform a grid search using 10-fold cross-validation. The model that provides the optimal combination includes second degree interactions and retain 34 terms. The cross-validated RMSE for these models are illustrated in Figure 5 and the optimal model's cross validated RMSE is $24,021.68$

```{r Fig.cap="Figure 5: Cross-validated RMSE for the 30 different hyperparameter combinations in our grid search. The optimal model retains 34 terms and includes up to 2nd degree interactions."}
# for reproducibility
set.seed(123)

# cross validation model
library(caret)
tuned_mars <- train(
  x = subset(ames_train, select= -Sale_Price),
  y = ames_train$Sale_Price,
  method = "earth",
  metric = "RMSE",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# best model
tuned_mars$bestTune
#    nprune degree
# 14     34      2

# plot residuals
ggplot(tuned_mars)
```

The above grid search helps to focus where we can further refine our model tuning. As a next step, we could perform a grid search that focuses in on a refined grid space for `nprune` (i.e. comparing 25-40 terms retained). However, for brevity we will leave this as an exercise for the reader.

So how does this compare to some other linear models for the Ames housing data? The following table compares the cross-validated RMSE for our tuned MARS model to a regular multiple regression model along with tuned principal component regression (PCR), partial least squares (PLS), and regularized regression (elastic net) models. By incorporating non-linear relationships and interaction effects, the MARS model provides a substantial improvement over the previous linear models that we have explored.

> Notes: Notice that we standardize the features for the linear model but we did not for the MARS model. Whereas linear models tend to be sensitive to the scale of the features. MARS models are not.

```{r}
# multiple regression
set.seed(123)
cv_model1 <- train(
  Sale_Price ~.,
  data = ames_train,
  method = "lm",
  metric = "RMSE",
  trControl = trainControl(method ="cv", number =10),
  preProcess = c("zv", "center", "scale")
)

# prncipal component regression
set.seed(123)
cv_model2 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pcr",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

# partial least squares regression
set.seed(123)
cv_model3 <- train(
  Sale_Price ~ ., 
  data = ames_train, 
  method = "pls",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 20
  )

# regularized regression
set.seed(123)
cv_model4 <- train(
  Sale_Price ~ ., 
  data = ames_train,
  method = "glmnet",
  trControl = trainControl(method = "cv", number = 10),
  metric = "RMSE",
  preProcess = c("zv", "center", "scale"),
  tuneLength = 10
)

# extract out of sample perfromance measrues

library(kableExtra)
summary(resamples(list(
  Multiple_regression = cv_model1, 
  PCR = cv_model2, 
  PLS = cv_model3,
  Elastic_net = cv_model4,
  MARS = tuned_mars
  )))$statistics$RMSE %>%
  kableExtra::kable() %>%
  kableExtra::kable_styling(bootstrap_options = c("striped", "hover"))

```

#### Feature interpretation {#MARS_FI}

MARS models via `earth::earth()` include a backwards elimination feature selection routine that looks at reductions in the GCV estimate of error as each predictor is added to the model. This total reduction is used as the variable importance measure (`value = "gcv`). Since MARS will automatically include and exclude terms during the pruning process, , it essentially performs automated feature selection. If a predictor was never used in any of the MARS basis functions in the final model (after pruning), it has an importance value of zero. This is illustrated in Figure 6 where 27 features have $>0$ importance values while the rest of the feature have an importance value of zero since they were not included in the final model.

Alternatively, you can also monitor the change in the residual sum of squares (RSS) as terms are added (`value = "rss`); however, you will see very little difference between these methods.

```{r}
# variable importance plots
library(vip)
p1 <- vip(tuned_mars, num_features = 40, bar = FALSE, value ="gcv")+ ggtitle("GCV")
p2 <- vip(tuned_mars, num_features = 40, bar = FALSE, value ="rss")+ ggtitle("RSS")

gridExtra::grid.arrange(p1, p2, ncol = 2)
```

It is important to realize that variable importance will only measure the impact of the prediction error as features are included; however, it does not measure the impact for particular hinge functions created for a given feature. For example,  in Figure 6, we see that `Gr_Liv_Area` and `Year_Built` are the two most influential variables; however, variable importance does not tell us how our model is treating the non-linear patterns for each feature. 

Also, if we look at the interaction terms, our model retained, we see interactions between different hinge functions for `Gr_Liv_Area` and `Year_Built`.

```{r}
coef(tuned_mars$finalModel) %>% 
  broom::tidy() %>% 
  filter(str_detect(names, "\\*"))
## # A tibble: 16 x 2
##    names                                                   x
##    <chr>                                               <dbl>
##  1 h(Year_Built-2003) * h(Gr_Liv_Area-2274)           18.7  
##  2 h(Year_Built-2003) * h(2274-Gr_Liv_Area)          -10.9  
##  3 h(Total_Bsmt_SF-1035) * Kitchen_QualTypical       -33.1  
##  4 NeighborhoodEdwards * h(Gr_Liv_Area-2945)        -507.   
##  5 h(Lot_Area-4058) * h(3-Garage_Cars)                -0.791
##  6 h(2003-Year_Built) * h(Year_Remod_Add-1974)         7.00 
##  7 Overall_QualExcellent * h(Total_Bsmt_SF-1035)     104.   
##  8 NeighborhoodCrawford * h(2003-Year_Built)         424.   
##  9 h(Lot_Area-4058) * Overall_CondFair                -3.29 
## 10 Overall_QualAbove_Average * h(2003-Year_Built)    136.   
## 11 h(Lot_Area-4058) * Overall_CondGood                 1.35 
## 12 Bsmt_ExposureNo * h(Total_Bsmt_SF-1035)           -22.5  
## 13 NeighborhoodGreen_Hills * h(5-Bedroom_AbvGr)    27362.   
## 14 Overall_QualVery_Good * Bsmt_QualGood          -18641.   
## 15 h(2003-Year_Built) * Sale_ConditionNormal         192.   
## 16 h(Lot_Area-4058) * h(Full_Bath-2)                   1.61
```


To better understand the relationship between these features and `Sale_Price`, we can create partial dependence plot (PDPs) for each feature individually and also an interaction PDF. The individual PDPs illustrate that our model found that one knot in each feature provides the best fit.

For `Gr_Liv_Area`, as homes exceed 2,945 square feet, each additional square foot demands a higher marginal increase in sale price than homes with less that 2,945 square feet. Similarly, for homes built after 2003, there is a greter marginal effect on sales price based on the age of the home than for homes built priot to 2003. 

The interaction plot (far right plot) illustrates the strong effect these two features have when combined.

```{r fig.cap="Figure 7: Partial dependence plots to understand the relationship between `Sale_Price` and the `Gr_Liv_Area` and `Year_Built` features. The PDPs tell us that as Gr_Liv_Area increases and for newer homes, Sale_Price increases dramatically."}
p1 <- partial(tuned_mars, pred.var = "Gr_Liv_Area", grid.resolution = 10) %>% autoplot()
p2 <- partial(tuned_mars, pred.var = "Year_Built", grid.resolution = 10) %>% autoplot()
p3 <- partial(tuned_mars, pred.var = c("Gr_Liv_Area", "Year_Built"), grid.resolution = 10) %>% 
  plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, colorkey = TRUE, screen = list(z = -20, x = -60))


gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```

#### Final thoughts {#MARS_SUM}

MARS provides a great stepping stone into nonlinear modeling and tends to be fairly intuitive due to being closely related to multiple regression techniques. They are also easy to train and tune. This tutorial illustrated how incorporating non-linear relationships via MARS modeling greatly improved predictive accuracy on our Ames housing data. The following summarizes some of the advantages and disadvantages discussed regarding MARS modeling:

__Advantages__:

- Accurate if the local linear relationships are correct.
- Quick computation.
- Can work well even with large and small data sets.
- Provides automated feature selection.
- The non-linear relationship between the features and response are fairly intuitive.
- Can be used for both regression and classification problems.
- Does not require feature standardization.

__Disadvantages__:
- Not accurate if the local linear relationships are incorrect.
- Typically not as accurate as more advanced non-linear algorithms (random forests, gradient boosting machines).
- The earth package does not incorporate more advanced spline features (i.e. Piecewise cubic models).
- Missing values must be pre-processed.

#### Learning more {#MARS_LM}
This will get you up and running with MARS modeling. Keep in mind that there is a lot more you can dig into so the following resources will help you learn more:

- An Introduction to Statistical Learning, Ch. 7
- Applied Predictive Modeling, Ch. 7
- Elements of Statistical Learning, Ch. 5
- [Notes on the earth package by Stephen Milborrow](http://www.milbo.org/doc/earth-notes.pdf)


### Regression trees & bagging

Basic regression trees partition a dataset into small groups and then fit a simple model for each subgroup. Unfortunately, a single tree model tends to be highly unstable and a poor predictor. However, by bootstrap aggregating ( __bagging__) regression trees, this technique can become quite powerful and effective. Moreover, this provides the fundamental basis of more complex tree-based models such as random forests and _gradient boosting machines_. This tutorial will get you started with regression trees and bagging.

#### tl;dr

This tutorial services as an introduction to the Regression Decision Trees. This tutorail will cover the following material:

- [Replication Requirements](#RT_RR): What youâ€™ll need to reproduce the analysis in this tutorial.
- The idea: A quick overview of how regression trees work.
- [Basic implementation](#RT_IMP): Implementing regression trees in R.
- [Tuning](#RT_TUNE): Understanding the hyperparameters we can tune.
- [Bagging](#RT_BAGG): Improving performance by fitting many trees.
- [Learning more](#RT_MORE): Where you can learn more.

#### Replication Requirements {#RT_RR}

This tutorial leverages the following packages. Most of these packages are playing a supportive role while the main emphasis will be  

```{r}
library(rsample)     # data splitting 
library(dplyr)       # data wrangling
library(rpart)       # performing regression trees
library(rpart.plot)  # plotting regression trees
library(ipred)       # bagging
library(caret)       # bagging
```

To illustrate various regularization concepts we will use the Ames Housing data that has been included in the `AmesHousing` package.

```{r}
# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data
# Use set.seed for reproducibility

set.seed(123)
ames_split <- initial_split(AmesHousing::make_ames(), prop=.7)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)
```



#### Basic Implementation {#RT_IMP}

__The Idea__

There are many methodologies for constructing regression trees but one of the oldest is known as the classification and regression tree (CART) approach developed by Breiman et al. (1984). This tutorial focuses on the regression part of CART. Basic regression trees partition a data set into smaller subgroups and then fit a simple constant for each observation in the subgroup. The partitioning is achieved by successive binary partitions (aka _recursive partitioning_) based on the different predictors. The constant to predict is based on the average response values for all observations that fall in that subgroup.

or example, consider we want to predict the miles per gallon a car will average based on cylinders (`cyl`) and horsepower (`hp`). All observations go through this tree, are assessed at a particular node, and proceed to the left if the answer is gyesh or proceed to the right if the answer is gnoh. So, first, all observations that have 6 or 8 cylinders go to the left branch, all other observations proceed to the right branch. Next, the left branch is further partitioned by horsepower. Those 6 or 8 cylinder observations with horsepower equal to or greater than 192 proceed to the left branch; those with less than 192 hp proceed to the right. These branches lead to terminal nodes or leafs which contain our predicted response value. Basically, all observations (cars in this example) that do not have 6 or 8 cylinders (far right branch) average 27 mpg. All observations that have 6 or 8 cylinders and have more than 192 hp (far left branch) average 13 mpg.

This simple example can be generalized to state we have a continuous response variable $Y$ and two inputs $X_1$ and $X_2$. The recursive partitioning results in three regions ($R_1, R_2, R_3$) where the model predicts $Y$ with a constanct $c_m$ for region $R_m$:

$$
\hat{f}(X) = \Sigma_{m=1}^3 I(X_1, X_2) \in R_m...(1)

$$

Howeer, an important question remains of how to grow a regression tree.

__Deciding on splits__

First, it is important to realize the partitioning of variables are done in a top-down, _greedy_ fashion. This just means that a partition performed earlier in the tree will not change based on later partitions. But how are these partions made? The model begins with the entire data set, S, and searches every distinct value of every input variable to find the predictor and split value that partitions the data into two regions ($R_1 and $_2$) such that the overall sums of squares error are minimized:

$$
minimize \ (SSE = \Sigma_{i \in R_1}(y_i-c_1)^2+
\Sigma_{i \in R_2}(y_i-c_2)^2)...(2)
$$

Having found the best split, we partition the data into the two resulting regions and repeat the splitting process on each of the two regions. This process is continued until some stopping criterion is reached. What results is, typically, a very deep, complex tree that may produce good predictions on the training set, but is likely to overfit the data, leading to poor performance on unseen data.

For example, using the well-known Boston housing data set, I create three decision trees based on three different samples of the data. You can see that the first few partitions are fairly similar at the top of each tree; however, they tend to differ substantially closer to the terminal nodes. These deeper nodes tend to overfit to specific attributes of the sample data; consequently, slightly different samples will result in highly variable estimate/predicted values in the terminal nodes. By pruning these lower level decision nodes, we can introduce a little bit of bias in our model that help to stabilize predictions and will tend to generalize better to new, unseen data.

___Cost complexity criterion__

There is often a balance to be achieved in the depth and complexity of the tree to optimize predictive performance on some unseen data. To find this balance, we typically grow a very large tree as defined in the previous section and then prune back to find an _optimal subtree_.

We find the optimal subtree by using a _cost complexity parameter_ ($\alpha$) that penalizes our objective function in Eq.2 for the number of terminal nodes of the tree (T) as Eq.3.

$$
minimize (SSE + \alpha|T|)
$$

For a given value of $\alpha$, we find the smallest pruned tree that has the lowest penalized error. If you are familiar with [regularized regression](http://uc-r.github.io/regularized_regression), you will realize the close association to the [lasso $L_1$ norm penalty](http://uc-r.github.io/regularized_regression#lasso). 

As with these regularization methods, smaller penalties tend to produce more complex models, which result in larger trees. Whereas larger penalties result in much smaller trees.

consequently, as a tree grows larger, the reduction in the SSE must be greater than the cost compexity penalty. Typically, we evaluate multiple models across a spectrum of $\alpha$ and use cross-validation to identify the optimal $\alpha$ and, therefore, the optimal subtree.

__Strenghts and weakness__

There are several __advantages__ to regression trees:

- They are very interpretable.
- Making prediction is fast (no compicated calculations, just looking up constants in the trees)
- Itfs easy to understand what variables are important in making the prediction. The internal nodes (splits) are those variables that most largely reduced the SSE.
- If some data is missing, we might not be able to go all the way down the tree to a leaf, but we can still make a prediction by averaging all the leaves in the sub-tree we do reach.
- The model provides a non-linear gjaggedh response, so it can work when the true regression surface is not smooth. If it is smooth, though, the piecewise-constant surface can approximate it arbitrarily closely (with enough leaves).
- There are fast, reliable algorithms to learn these trees.

But there are also some significant __weaknesses__:

- Single regression trees have high variance, resulting in unstable predictions (an alternative subsample of traiinng data can significantly change the terminal nodes)
- Due to high variance, single regreesion trees have poor predictive accuracy

__Implementation details__

We can fit a regression tree using `rpart` and then visualize it using `rpart.plot`. The fitting process and the visual output of regression trees and classification trees are very similar. Both use the formula method for expressing the model (similar to `lm`). 

However, when fitting a regression tree, we need to set `method = "anova"`. By default, `rpart` will make an intelligent guess as to what the method value should be based on the data type of your response column, but it is recommended that you explicitly set the method for reproducibility reasons (since the auto-guess may change in the future).

```{r}
m1 <- rpart::rpart(
  formula = Sale_Price ~ .,
  data = ames_train,
  method = "anova"
)

m1 %>% rpart.plot::rpart.plot()
```

Once we have fit our we can take a peak at the `m1` output. This just explains steps of the splits. For example, we start with 2051 observations at the root node (very beginning) and the first variables we split on (the first variable that optimizes a reduction in SSE) is `Overall_Qual`. We see at the first node all observations with `Overall_Qual=Very_Poor,Poor,Fair,Below_Average,Average,Above_Average,Good` go to the 2nd (`2`)) branch. The total number of observations that follow this branch ($1699$), their average sales price ($156147.10$) and SSE ($4.001092e+12$) are listed. If you look for the 3rd branch (`3`)) you will see that $352$ observations with Overall_Qual=Very_Good,Excellent,Very_Excellent follow this branch and their average sales prices is 304571.10 and the SEE in this region is 2.874510e+12. Basically, this is telling us the most important variable that has the largest reduction in SEE initially is `Overall_Qual` with those homes on the upper end of the quality spectrum having almost double the average sales price.

```{r}
m1
```

We can visualize our model with `rpart.plot`. `rpart.plot` has many plotting options, which we will leave to the reader to explorer. However, in the default print, it will show the percentage of data that fall to that node and the average sales price for that branch. One thing you may notice is that this tree contains 11 internal nodes resulting in 12 terminal nodes. Basically, this tree is partitioning on 11 variables to produce its model. However, there are 80 variables in `ames_train`. So what happened?

```{r}
library(rpart.plot)
rpart.plot(m1)
```

Behind the scenes, `rpart` is automatically applying a range of cost compexity ($\alpha$ values to prune the tree). To compare the error for each $\alpha$ value, `rpart` performs a 10-fold cross validation so that the error associated with a given $\alpha$ value is computed on the hold-out validation data. In this example we find diminishing returns after 12 terminal nodes as illustrated below (y-axis is cross validation error, lower x-axis is cost complexity $\alpha$ value, upeer x-axis is the number of terminal nodes (tree size =$|T|$). You may also notice the dashed line which goes through the point $|T|=9$. Breiman et al. (1984) suggested that in actual practice, it is common to instead use the smallest tree within 1 standard deviation of the minimum cross validation error (aka the 1-SE rule).  Thus, we could use a tree with 9 terminal nodes and reasonably expect to experience similar results within a small margin of error.

```{r}
rpart::plotcp(m1)
```

To illustrate the point of selecting a tree with 12 terminal nodes (or 9 if you go by the 1-SE rule), we can force `rpart` to generate a full tree by using `cp=0` (no penalty results in a fully grown tree).  we can see that after 12 terminal nodes, we see diminishing returns in error reduction as the tree grows deeper. Thus, we can significantly prune our tree and still achieve minimal expected error.

```{r}
library(rpart)

m2 <- rpart(
  formula = Sale_Price ~.,
  data = ames_train,
  method = "anova",
  control = list(cp = 0, xval = 10)
)

plotcp(m2)
abline(v = 12, lty = "dashed")
```

So, by default, `rpart` is performing some automated tuning, with an optimal subtree of 11 splits, 12 terminal nodes, and a cross-validated error of 0.272 (note that this error is equivalent to the PRESS statistic but not the MSE). However, we can perform additional tuning to try improve model performance.

```{r}
m1$cptable %>% broom::tidy() 
```


#### Tuning {#RT_TUNE}

In addition to the cost complexity paramter($\alpha$), it is also common to tune:

- `minsplit`: the minimum nunber of data points required to attempt a split before it is forced to create a terminal node. The default is 20. Making this smaller allows for terminal nodes that may contain only a handful of observations to create the predicted value.

- `maxdepth`: the maximum number of internal nodes between the root node and the terminal nodes. The default is 30, which is quite liberal and allows for fairly large trees to be built.

`rpart` uses special `control` argument where we provide a list of hyperparameter values. For example, if we wanted to assess a model with `minsplit`=10 and `maxdepth`=12, we could execute the following:

```{r}
m3 <- rpart(
  formula = Sale_Price ~.,
  data = ames_train,
  method = "anova",
  control = list(minsplit=10, maxdepth=12, xval=10)
)

m3$cptable %>% broom::tidy()
```

Although useful, this approach requires you to manually assess multiple models. Rather, we can perform a grid search to automatically search across a range of differently tuned models to identify the optimal hyperparameter setting.

To perform a grid search we first create our hyperparameter grid. In this example, I search a range of `minsplit` from 5-20 and vary `maxdepth` from 8-15 (since our original model found an optimal depth of 12). What results is 128 different combinations, which requires 128 different models.

```{r}
hyper_grid <- expand.grid(
  minsplit = seq(5, 20, 1),
  maxdepth = seq(8, 15, 1)
)

head(hyper_grid)
```

To automate the modeling we simply set up a `for` loop and iterate through each minsplit and maxdepth combination. We save each model into its own list item.

```{r}
models <- list()

for (i in 1:nrow(hyper_grid)){
  
  # get minsplit, maxdepth values at row i
  minsplit <- hyper_grid$minsplit[i]
  maxdepth <- hyper_grid$maxdepth[i]
  
  # train a model and store in the list
  models[[i]] <- rpart(
    formula = Sale_Price~.,
    data = ames_train,
    method = "anova",
    control = list(minsplit = minsplit,
                   maxdepth = maxdepth)
  )
}

# option - purrr::map2()

h <- hyper_grid %>% 
  mutate(models=map2(minsplit, maxdepth,
    ~rpart(
      formula = Sale_Price~.,
      data = ames_train,
      method = "anova",
      control = list(minsplit = .x,
                     maxdepth = .y)
    ),
    tidy = broom::tidy(models)))

h

h$models[[1]]$cptable[,"xerror"]
```

We can now create a function to extract the minimum error associated with the optimal cost complexity $\alpha$ value for each model. After a little data wrangling to extract the optimal $\alpha$ value and its respective error, adding it back to our grid, and filter for top 5 minimal error values we see that the optimal model makes a slight improvement over our ealier model (xerror of 0.242 versus 0.272).

```{r eval=FALSE}
# function to get optimal cp
get_cp <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  cp <- x$cptable[min, "CP"] 
}

# function to get minimum error
get_min_error <- function(x) {
  min    <- which.min(x$cptable[, "xerror"])
  xerror <- x$cptable[min, "xerror"]}


h %>%
  mutate(
    cp    = purrr::map_dbl(models, get_cp),
    error = purrr::map_dbl(models, get_min_error)) %>% 
  arrange(error) %>%
  top_n(-5, wt = error)
##   minsplit maxdepth        cp     error
## 1       15       12 0.0100000 0.2419963
## 2        5       13 0.0100000 0.2422198
## 3        7       10 0.0100000 0.2438687
## 4       17       13 0.0108982 0.2468053
## 5       19       13 0.0100000 0.2475141
```

If we were satisfied with these results, we could apply this final optimal model and predict on our test set. The final RMSE is $39145.39$ which suggests that, on average, our predicted sales prices are about $39,145 off from the actual sales price.

```{r}
optimal_tree <- rpart(
  formula = Sale_Price ~.,
  data = ames_train,
  method = "anova",
  control = list(minsplit = 11,
                 maxdepth = 8,
                 cp       = 0.01)
)

pred <- predict(optimal_tree, newdata = ames_test)
RMSE(pred = pred, obs = ames_test$Sale_Price)
```


#### Bagging {#RT_BAGG}

__The Idea__

As previously mentioned, single tree models suffer from high varianace. Although pruning the tree helps reduce this variance, there are alternative methods that actually exploit the variability of single trees in a way that can significantly improve performance over and above that of _Bootstrap Aggregate (Bagging)_ is one such approach (oroginally proposed by [Breiman, 1996](https://link.springer.com/article/10.1023%2FA%3A1018054314350))

Bagging combines and averages multiple models. Averaging across multiple trees reduces the variability of any tone tree and reduces overfitting, which imrpoves predictive performance. Bagging follows three simple steps:

1. Create m [bootstrap samples](http://uc-r.github.io/bootstrapping) from the training data.Boostrapped samples allow us to create many slightly different data sets but with the same distribution as the overall training set.
2. For each boostrap sample train a single, unpruned regression tree.
3. Average individual predictions from each tree to create an overall average predicted value.

```{r}
knitr::include_graphics("C:/Protected/Data Science/UC Business Analytics/image/bagging3.png")
```

This process can actually be applied to any regression or classification model; however, it provides the greatest improvement for models that have high variance. For example, more stable parametric models such as linear regression and multi-adaptive regression splines tend to experience less improvement in predictive performance.

One benefit of bagging is that, on average, a bootstrap sample will contain 63% of the training data. This leaves about 33% of the data out of the bootstrapped sample. We call this is the __out-of-bag (OOB)__ sample. We can use the OOB observations to estimate the model's accuracy, creating a natural cross-validation process.

__Bagging with `ipred`__

Fitting a bagged tree model is quite simple. Instead of using `rpart` we use `opred::bagging`. We use `coob=TRUE` to use the OOB sample to estimate the test error. We see that our initial estimate error is close to $3K less than the test error we achieved with our singple optimal tree (36543 vs 39145).

```{r}
# make boostrapping reproducible
set.seed(123)

# train bagged model
bagged_m1 <- bagging(
  formula = Sale_Price ~ .,
  data = ames_train,
  coob = TRUE
)

bagged_m1
## 
## Bagging regression trees with 25 bootstrap replications 
## 
## Call: bagging.data.frame(formula = Sale_Price ~ ., data = ames_train, 
##     coob = TRUE)
## 
## Out-of-bag estimate of root mean squared error:  36543.37

```

One thing to note is that typically, the more trees the better. As we add more trees we are averaging over more high variance single trees. What result is that early on, we see a dramatic reduction in variance (and hence our error) and eventually the reduction in error will flatline signaling an appropriate number of trees to create a stable model.

Rarely you need more than 50 trees to stabilize the error.

By default, `bagging` performs 25 booststrap samples and trees but we may require more. We can assess the error versus number of trees as below. We see that the error is stabilizing at about 25 trees so we will likely not gain much improvement by simply bagging more trees.

```{r}
# assess 10-50 bagged trees
ntree <- 10:50

# create empty vector to store OOB RMSE values
rmse <- vector(mode = "numeric", length=length(ntree))

for (i in seq_along(ntree)){
  # reproducibility
  set.seed(123)
  
  # perform bagged model
  model <- bagging(
    formula = Sale_Price ~.,
    data = ames_train,
    coob = TRUE,
    nbagg = ntree[i]
  )
  
  # get OOB error
  rmse[i] <- model$err
}

plot(ntree, rmse, type = 'l', lwd = 2)
abline(v = 25, col = "red", lty = "dashed")
```

__Bagging with `caret`__

Bagging with `ipred` is quite simple; however, there are some additional benefits of bagging with `caret`.

1. It is easier to perform cross-validation. Although we can use the OOBE error, performing cross validation will provide a more robust understanding of the true expected test error.
2. We can assess variable important across the bagged trees.

Here, we performa a 10-fold corss-validated model. We see that the cross-validated RMSE is $36,477$. We also assess the top 20 variables from our model.

Variable importance for regression trees is measured by assessing the total amount SSE is decreased by splits over a given predictor, averaged over all $m$ trees. The predictors with the largest average impact to SSE are considered most important. The importance value is simple the relative mean decrease in SSE compared to the most important variables.

```{r}
# specify 10-fold cross validation
library(caret)
ctrl <- trainControl(method = "cv", number = 10)

# CV bagged model
bagged_cv <- train(
  Sale_Price ~.,
  data = ames_train,
  method = "treebag",
  trControl = ctrl,
  importance = TRUE
)

# assess results
bagged_cv

## Bagged CART 
## 
## 2051 samples
##   80 predictor
## 
## No pre-processing
## Resampling: Cross-Validated (10 fold) 
## Summary of sample sizes: 1846, 1845, 1847, 1845, 1846, 1847, ... 
## Resampling results:
## 
##   RMSE      Rsquared   MAE     
##   36477.25  0.8001783  24059.85

# plot most important variables
plot(varImp(bagged_cv), 20)  

```



#### Learning more {#RT_MORE}

Decision trees provide a very intuitive modeling approach that have several, flexible benefits. Unfortunately, they suffer from high variance; however, when you combine them with bagging you can minimize this drawback. Moreover, bagged trees provides the fundamental basis for more complex and powerful algorithms such as random forests and gradient boosting machines. To learn more I would start with the following resources listed in order of complexity:

- An Introduction to Statistical Learning
- Applied Predictive Modeling
- [Classification and Regression Trees](https://www.amazon.com/Classification-Regression-Wadsworth-Statistics-Probability/dp/0412048418)
- The Elements of Statistical Learning
- [Bagging Predictors](https://link.springer.com/article/10.1023%2FA%3A1018054314350)

### Random forests 

[Bagging (bootstrap aggregating) regression trees](http://uc-r.github.io/regression_trees) is a technique that can turn a single tree model with high variance and poor preditive power into a fairly accurate prediction function.

However, bagging trees typically suffers from tree correlation, which reduces the overall performance of the model.

Random forests are a modification of bagging that builds a large collection of _de-correlated_ trees and have become a very popular "out-of-the-box" learning algorithm that enjoys good predictive peformance. This tutotiral will cover the fundamentals of random forests.

#### tl;dr

This tutorial serves as an introduction to the random forests. This tutorial will cover the following material:

- [Replication Requirements](#RF_RR): What youfll need to reproduce the analysis in this tutorial.
- [The idea](#RF_Idea): A quick overview of how random forests work.
- [Basic implementation](#RF_BI): Implementing regression trees in R.
- [Tuning](#RF_Tune): Understanding the hyperparameters we can tune and performing grid search with ranger & h2o.
- [Predicting](#RF_Pred): Apply your final model to a new data set to make predictions.
- [Learning more](#RF_LM): Where you can learn more

#### Replication Requirements {#RF_RR}

This tutorial leverages the following packages. Some of these packages play a supporting role; however, we demonstrate how to implement random forests with several different packages and discuss the pros and cons to each.

```{r}

library(rsample)      # data splitting 
library(randomForest) # basic implementation
library(ranger)       # a faster implementation of randomForest
library(caret)        # an aggregator package for performing many machine learning models
library(h2o)          # an extremely fast java-based platform

```

To illustrate various regularization concepts we will use the Ames Housing data that has been included in the `AmesHousing` package.

```{r}

# Create training (70%) and test (30%) sets for the AmesHousing::make_ames() data.
# Use set.seed for reproducibility
set.seed(123)

amessplit <- initial_split(AmesHousing::make_ames(), prop=.7)
ames_train <- training(ames_split)
ames_test <- testing(ames_split)

```

#### The idea {#RF_Idea}

Random forests are built on the same fundamental principles as decision trees and bagging (check out this [tutorial](http://uc-r.github.io/regression_trees) if you need a refresher on these techniques). 

Bagging trees introduces a random component in to the tree building process that reduces the variance of a single tree's prediction and improve predictive performance. However, the trees in bagging are not completely independent of each other since all the original predictors are considered at every split of every tree. Rather, trees from different bootstrap samples typically have similar structure to each other (especially at the top of the tree) due to underlying relationships.

For example, if we create six decision trees with different bootstrapped samples of the Boston housing data, we see that the top of the trees all have a very similar structure. Although there are 15 predictor variables to split on, all six trees have both `lstat` and `rm` variables driving the first few splits.

This characteristic is known as _tree correlation_ and prevents bagging from optimally reducing variance of the predictive values. In order to reduce variance further, we need to minimize the amount of correlation between the trees. 

This can be achieved by injecting more randomness into the tree-growing process. random forecast achieve this in two ways:

1. __Bootstrap__: similar to bagging, each tree is grown to a bootstrap resampled data set, which makes them different and somewhat decorrelates them.

2. __Split-variable randomization__: each time a split is to be performed, the search for the split variable is limited to a random subset of _m_ of the _p_ variables. 

For regression trees, typical default values are $m = \frac{p}{3}$, but this should be considere a tuning parameter. When $m=p$, the randomization amounts to using only step 1 and is the same as __bagging__.

The basic algorthim for a regression random forest can be generalized to the following:

```{r eval=FALSE}

1.  Given training data set
2.  Select number of trees to build (ntrees)
3.  for i = 1 to ntrees do
4.  |  Generate a bootstrap sample of the original data
5.  |  Grow a regression tree to the bootstrapped data
6.  |  for each split do
7.  |  | Select m variables at random from all p variables
8.  |  | Pick the best variable/split-point among the m
9.  |  | Split the node into two child nodes
10. |  end
11. | Use typical tree model stopping criteria to determine when a tree is complete (but do not prune)
12. end

```

since the algorithm randomly selects a bootstrap sample to traini on __and__ predictors to use at each split, tree correlation will be lessened beyond bagged trees.

__OOB error vs test set error__

Similar to bagging, a natural benefit of the bootstrap resampling process is that random forests have an out-of-bag (OOB) sample that provides an eifficient and reasonable approximation of the test error. This provides a built-in validation set without an extra work on your part, and you do not need to sacrifice any of your training data to use for validation.

This makes identifying the number of trees required to stabilize the error rate during tuning more efficient; however, as illustrated below, some difference between the OOB erro and test error are expected.

Furthermore, many packages do not keep track of which observations were part of the OOB sample for a given tree and which were not. If you are comparing multiple models to one-another, youfd want to score each on the same validation set to compare performance. Also, although technically it is possible to compute certain metrics such as root mean squared logarithmic error (RMSLE) on the OOB sample, it is not built in to all packages. So if you are looking to compare multiple models or use a slightly less traditional loss function you will likely want to still perform cross validation.

__Advantages & Disadvantages__
_Advantages:_

- Typically have very good performance
- Remarkably good gout-of-the boxh - very little tuning required
- Built-in validation set - donft need to sacrifice data for extra validation
- No pre-processing required
- Robust to outliers

_Disdvantages:_

- Can become slow on large data sets
- Although accurate, often cannot compete with advanced boosting algorithms
- Less interpretable


#### Basic implementation {#RF_BI}

There are over 20 random forest packages in R.1 To demonstrate the basic implementation we illustrate the use of the `randomForest` package, the oldest and most well known implementation of the Random Forest algorithm in R. However, as your data set grows in size `randomForest` does not scale well (although you can parallelize with `foreach`). Moreover, to explore and compare a variety of tuning parameters we can also find more effective packages. Consequently, in the [Tuning](#RF_Tune) section we illustrate how to use the `ranger` and `h2o` packages for more efficient random forest modeling.

`randomForest::randomForest` can use the formula or separate x, y matrix notation for specifying our model. Below we apply the default `randomForest` model using the formulaic specification.

The default random forest performs 500 trees and $\frac{features}{3} = 26$ randomly selected predictor variables at each split. Averaging across all 500 trees provides an OOB $MSE=659550782 (RMSE=25682)$

```{r}
# for reproducibility
set.seed(123)

# default RF model
m1 <- randomForest(
  formula = Sale_Price ~.,
  data = ames_train
)

# m1.caret <- caret::train(
#   Sale_Price ~., 
#   data=ames_train, 
#   method = "rf",
#   preProcess = c("center", "scale", "zv"),
#   trcontrol = trainControl(method="cv"))
# https://www.rdocumentation.org/packages/caret/versions/6.0-82/topics/preProcess

# predicted value computation
# predRF <- predict(m1.caret, ames_test)

# RMSE computation
# sqrt(sum((ames_train$Sale_Price - predRF)^2))
```

Plotting the model will illustrate the error rate as we average across more trees and shows that our error rate stabilizies with around 100 trees but continues to decrease slowly around 300 or so trees.

```{r}
plot(m1)
```

The plotted error rate above is based on the OOB sample error and can be accessed directly at `m1$mse`. Thus, we can find which number of trees providing the lowest error rate, which is 344 trees providing an average home sales price error of $25,673$

```{r}
# number of trees with lowest MSE
which.min(m1$mse)

# RMSE of this optimal random forest
sqrt(m1$mse[which.min(m1$mse)])

```

`randomForest` also allows us to use a validatio set to measure predictive accuracy if we did not want to use the OOB samples. Here we split our training set further to create a training and validation set.

We then supply the validation data in the `xtest` and `ytest` arguments. 

```{r}
# create training and validation data
set.seed(123)
valid_split <- initial_split(ames_train, .8)

# training data
ames_train_v2 <- analysis(valid_split)

# validation data
ames_valid <- assessment(valid_split)
x_test <- ames_valid[setdiff(names(ames_valid), "Sale_Price")]
y_test <- ames_valid$Sale_Price

rf_oob_comp <- randomForest(
  formula = Sale_Price ~ .,
  data    = ames_train_v2,
  xtest   = x_test,
  ytest   = y_test
)

# extract OOB & validation errors
oob <- sqrt(rf_oob_comp$mse)
validation <- sqrt(rf_oob_comp$test$mse)

# compare errror rates
tibble::tibble(
  `Out of Bag Error` = oob,
  `Test error` = validation,
  ntrees = 1:rf_oob_comp$ntree
) %>%
  gather(Metric, RMSE, -ntrees) %>%
  ggplot(aes(ntrees, RMSE, color = Metric)) +
  geom_line() +
  scale_y_continuous(labels = scales::dollar) +
  xlab("Number of trees")
```

Random forests are one of the best "out-of-the-box" machine learning algorithms. They typically perform remarkably well with very little tuning required. For example, as we saw above,  we were able to get an RMSE of less than `$30K` without any tuning which is over a `$6K` reduction to the RMSE achieved with a fully-tuned [bagging model](http://uc-r.github.io/regression_trees#bag) and $4K reduction to to a fully-tuned [elastic net model](http://uc-r.github.io/regularized_regression#elastic). However, we can still seek improvement by tuning our random forest model.


#### Tuning {#RF_Tune}

Random forests are fairly easy to tune since there are only a handful of tuning parameters. Typically, the primary concern when starting out is tuning the number of candidate variables to select from each split. However, there are a few additional hyperparameters that we should be aware of. Although the argument names may differ across packages, these hyperparameters should be present:

- `ntree`: number of trees. We want enough trees to stabalize the error but using too many trees is unncessarily inefficient, especially when using large data sets.

- `mtry`: the number of variables to randomly sample as candiates at each split. When `mtry`=p, the model equates to bagging. When `mtry`=1, the split variable is completely random, so all variable get a chance btu can lead to overly biased results. A common suggestion is to start with 5 values evenly spaced across the range from 2 to $p$.

- `sampsize`:  the number of samples to train on. The default value is 63.25% of the training set since this is the expected value of unique observations in the bootstrap sample. Lower sample sizes can reduce the training time but may introduce more bias than necessary. Increasing the sample size can increase performance but at the risk of overfitting because it introduces more variance. Typically, when tuning this parameter we stay near the 60-80% range.

- `nodesize`: minimum number of samples within the terminal nodes. Controls the complexity of the trees. Smaller node size allows for deeper, more complex trees and smaller node results in shallower trees. This is another bias-variance tradeoff where deeper trees introduce more variance (risk of overfitting) and shallower trees introduce more bias (risk of not fully capturing unique patters and relatonships in the data).

- `maxnodes`: maximum number of terminal nodes. Another way to control the complexity of the trees. More nodes equates to deeper, more complex trees and less nodes result in shallower trees.

__Initial tuning with randomForest__

If we are interested with just starting out and tuning the `mtry` parameter we can use `randomForest::tuneRF` for a quick and easy tuning assessment. 

`tuneRf` will start at a value of `mtry` that you supply and increase by a certain step factor until the OOB error stops improving be a specified amount. For example, the below starts with `mtry=5` and increases by a factor of 1.5 until the OOB error stops improving by 1%. Note that `tuneRF` requires a separate `xy` specification. We see that the optimal `mtry` value in this sequence is very close to the default `mtry` value of $\frac{features}{3}=26$.

```{r}
# names of features
features <- setdiff(names(ames_train), "Sale_Price")

set.seed(123)

m2 <- tuneRF(
  x = ames_train[features],
  y = ames_train$Sale_Price,
  ntreeTry = 500,
  mtryStart = 5,
  stepFactor = 1.5,
  improve = 0.01,
  trace = FALSE # to not show real-time progress
)

## -0.02973818 0.01 
## 0.0607281 0.01 
## 0.01912042 0.01 
## 0.02776082 0.01 
## 0.01091969 0.01 
## -0.01001876 0.01

```

__Full grid search with ranger__

To perfrom a larger grid search across several hyperparameters, we will need to create a grid and loop through each hyperparameter combination and evaluate the model. Unfortunately, this is where `randomForest` becomes quite inefficient since it does not scale well. Instead, we can use `ranger` wihch is a C++ implementation of Brieman's random forest algorithm, and as the folliwing illustrates, is over 6 times faster than `randomForest`.

```{r}
#randomForest speed
system.time(
  ames_randomForest <- randomForest(
    formula = Sale_Price ~.,
    data = ames_train,
    ntree = 500,
    mtry = floor(length(features)/3)
  )
)
##    user  system elapsed 
##  55.371   0.590  57.364

# ranger speed
system.time(
  ames_ranger <- ranger(
    formula = Sale_Price ~.,
    data = ames_train,
    num.tree = 500,
    mtry = floor(length(features)/3)
  )
)
##    user  system elapsed 
##   9.267   0.215   2.997
```

To perform the grid search, first we want to construct our grid of hyperparameters.We are going to search across 96 different models with varying `mtry`, minimum node size and sample size.

```{r}
# hyperparameter grid srearch

hyper_grid <- expand.grid(
  mtry = seq(20, 30, by = 2),
  node_size = seq(3, 9, by = 2),
  sample_size = c(.55, .632, .7, .8),
  OOB_RMSE = 0
)
  
# total number of combinations
nrow(hyper_grid)
```

We look through each yperparameter combination and apply 500 trees since our previous examples illustrated that 500 was plenty to achieve a stable error rate. Also note that we set the random number generator seed. This allows us to consistently sample the same observations for each sample size and make it more clear the impact that each change makes. Our OOB RMSE ranges between ~26,000-27,000. Our top 10 performing models all have RMSE values right around 26,000 and the results show that models with slighly larger sample sizes (70-80%) and deeper trees (3-5 observations in an terminal node) perform best. We get a full range of `mtry` values showing up in our top 10 so is does not look like that is over influential.

```{r}

for (i in 1:nrow(hyper_grid)){
  # train model
  model <- ranger(
    formula = Sale_Price ~.,
    data = ames_train,
    num.trees = 500,
    mtry = hyper_grid$mtry[i],
    min.node.size = hyper_grid$node_size[i],
    sample.fraction = hyper_grid$sample_size[i],
    seed = 123)
  
  # add OOB error to grid
  hyper_grid$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid %>% 
  dplyr::arrange(OOB_RMSE) %>% 
  head()
```

Although, random forests typically perform quite well with categorical variables in their original columnar form, it is worth checking to see if alternative encodings can increase performance. For example, the following one-hot encodes our categorical variables which produces 353 predictor variables versus the 80 we were using above. We adjust our `mtry` parameter to search from 50-200 random predictor variables at each split and re-perform our grid search. The results suggest that one-hot encoding does not improve performance.

```{r}
# one-hot encode our categorical variables
# caret package dummyVars()
one_hot <- dummyVars(~., ames_train, fullRank=FALSE)
ames_train_hot <- predict(one_hot, ames_train) %>% as.data.frame()

# makr ranger compatible names
names(ames_train_hot) <- make.names(names(ames_train_hot), allow_=FALSE)
ames_train_hot

#hyperparameter grid search --> same as above but with increased mtry values
hyper_grid_2 <- expand.grid(
  mtry       = seq(50, 200, by = 25),
  node_size  = seq(3, 9, by = 2),
  sampe_size = c(.55, .632, .70, .80),
  OOB_RMSE  = 0
)

# perform grid search
for(i in 1:nrow(hyper_grid_2)) {
  
  # train model
  model <- ranger(
    formula         = Sale.Price ~ ., 
    data            = ames_train_hot, 
    num.trees       = 500,
    mtry            = hyper_grid_2$mtry[i],
    min.node.size   = hyper_grid_2$node_size[i],
    sample.fraction = hyper_grid_2$sampe_size[i],
    seed            = 123
  )
  
  # add OOB error to grid
  hyper_grid_2$OOB_RMSE[i] <- sqrt(model$prediction.error)
}

hyper_grid_2 %>% 
  dplyr::arrange(OOB_RMSE) %>%
  head(10)
##    mtry node_size sampe_size OOB_RMSE
## 1    50         3        0.8 26981.17
## 2    75         3        0.8 27000.85
## 3    75         5        0.8 27040.55
## 4    75         7        0.8 27086.80
## 5    50         5        0.8 27113.23
## 6   125         3        0.8 27128.26
## 7   100         3        0.8 27131.08
## 8   125         5        0.8 27136.93
## 9   125         3        0.7 27155.03
## 10  200         3        0.8 27171.37

```

Currently, the best random forest model we have found retains columnar categorical variables and uses `mtry` = 24, terminal node size of 5 observations, and a sample size of 80%. Lets repeat this model to get a better expectation of our error rate. We see that our expected error ranges between ~25,800-26,400 with a most likely just shy of 26,200.

```{r}
OOB_RMSE <- vector(mode="numeric", length=100)

for (i in seq_along(OOB_RMSE)){
  
  optimal_ranger <- ranger(
    formula = Sale_Price ~.,
    data = ames_train,
    num.trees = 500,
    mtry = 24,
    min.node.size = 5,
    sample.fraction =.8,
    importance = "impurity"
  )
   OOB_RMSE[i] <- sqrt(optimal_ranger$prediction.error)
}

hist(OOB_RMSE, breaks = 20)
```

Furthermore, you may have noticed we set `imporance ="impurity` in the above modeling, which allows us to assess variable importance. Variable importance is measured by recording the decrease in MSE each time a variable is used as a node split in a tree.

The remaining error left in predictive accuracy after a node split is known as __node impurity__ and a variable that reduces this impurity is considered more imporant than those variables that do not. Consequently, we accumulate the reduction in MSE for each variable across all the trees and the variable with the greatest accumulated impact is considered the more important, or impactful. We see that `Overall_Qual` has the greatest impact in reducing MSE across our trees, followed by `Gr_Liv_Area`, `Garage_Cars`, etc.

```{r}
optimal_ranger$variable.importance %>% 
  tidy() %>% 
  dplyr::arrange(desc(x)) %>% 
  dplyr::top_n(25) %>% 
  ggplot(aes(reorder(names,x), x))+
  geom_col()+
  coord_flip()+
  ggtitle("Top 25 imporant variables")
```

__Full grid search with H2O__

If you ran the grid search code above you probably noticed the code took a while to run. Although `ranger` is computationally efficient, as the grid search space expands, the manual `for` loop process becomes less efficient. `h2o` is a powerful and efficient java-based interface that provides parallel distributed algorithms. Moreover, `h2o` allows for different optimal search paths in our grid search. This allows us to be more efficient in tuning our models. Here, I demonstrate how to tune a random forest model with `h2o`. Lets go ahead and start up `h2o`:

```{r eval=FALSE}
# start up h2o (I turn off progress bars when creating reports/tutorials)
h2o.no_progress()
h2o.init(max_mem_size = "5g")
```

First, we can try a comprehensive ( __full cartesian__) grid search, which means we will examine every combination of hyperparameter settings that we specify in `hyper_grid.h2o`. Here, we search across 96 models but since we perform a full cartesian search this process is not any faster than that which we did above. However, note that the best performing model has an OOB RMSE of 24504 ($\sqrt{6.0046E8}$), which is lower than what we achieved previously. This is because some of the default settings regarding minimum node size, tree depth, etc. are more ggeneroush than ranger and randomForest (i.e. h2o has a default minimum node size of one whereas ranger and randomForest default settings are 5).

```{r eval=FALSE}
# create feature names
y <- "Sale_Price"
x <- setdiff(names(ames_train), y)

# turn training set into h2o object
train.h2o <- as.h2o(ames_train)

# hyperparameter grid
hyper_grid.h2o <- list(
  ntrees      = seq(200, 500, by = 100),
  mtries      = seq(20, 30, by = 2),
  sample_rate = c(.55, .632, .70, .80)
)

# build grid search
grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid",
  x = x,
  y =y,
  training_frame = train.h2o,
  hyper_params = hyper_grid.h2o,
  searchsearch_criteria = list(strategy = "Cartesian")
)

# collect the results and sor by our model performance metric of choice
grid_perf <- h2o.getGrid(
  grid_id = "rf_grid",
  sort_by = "mse",
  decreasing = FALSE
)

print(grid_perf)

```

Because of the combinatorial explosion, each additional hyperparameter that gets added to our grid search has a huge effect on the time to complete. Consequently, `h2o` provides an additional grid search path called __gRandomDiscreteh__, which will jump from one random combination to another and stop once a certain level of improvement has been made, certain amount of time has been exceeded, or a certain amount of models have been ran (or a combination of these have been met). Although using a random discrete search path will likely not find the optimal model, it typically does a good job of finding a very good model.

For example, the following code searches a large ggrid search of 2,025 hyperparameter combinations. e create a random grid search that will stop if none of the last 10 models have managed to have a 0.5% improvement in MSE compared to the best model before that. If we continue to find improvements then I cut the grid search off after 600 seconds (30 minutes). Our grid search assessed 190 models and the best model (`max_depth` = 30, `min_rows` = 1, `mtries` = 25, `nbins` = 30, `ntrees` = 200, `sample_rate` = .8) achived an RMSE of 24686 ($\sqrt{6.094E8}$).

```{r eval=FALSE}
#hyperparameter grid search criteria

search_criteria <- list(
  strategy = "RandomDiscrete",
  stopping_metric = "mse",
  stopping_tolerance = 0.005,
  stopping_rounds = 10,
  max_runtime_secs = 30*60
)

# build grid search
random_grid <- h2o.grid(
  algorithm = "randomForest",
  grid_id = "rf_grid2",
  x = x,
  y = y,
  training_frame = train.h2o,
  search_criteria = search_criteria
)

# collect the results and sort by our model performance metric of choice
grid_perf2 <- h2o.getGrid(
  grid_id = "rf_grid2", 
  sort_by = "mse", 
  decreasing = FALSE
  )
print(grid_perf2)
## H2O Grid Details
## ================
## 
## Grid ID: rf_grid2 
## Used hyper parameters: 
##   -  max_depth 
##   -  min_rows 
##   -  mtries 
##   -  nbins 
##   -  ntrees 
##   -  sample_rate 
## Number of models: 190 
## Number of failed models: 0 
## 
## Hyper-Parameter Search Summary: ordered by increasing mse
##   max_depth min_rows mtries nbins ntrees sample_rate          model_ids
## 1        30      1.0     25    30    200         0.8 rf_grid2_model_114
## 2        30      1.0     30    30    400         0.8  rf_grid2_model_60
## 3        25      1.0     20    25    200         0.8  rf_grid2_model_62
## 4        20      1.0     20    15    400         0.8  rf_grid2_model_48
## 5        20      1.0     15    15    350        0.75 rf_grid2_model_149
##                   mse
## 1  6.09386451519276E8
## 2 6.141013192008269E8
## 3 6.143676001174936E8
## 4 6.181798579219993E8
## 5 6.182797259475644E8
## 
## ---
##     max_depth min_rows mtries nbins ntrees sample_rate          model_ids
## 185        30      5.0     30    15    500        0.55 rf_grid2_model_126
## 186        35      5.0     15    10    300        0.55  rf_grid2_model_84
## 187        25      5.0     15    20    200        0.55  rf_grid2_model_20
## 188        35      5.0     15    10    200        0.55 rf_grid2_model_184
## 189        40      1.0     15    20    400        0.55 rf_grid2_model_127
## 190        30      1.0     25    20    500       0.632 rf_grid2_model_189
##                      mse
## 185  7.474602079646143E8
## 186  7.530174943920757E8
## 187  7.591548840980767E8
## 188  7.721963479865576E8
## 189 1.0642428537243171E9
## 190 1.4912496290688899E9
```

Once wefve identifed the best model we can get that model and apply it to our hold-out test set to compute our final test error. We see that wefve been able to reduce our RMSE to near $23,000, which is a $10K reduction compared to elastic nets and bagging.

```{r eval=FALSE}
#grab the model id for the top model, chosen by validation error

best_model_id <- grid_perf2@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)

# Now letfs evaluate the model performance on a test set
ames_test.h2o <- as.h2o(ames_test)
best_model_perf <- h2o.performance(model = best_model, newdata = ames_test.h2o)

# RMSE of best model
h2o.mse(best_model_perf) %>% sqrt()
## [1] 23104.67
```

#### Predicting {#RF_Pred}

Once wefve identified our preferred model we can use the traditional `predict` function to make predictions on a new data set. We can use this for all our model types (`randomForest`, `ranger`, and `h2o`); although the outputs differ slightly. Also, not that the new data for the `h2o` model needs to be an h2o object.

```{r}
# randomForest
pred_randomForest <- predict(ames_randomForest, ames_test)
head(pred_randomForest)
##        1        2        3        4        5        6 
## 128266.7 153888.0 264044.2 379186.5 212915.1 210611.4

# ranger
pred_ranger <- predict(ames_ranger, ames_test)
head(pred_ranger$predictions)
## [1] 128440.6 154160.1 266428.5 389959.6 225927.0 214493.1

# h2o - commented
# pred_h2o <- predict(best_model, ames_test.h2o)
# head(pred_h2o)
##    predict
## 1 126903.1
## 2 154215.9
## 3 265242.9
## 4 381486.6
## 5 211334.3
## 6 202046.5
```


#### Learning more {#RF_LM}

Random forests provide a very powerful out-of-the-box algorithm that often has great predictive accuracy. Because of their more simplistic tuning nature and the fact that they require very little, if any, feature pre-processing they are often one of the first go-to algorithms when facing a predictive modeling problem. To learn more I would start with the following resources listed in order of complexity:

- An Introduction to Statistical Learning
- Applied Predictive Modeling
- Computer Age Statistical Inference
- The Elements of Statistical Learning
