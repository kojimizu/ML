---
title: "UC business Analytics R programming Guide 3"
author: "Koji Mizumura"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```


# Time series analysis
## Exploring & visualizing time series

Time series forecasting is performed in nearly every organization that works with quantifiable data. Retail stores forecast sales. Energy companies forecast reserves, production, demand, and prices. Educational institutions forecast enrollment. Goverments forecast tax receipts and spending. International financial organizations forecast inflation and economic activity. The list is long but the point is short - forecasting is a fundamental analytic process in every organization. The purpose of this tutorial is to get you started doing some fundamental time series exploration and visualization.

### tl;dr

This tutotorial serves as an introduction to exploring and visualizing time series data and covers:

1. [Replication requirements](#TS_RR): What you will need to reproduce the analysis
2. [Creating time series objects](#TS_TS): Convert your data to a `ts` object for time series analysis.
3. [Time series plots](#TS_Plot): Basic visualization of `ts` objects and differentiating trends, seasonality, and cycle variation.
4. [Seasonal plots](#TS_Seasonal): Plotting seasonality trends in time series data.
5. [Autocorrelation of time series](#RS_ACF): Computing and visualizing autocorrelation.
6. [White noise](#TS_WN): Differentiating signal from the noise.
7. [Exercises](#TS_Exercise): Practice what youfve learned.

### Replication Requirements 

This tutorial leverages a variety of data sets to illustrate unique time series features. The data sets are all provided by the `forecast` and `fpp2` packages. Furthermore, these packages provide various functions for computing and visualizing basic time series components.

```{r}
library(tidyverse)
library(tidymodels)
library(magrittr)
library(forecast)
library(fpp2)
```

### Creating time series objects 

A time series can be thought of as a vector or matrix numbers, along with some information about what times those numbers were recorded. This information is stored in a `ts` object in R. 

In most examples and exercies throughout the forecasting tutorial ou will use data that are already in the time series format. However, if you want to work with your own data, you need to know how to create a `ts` object in R.

Here, I illustrate how to convert a data frame to a `ts` object. First, letfs assume we have a data frame named `pass.df` that looks like the following where we have the total number of airline passengers for each month for the years 1949-1960.

```{r}
airpass %>% head()
```

We can convert this data frame to a time series object by us the ts() function. Here, thec

- __first argument__ supplies it the `pass.df` data frame and we index for just the columns with the data (we store the date-time data separately).
- __second argument__ supplies the start date for the first observation (first period in 1949).
- __third argument__ identifies the frequency, which in this case is monthly (hence 12 months in a year).

```{r}
airpass %>% as.tibble() %>% 
  ts(., start=c(1949,1), frequency=12)

# pass.ts <- ts(pass.df["AirPassengers"], start = c(1949, 1), frequency = 12)
```

We now have convered our data fraime into a time series object

```{r}
str(airpass)
airpass
```

Go ahead and compute this `pass.ts` time series to the built-in `AirPassengers` data set.

### Time series plot {#TS_Plot}

The first step in any data analysis is to plot the data. Graphs enable you to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. Just as the type of data determines which forecasting method to use, it also determines which graphs are appropriate.

Here, we use the `autoplot()` function to produce time plots of ts data. In time series plots, we should always look for outliers, seasonal patterns, overall trends, and other interesting features. This plot starts to illustrate the obvious trends that emerge over time.

```{r}
autoplot(AirPassengers)
```

Often, we will have have  time series data that has multiple variables. For example, the `fpp2::arrivals` data set has time series data for "quarterly international arrivals (in thousands) Australia from Japan, New Zealand, UK and the US 1919Q1 - 2012Q3".

So this time series data has two variables (over and above the time stamp data) - (1) arrivals in thousands and (2) country.
 
```{r}
head(arrivals)
```
 
We can compare the trends across the different variables (countries) either in one plot or use the facetting option to separate the plots:

```{r}
autoplot(arrivals)
autoplot(arrivals, facets = T)
```

You may have noticed that `autoplot()` looks a lot like `ggplot2` outputs. Thatfs because many of the visualizations in the `forecast` package are built on top of `ggplot2`. This allows us to easily add on to these plots with ggplot2 syntax. For example, we can add a smooth trend line and adjust titles:

```{r}
autoplot(arrivals, facets = TRUE) +
  geom_smooth() +
  labs(title   = "International arrivals to Australia",
       y       = "Arrivals (in thousands)",
       x       = NULL,
       subtitle =  "Each country has different trends.")
```

These initial visualizations may spor additional questions such as what was the min, max, or average arrival amount for Japan. We can index and use many normail functions to assess these types of questions.

```{r}
# index for Japan
japan <- arrivals[, "Japan"]

# Identify max arrival amount
summary(japan)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   9.321  74.135 135.461 122.080 176.752 227.641
```

You can also use the `frequency()` function to get the number of observations per unit time. This example returns 4 which means the data are recorded on a quarterly interval.

```{r}
frequency(japan)
```

In viewing time series plots we can describe different components. We can describe the common components using this quarterly cement production data.

```{r}
autoplot(fpp2::qcement)
```

- the __trend__ is the long-term increase or decrease in the data. There is an increasing trend in the cement data.
- the __seasonal__ pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. The quarterly cement data above shows seasonality likely induced by the change in weather and its impact on being able to pour cement.
- the __cycle__ occurs when the data exhibit rises and falls that are not of a fixed period. These fluctuations are usually due to economic conditions and are often related to the gbusiness cycleh. We can see a few cycles in our cement data in the early f80s, f90s, f00s, and around 2008 - all these date ranges are around economic depressions that occurred.

Later tutorials will illustrate how to decompose each of these components; however, next we will look at how to do some initial investigating regarding seasonal patterns.

### Seasonal plots {#TS_Seasonal}

There are a few useful ways of plotting data to emphasize seasonal patterns and show changes in these patterns over time. First, a seasonal plot is similar to a time plot except that the data are plotted against the individual gseasonsh in which the data were observed. We can produce a seasonal plot with `ggseasonplot()`:

```{r}
ggseasonplot(qcement, year.labels = FALSE, continuous = T)
```

This is the same `qcement` data shown above, but now the data from each season are overlapped. A seasonal plot allows the underlying seasonal pattern to be seen more clearly, and can be useful in identifying years in which the pattern changes. Here, we see that cement production has consistently increased over the years as the lower (darker) lines represent earlier years and the higher (lighter) lines represent recent years. Also, we see that cement production tends to be the lowest in Q1 and typically peaks in Q3 before leveling off or decreasing slightly in Q4.

A particular useful variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal. Here, we plot the `a10` data with the conventional seasonal plot versus a polar coordinate option to illustrate this variant. Both plots illustrate a sharp decrease in values in Feb and then a slow increase from Apr-Jan.

```{r}
# left
p1 <- ggseasonplot(a10, year.labels=FALSE, continuous=TRUE)

#right
p2 <- ggseasonplot(a10, year.labels=FALSE, continuous=TRUE, polar = TRUE)

gridExtra::grid.arrange(p1,p2, ncol=2)
```

An alternative plot that emphasizes the seasonal patterns is where the data for each season (quarter in our example) are collected together in separate mini time plots. A subseries plot produced by `ggsubseriesplot()` creates mini time plots for each season. Here, the mean for each season is shown as a blue horizontal line.

```{r}
ggsubseriesplot(qcement)
```

This form of plot enables the underlying seasonal pattern to be seen clearly, and also shows the changes in seasonality over time. It is especially useful in identifying changes within particular seasons. In this example, the plot is not particularly revealing; but in some cases, this is the most useful way of viewing seasonal changes over time.

### Autocorrelation of Time Series {#RS_ACF}

Another way to look at time series data is to plot each observation against another observation that occured some time previously. For example, you could plot $y_t$ against $y_{t-1}$. This is called a lag plot because you are plotting the time series against lags of itself. The `gglagplot()` function produces various types of lag plots.


The correlations associated with the lag plots form what is called the gautocorrelation functionh. Autocorrelation is nearly the same as correlation, which you can learn about in the Assessing Correlations tutorial. However, autocorrelation is the correlation of a time series with a delayed copy of itself. Autocorrelation between $y_t$ and $y_{t|k}$ for different values of k can be written as:
 
$$
r_k = \frac{\Sigma_{t=k+1}^T(y_t-\bar{y})(y_{t-k}-\bar{y})}
{\Sigma_{t=1}^T(y_t-\bar{y})^2}
$$

where T is the length of the time series. And similar to correlation, autocorrelation will always between +1 and -1.

When these autocorrelations are plotted, we get an ACF plot. The `ggAcf()` function produces ACF plots. Here we look at the total quarterly beer production in Australia (in megalitres) from 1956:Q1 to 2010:Q2. The data are available in the `fpp2::ausbeer` time series data.

```{r}
# left: autoplot of the beer data
p1 <- autoplot(ausbeer)

# middle: lag plot of the beer data
p2 <- gglagplot(ausbeer)

# right: ACF plot of the beer data
p3 <- ggAcf(ausbeer)

gridExtra::grid.arrange(p1,p2,p3, ncol=3)
```

The middle plot provides the bivariate scatter plot for each level of lag (1-9 lags). The right plot provides a condensed plot of the autocorrelation values for the first 23 lags. The right plot shows that the greatest autocorrelation values occur at lags 4, 8, 12, 16, and 20. We can adjust the `gglagplot` to help illustrate this relationship. Here, we create a scatter plot for the first 16 lags. If you look at the right-most column (lags 4, 8, 12, 16) you can see that the relationship appears strongest for these lags, thus supporting our far right plot above.


```{r}
p2
```

We can also access these autocorrelation values with `Acf`. Here, we can see that the autocorrelation for the two strongest lags (4 and 8) is 0.94 and 0.887.

```{r}
acf <- acf(ausbeer, plot = F)

acf$acf %>% as.tibble() %>% 
  rename(ACF =V1) %>% 
  mutate(Time = 1:24) %>% 
  select(Time, ACF) %>% 
  ggplot(aes(Time, ACF))+
  geom_point()+
  geom_line()
```

When data are either seasonal or cyclical, the ACF will peak around the seasonal lags or at the average cycle length. Thus, we see that the maximal autocorrelation for the `ausbeer` data occurs at a lag of 4 (right plot above). This makes sense since this is quarterly production data so the highest correlated value for a particular quarter will be the same quarter in the previous year.

A simplified approach to thinking about time series features and autocorrelation is as follows:

1. __Trends__ induce passive correlations in the early lags. Strong trend will result in the more recent observations being of closer value to one another.

```{r}
# left plot
p1 <- autoplot(AirPassengers)

# right plot
p2 <- ggAcf(AirPassengers)

gridExtra::grid.arrange(p1, p2, ncol=2)
```

2. __Seasonality__ will induce peaks at the seasonal lags. Think about the holidays, each holiday will have certain products that peak at the time each year and so the strongest correlation will be the values at thatsame time each year.

```{r}
# left plot
p1 <- autoplot(USAccDeaths)

# right plot
p2 <- ggAcf(USAccDeaths)

gridExtra::grid.arrange(p1, p2, ncol=2)
```

3. __Cyclicity__ induces peaks at the average cycle length. Here we see that there tends to be cyclic impact to the mink population every 10 years. We also see this cause a peak in the ACF plot.


### White Noise {#TS_WN}

Time series that show no autocorrelation are called "white noise". For example, the following plots 36 random numbers and illustrates a white noise series. 

This data is considered independent and identically distributed (__iid__) because there is no trend, no seasonality, no autocorrelation ... just randomness.

```{r}
set.seed(3)
wn <- ts(rnorm(36))
autoplot(wn)

ggAcf(wn)
```

For white noise series, we expect each autocorrelation to be close to zero. Of course, they are not exactly  equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within $+-2/\sqrt{T}$ where T is the length of the time series.

It is common to plot these bounds on a graph of the ACF. If there are one or more large spikes outside these bounds, or if more than 5% of spikes are outside these bounds, then the series is probably not white noise.

When using `ggAcf`, the dotted blue lines represent the 95% threshold. Here, we see that none of the autocorrelations exceed the blue line $+-2/\sqrt{50} = +-0.28$ os we can be confident that there is no time series component to this data.

```{r}
ggAcf(wn)
```

Assessing autocorrelation can be quite useful for data sets where trends and seasonalities are hard to see. For example, the following displays the monthly number of pigs slaughtered in Victoria, Australia from 1990-1995. There may be a slight trend over time but it is unclear.

```{r}
pigs.ts <- ts(pigs[121:188], start = c(1990, 1), frequency = 12)
autoplot(pigs.ts)
```
However, looking at the ACF plot makes the feature more clear. There is more information in this data then the plain time series plot provided. We see that the first three lags clearly exceed the blue line suggesting there is possible some signal in this time series component that can be used in a forecasting approach.

```{r}
ggAcf(pigs.ts)
```

The ACF plots test if an individual lag autocorrelation is different than zero. An alternative approach is to use the __Ljung-Box test__, which tests whether any of a group of autocorrelations of a time series are different from zero. In essence it tests the goverall randomnessh based on a number of lags. If the result is a small p-value than it indicates the data are probably not white noise.

Here, we perform a Ljung-Box test on the first 24 lag autocorrelations. The resulting p-value is significant at $p<.001$, so this supports our ACF plot consideration above where we stated its likely this is not purely whitenoise and that some time series information exists in this data.

```{r}
Box.test(pigs, lag = 24, fitdf = 0, type = "Lj")
## 
## 	Box-Ljung test
## 
## data:  pigs
## X-squared = 634.15, df = 24, p-value < 2.2e-16
```

## Benchmark Methods and Forecast Accuracy

In this tutorial, you will learn general tools that are useful for many different forecasting situations. t will describe some methods for benchmark forecasting, methods for checking whether a forecasting model has adequately utilized the available information, and methods for measuring forecast accuracy. Each of the tools discussed in this tutorial will be used repeatedly in subsequent tutorials as you develop and explore a range of forecasting methods.

### tl;dr

1. [Replication requirements](#TS_BM_RR): What youfll need to reproduce the analysis.
2. [Naive Forecasting Methods](#TS_BM_Naive): A simple but useful benchmark approach.
3. [Fitted Values and Residuals](#TS_BM_Pred): Always check the residuals.
4. [Training and Test Sets](#TS_BM_Split): How to partition time series data.
5. [Evaluating Forecast Accuracy](#TS_BM_Eval): How to evaluate accuracy of non-seasonal and non-season forecast methods.
6. [Time Series Cross-validation](#TS_BM_CV): A more sophisticated approach for evaluating predictive accuracy.
7. Exercises: Practice what youfve learned.

### Replication Requirements {#TS_BM_RR}
This tutorial leverages a variety of data sets to illustrate unique time series features. The data sets are all provided by the forecast and fpp2 packages. Furthermore, these packages provide various functions for computing and visualizing basic time series components.

```{r}
library(forecast)
library(fpp2)
```

### Naive Forecasting Methods {#TS_BM_Naive}

Although it is tempting to apply "sophisticated" forecasting methods, one must remeber to consider _naive forecasting_. A naive forecast is simply the most recently observed value. In other words, at the time _t_, the _k_-step-ahead naive forecast ($F_{t+k}$) equals the observed value at time t($Y_t$). 

$$
F_{t+k} = y_t
$$

Sometimes, this is the best that can be done for many time series including most stock price data (for reasons illustrated in the [previous tutorialfs exercise](http://uc-r.github.io/ts_exploration#exercises)). Even if it is not a good forecasting method, it provides a useful benchmark for other forecasting methods. We can perform a naive forecast with the `naive` function.

Here, we use `naive` to forecast the next 10 values. the resulting output is an object of class `forecast`. This is the core class of objects in the `forecast` package, and there are many functions for dealing with them. We can print off the model summary with `summary()`, which provides us with the residual standard deviation, some error measures and actual forecasted values.

```{r}
fc_goog <- naive(goog, 10)
summary(fc_goog)
autoplot(fc_goog)
```











