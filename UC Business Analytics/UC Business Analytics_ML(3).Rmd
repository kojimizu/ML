---
title: "UC business Analytics R programming Guide 3"
author: "Koji Mizumura"
date: "`r Sys.Date()`"
output:
  word_document:
    toc: yes
    toc_depth: '4'
  html_document:
    number_sections: yes
    section_divs: yes
    theme: readable
    toc: yes
    toc_depth: 4
    toc_float: yes
always_allow_html: yes
---

```{r setup4, include=FALSE}
# Set global knitr chunk options
knitr::opts_chunk$set(
  fig.align = "center",
  fig.height = 4.5,
  error = FALSE,
  message = FALSE, 
  warning = FALSE, 
  collapse = TRUE,
  cache = TRUE
)
```


# Time series analysis
## Exploring & visualizing time series

Time series forecasting is performed in nearly every organization that works with quantifiable data. Retail stores forecast sales. Energy companies forecast reserves, production, demand, and prices. Educational institutions forecast enrollment. Goverments forecast tax receipts and spending. International financial organizations forecast inflation and economic activity. The list is long but the point is short - forecasting is a fundamental analytic process in every organization. The purpose of this tutorial is to get you started doing some fundamental time series exploration and visualization.

### tl;dr

This tutotorial serves as an introduction to exploring and visualizing time series data and covers:

1. [Replication requirements](#TS_RR): What you will need to reproduce the analysis
2. [Creating time series objects](#TS_TS): Convert your data to a `ts` object for time series analysis.
3. [Time series plots](#TS_Plot): Basic visualization of `ts` objects and differentiating trends, seasonality, and cycle variation.
4. [Seasonal plots](#TS_Seasonal): Plotting seasonality trends in time series data.
5. [Autocorrelation of time series](#RS_ACF): Computing and visualizing autocorrelation.
6. [White noise](#TS_WN): Differentiating signal from the noise.
7. [Exercises](#TS_Exercise): Practice what youÅfve learned.

### Replication Requirements{#TS_RR}

This tutorial leverages a variety of data sets to illustrate unique time series features. The data sets are all provided by the `forecast` and `fpp2` packages. Furthermore, these packages provide various functions for computing and visualizing basic time series components.

```{r}
library(tidyverse)
library(tidymodels)
library(magrittr)
library(forecast)
library(fpp2)
```

### Creating time series objects {#TS_TS}

A time series can be thought of as a vector or matrix numbers, along with some information about what times those numbers were recorded. This information is stored in a `ts` object in R. 

In most examples and exercies throughout the forecasting tutorial ou will use data that are already in the time series format. However, if you want to work with your own data, you need to know how to create a `ts` object in R.

Here, I illustrate how to convert a data frame to a `ts` object. First, letÅfs assume we have a data frame named `pass.df` that looks like the following where we have the total number of airline passengers for each month for the years 1949-1960.

```{r}
airpass %>% head()
```

We can convert this data frame to a time series object by us the ts() function. Here, theÅc

- __first argument__ supplies it the `pass.df` data frame and we index for just the columns with the data (we store the date-time data separately).
- __second argument__ supplies the start date for the first observation (first period in 1949).
- __third argument__ identifies the frequency, which in this case is monthly (hence 12 months in a year).

```{r}
airpass %>% as.tibble() %>% 
  ts(., start=c(1949,1), frequency=12)

# pass.ts <- ts(pass.df["AirPassengers"], start = c(1949, 1), frequency = 12)
```

We now have convered our data fraime into a time series object

```{r}
str(airpass)
airpass
```

Go ahead and compute this `pass.ts` time series to the built-in `AirPassengers` data set.

### Time series plot {#TS_Plot}

The first step in any data analysis is to plot the data. Graphs enable you to visualize many features of the data, including patterns, unusual observations, changes over time, and relationships between variables. Just as the type of data determines which forecasting method to use, it also determines which graphs are appropriate.

Here, we use the `autoplot()` function to produce time plots of ts data. In time series plots, we should always look for outliers, seasonal patterns, overall trends, and other interesting features. This plot starts to illustrate the obvious trends that emerge over time.

```{r}
autoplot(AirPassengers)
```

Often, we will have have  time series data that has multiple variables. For example, the `fpp2::arrivals` data set has time series data for "quarterly international arrivals (in thousands) Australia from Japan, New Zealand, UK and the US 1919Q1 - 2012Q3".

So this time series data has two variables (over and above the time stamp data) - (1) arrivals in thousands and (2) country.
 
```{r}
head(arrivals)
```
 
We can compare the trends across the different variables (countries) either in one plot or use the facetting option to separate the plots:

```{r}
autoplot(arrivals)
autoplot(arrivals, facets = T)
```

You may have noticed that `autoplot()` looks a lot like `ggplot2` outputs. ThatÅfs because many of the visualizations in the `forecast` package are built on top of `ggplot2`. This allows us to easily add on to these plots with ggplot2 syntax. For example, we can add a smooth trend line and adjust titles:

```{r}
autoplot(arrivals, facets = TRUE) +
  geom_smooth() +
  labs(title   = "International arrivals to Australia",
       y       = "Arrivals (in thousands)",
       x       = NULL,
       subtitle =  "Each country has different trends.")
```

These initial visualizations may spor additional questions such as what was the min, max, or average arrival amount for Japan. We can index and use many normail functions to assess these types of questions.

```{r}
# index for Japan
japan <- arrivals[, "Japan"]

# Identify max arrival amount
summary(japan)
##    Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
##   9.321  74.135 135.461 122.080 176.752 227.641
```

You can also use the `frequency()` function to get the number of observations per unit time. This example returns 4 which means the data are recorded on a quarterly interval.

```{r}
frequency(japan)
```

In viewing time series plots we can describe different components. We can describe the common components using this quarterly cement production data.

```{r}
autoplot(fpp2::qcement)
```

- the __trend__ is the long-term increase or decrease in the data. There is an increasing trend in the cement data.
- the __seasonal__ pattern occurs when a time series is affected by seasonal factors such as the time of the year or the day of the week. The quarterly cement data above shows seasonality likely induced by the change in weather and its impact on being able to pour cement.
- the __cycle__ occurs when the data exhibit rises and falls that are not of a fixed period. These fluctuations are usually due to economic conditions and are often related to the Ågbusiness cycleÅh. We can see a few cycles in our cement data in the early Åf80s, Åf90s, Åf00s, and around 2008 - all these date ranges are around economic depressions that occurred.

Later tutorials will illustrate how to decompose each of these components; however, next we will look at how to do some initial investigating regarding seasonal patterns.

### Seasonal plots {#TS_Seasonal}

There are a few useful ways of plotting data to emphasize seasonal patterns and show changes in these patterns over time. First, a seasonal plot is similar to a time plot except that the data are plotted against the individual ÅgseasonsÅh in which the data were observed. We can produce a seasonal plot with `ggseasonplot()`:

```{r}
ggseasonplot(qcement, year.labels = FALSE, continuous = T)
```

This is the same `qcement` data shown above, but now the data from each season are overlapped. A seasonal plot allows the underlying seasonal pattern to be seen more clearly, and can be useful in identifying years in which the pattern changes. Here, we see that cement production has consistently increased over the years as the lower (darker) lines represent earlier years and the higher (lighter) lines represent recent years. Also, we see that cement production tends to be the lowest in Q1 and typically peaks in Q3 before leveling off or decreasing slightly in Q4.

A particular useful variant of a season plot uses polar coordinates, where the time axis is circular rather than horizontal. Here, we plot the `a10` data with the conventional seasonal plot versus a polar coordinate option to illustrate this variant. Both plots illustrate a sharp decrease in values in Feb and then a slow increase from Apr-Jan.

```{r}
# left
p1 <- ggseasonplot(a10, year.labels=FALSE, continuous=TRUE)

#right
p2 <- ggseasonplot(a10, year.labels=FALSE, continuous=TRUE, polar = TRUE)

gridExtra::grid.arrange(p1,p2, ncol=2)
```

An alternative plot that emphasizes the seasonal patterns is where the data for each season (quarter in our example) are collected together in separate mini time plots. A subseries plot produced by `ggsubseriesplot()` creates mini time plots for each season. Here, the mean for each season is shown as a blue horizontal line.

```{r}
ggsubseriesplot(qcement)
```

This form of plot enables the underlying seasonal pattern to be seen clearly, and also shows the changes in seasonality over time. It is especially useful in identifying changes within particular seasons. In this example, the plot is not particularly revealing; but in some cases, this is the most useful way of viewing seasonal changes over time.

### Autocorrelation of Time Series {#RS_ACF}

Another way to look at time series data is to plot each observation against another observation that occured some time previously. For example, you could plot $y_t$ against $y_{t-1}$. This is called a lag plot because you are plotting the time series against lags of itself. The `gglagplot()` function produces various types of lag plots.


The correlations associated with the lag plots form what is called the Ågautocorrelation functionÅh. Autocorrelation is nearly the same as correlation, which you can learn about in the Assessing Correlations tutorial. However, autocorrelation is the correlation of a time series with a delayed copy of itself. Autocorrelation between $y_t$ and $y_{tÅ|k}$ for different values of k can be written as:
 
$$
r_k = \frac{\Sigma_{t=k+1}^T(y_t-\bar{y})(y_{t-k}-\bar{y})}
{\Sigma_{t=1}^T(y_t-\bar{y})^2}
$$

where T is the length of the time series. And similar to correlation, autocorrelation will always between +1 and -1.

When these autocorrelations are plotted, we get an ACF plot. The `ggAcf()` function produces ACF plots. Here we look at the total quarterly beer production in Australia (in megalitres) from 1956:Q1 to 2010:Q2. The data are available in the `fpp2::ausbeer` time series data.

```{r}
# left: autoplot of the beer data
p1 <- autoplot(ausbeer)

# middle: lag plot of the beer data
p2 <- gglagplot(ausbeer)

# right: ACF plot of the beer data
p3 <- ggAcf(ausbeer)

gridExtra::grid.arrange(p1,p2,p3, ncol=3)
```

The middle plot provides the bivariate scatter plot for each level of lag (1-9 lags). The right plot provides a condensed plot of the autocorrelation values for the first 23 lags. The right plot shows that the greatest autocorrelation values occur at lags 4, 8, 12, 16, and 20. We can adjust the `gglagplot` to help illustrate this relationship. Here, we create a scatter plot for the first 16 lags. If you look at the right-most column (lags 4, 8, 12, 16) you can see that the relationship appears strongest for these lags, thus supporting our far right plot above.


```{r}
p2
```

We can also access these autocorrelation values with `Acf`. Here, we can see that the autocorrelation for the two strongest lags (4 and 8) is 0.94 and 0.887.

```{r}
acf <- acf(ausbeer, plot = F)

acf$acf %>% as.tibble() %>% 
  rename(ACF =V1) %>% 
  mutate(Time = 1:24) %>% 
  select(Time, ACF) %>% 
  ggplot(aes(Time, ACF))+
  geom_point()+
  geom_line()
```

When data are either seasonal or cyclical, the ACF will peak around the seasonal lags or at the average cycle length. Thus, we see that the maximal autocorrelation for the `ausbeer` data occurs at a lag of 4 (right plot above). This makes sense since this is quarterly production data so the highest correlated value for a particular quarter will be the same quarter in the previous year.

A simplified approach to thinking about time series features and autocorrelation is as follows:

1. __Trends__ induce passive correlations in the early lags. Strong trend will result in the more recent observations being of closer value to one another.

```{r}
# left plot
p1 <- autoplot(AirPassengers)

# right plot
p2 <- ggAcf(AirPassengers)

gridExtra::grid.arrange(p1, p2, ncol=2)
```

2. __Seasonality__ will induce peaks at the seasonal lags. Think about the holidays, each holiday will have certain products that peak at the time each year and so the strongest correlation will be the values at thatsame time each year.

```{r}
# left plot
p1 <- autoplot(USAccDeaths)

# right plot
p2 <- ggAcf(USAccDeaths)

gridExtra::grid.arrange(p1, p2, ncol=2)
```

3. __Cyclicity__ induces peaks at the average cycle length. Here we see that there tends to be cyclic impact to the mink population every 10 years. We also see this cause a peak in the ACF plot.


### White Noise {#TS_WN}

Time series that show no autocorrelation are called "white noise". For example, the following plots 36 random numbers and illustrates a white noise series. 

This data is considered independent and identically distributed (__iid__) because there is no trend, no seasonality, no autocorrelation ... just randomness.

```{r}
set.seed(3)
wn <- ts(rnorm(36))
autoplot(wn)

ggAcf(wn)
```

For white noise series, we expect each autocorrelation to be close to zero. Of course, they are not exactly  equal to zero as there is some random variation. For a white noise series, we expect 95% of the spikes in the ACF to lie within $+-2/\sqrt{T}$ where T is the length of the time series.

It is common to plot these bounds on a graph of the ACF. If there are one or more large spikes outside these bounds, or if more than 5% of spikes are outside these bounds, then the series is probably not white noise.

When using `ggAcf`, the dotted blue lines represent the 95% threshold. Here, we see that none of the autocorrelations exceed the blue line $+-2/\sqrt{50} = +-0.28$ os we can be confident that there is no time series component to this data.

```{r}
ggAcf(wn)
```

Assessing autocorrelation can be quite useful for data sets where trends and seasonalities are hard to see. For example, the following displays the monthly number of pigs slaughtered in Victoria, Australia from 1990-1995. There may be a slight trend over time but it is unclear.

```{r}
pigs.ts <- ts(pigs[121:188], start = c(1990, 1), frequency = 12)
autoplot(pigs.ts)
```
However, looking at the ACF plot makes the feature more clear. There is more information in this data then the plain time series plot provided. We see that the first three lags clearly exceed the blue line suggesting there is possible some signal in this time series component that can be used in a forecasting approach.

```{r}
ggAcf(pigs.ts)
```

The ACF plots test if an individual lag autocorrelation is different than zero. An alternative approach is to use the __Ljung-Box test__, which tests whether any of a group of autocorrelations of a time series are different from zero. In essence it tests the Ågoverall randomnessÅh based on a number of lags. If the result is a small p-value than it indicates the data are probably not white noise.

Here, we perform a Ljung-Box test on the first 24 lag autocorrelations. The resulting p-value is significant at $p<.001$, so this supports our ACF plot consideration above where we stated its likely this is not purely whitenoise and that some time series information exists in this data.

```{r}
Box.test(pigs, lag = 24, fitdf = 0, type = "Lj")
## 
## 	Box-Ljung test
## 
## data:  pigs
## X-squared = 634.15, df = 24, p-value < 2.2e-16
```

## Benchmark Methods and Forecast Accuracy

In this tutorial, you will learn general tools that are useful for many different forecasting situations. t will describe some methods for benchmark forecasting, methods for checking whether a forecasting model has adequately utilized the available information, and methods for measuring forecast accuracy. Each of the tools discussed in this tutorial will be used repeatedly in subsequent tutorials as you develop and explore a range of forecasting methods.

### tl;dr

1. [Replication requirements](#TS_BM_RR): What youÅfll need to reproduce the analysis.
2. [Naive Forecasting Methods](#TS_BM_Naive): A simple but useful benchmark approach.
3. [Fitted Values and Residuals](#TS_BM_Pred): Always check the residuals.
4. [Training and Test Sets](#TS_BM_Split): How to partition time series data.
5. [Evaluating Forecast Accuracy](#TS_BM_Eval): How to evaluate accuracy of non-seasonal and non-season forecast methods.
6. [Time Series Cross-validation](#TS_BM_CV): A more sophisticated approach for evaluating predictive accuracy.
7. Exercises: Practice what youÅfve learned.

### Replication Requirements {#TS_BM_RR}
This tutorial leverages a variety of data sets to illustrate unique time series features. The data sets are all provided by the forecast and fpp2 packages. Furthermore, these packages provide various functions for computing and visualizing basic time series components.

```{r}
library(forecast)
library(fpp2)
```

### Naive Forecasting Methods {#TS_BM_Naive}

Although it is tempting to apply "sophisticated" forecasting methods, one must remeber to consider _naive forecasting_. A naive forecast is simply the most recently observed value. In other words, at the time _t_, the _k_-step-ahead naive forecast ($F_{t+k}$) equals the observed value at time t($Y_t$). 

$$
F_{t+k} = y_t
$$

Sometimes, this is the best that can be done for many time series including most stock price data (for reasons illustrated in the [previous tutorialÅfs exercise](http://uc-r.github.io/ts_exploration#exercises)). Even if it is not a good forecasting method, it provides a useful benchmark for other forecasting methods. We can perform a naive forecast with the `naive` function.

Here, we use `naive` to forecast the next 10 values. the resulting output is an object of class `forecast`. This is the core class of objects in the `forecast` package, and there are many functions for dealing with them. We can print off the model summary with `summary()`, which provides us with the residual standard deviation, some error measures and actual forecasted values.

```{r}
fc_goog <- naive(goog, 10)
summary(fc_goog)
autoplot(fc_goog)
```

You will notice the forecast output provides a point forecast(the last observed value in the `goog` data set) and prediction confidence levels at the 80% and 95% level. A prediction interval gives an inbterval within which we expect $y_i$ to lie with a specified probability. For example, assuming the forecast errors are uncorrelated and normally distributed, then a simple 95% prediction interval for the next observation in a time series is 

$$
\hat{y_t} +- \ 1.96 \hat{\sigma}
$$

where $\hat{\sigma}$ an estimate of the standard deviation of the forecast distribution. In forecasting, it is common to calculate 80% intervals and 95% intervals, although any percentage may be used.

When forecasting one-step ahead, the standard deviation of the forecast distribution is almost the same as the standard deviation of the residuals. (In fact, the two standard deviations are identical if there are no parameters to be estimated such as with the naive method. For forecasting methods involving parameters to be estimated, the standard deviation of the forecast distribution is slightly larger than the residual standard deviation, although this difference is often ignored.)

For example, consider our naive forecast for the `goog` data. The last value of the observed series is 838.96, so the forecast of the next value is 838.96 and the standard deviation of the residuals from the naive method is 8.91. Hence, a 95% prediction interval for the next value of goog is

$$
936.96 +- 1.96(8.91) = [821, 856]
$$

Similarl, an 80% prediction interval is given by

$$
838.96 +- 1.28(8.91) = [828, 850]
$$

The vsalue of prediction intervals is that the express the uncertainty in the forecasts. If we only produce point forecasts, there is no way of telling how accurate the forecats are. But if we also produce prediction intervals, then it is clear how much uncertainty is associated with each forecast. Thus, with the naive forecast on the next `goog` value, we can be 80% confident that the next value will be in the range of 828-850 and 95% confident that the the value will be between 821-856.

We can illustrate this prediction interval by plotting the naive model (fc_goog). Here, we see the black point estimate line flat-line (equal to the last observed value) and the colored bands represent our 80% and 95% prediction confidence interval. A common feature of prediction intervals is that they increase in length as the forecast horizon increases. The further ahead we forecast, the more uncertainty is associated with the forecast, and so the prediction intervals grow wider.

```{r}
# forecast next 25 values
fc_goog <- naive(goog, 25)
autoplot(fc_goog)
```

For seasonal data, a related idea is to use the corresponding season from the last year of data. For example, if you want to forecast the sales volume for next March, you would use the sales volume from the previous March. For a series with M seasons, we can write this as

$$
F_{t+k} = y_{t-M+k}
$$

This is implemented in the `snaive` function meaning, _seasonal naive_. Here, I use `snaive` to forecast the next 16 values for the `ausbeer` series. Here, we see that the 4th quarter for each future year is 488 which is the last observed 4th quarter value in 2009.

```{r}
fc_beer <- snaive(ausbeer, 16)
summary(fc_beer)
```

Similar to naive, we can plot the `snaive` model with `autoplot`.

```{r}
autoplot(fc_beer)
```

### Fitted Values and Residuals {#TS_BM_Pred}

When applying a forecasting method, it is important to always check that the residuals are well-behaved (i.e., no outliers or patterns) and resemble white noise. Essential assumptions for an appropriate forecasting model include residuals being:

- uncorrelated
- centered at mean zero

Furthermore, the prediction intervals are computed assuming that the residuals:

- have constant variance
- are normally distributed

A convenient function to use to check these assumptions is the `checkresiduals` function. This function produces a time plot, ACF plot, histogram, and a __Ljung-Box test__ on the residuals. Here, I use `checkresiduals` for the fc_goog naive model. We see that the top plot shows residuals that appear to be white noise (no discernable pattern), the bottom left plot shows only a couple lags that exceed the 95% confidence interval, bottom right plot shows the residuals to be approximately normally distributed, and the Ljung-Box test results give a p-value of 0.22 suggesting the residuals are white noise. This is a good thing as it suggests that are model captures all (or most) of the available signal in the data.

```{r}
checkresiduals(fc_goog)
```

If we compare that to the `fc_beer` seasonal naive model we see that there is an apparent pattern in the residual time series plot, the ACF plot shows several lags exceeding the 95% confidence interval, and the Ljung-Box test has a statistically significant p-value suggesting the residuals are not purely white noise. This suggests that there may be another model or additional variables that will better capture the remaining signal in the data.

```{r}
checkresiduals(fc_beer)
```

### Training & Test sets {#TS_BM_Split}

A training set is a data set that is used to discover possible relationships. A test set is a data set that is used to verify the strength of these potential relationships. When you separate a data set into these parts, you generally allocate more of the data for training, and less for testing. There is one important difference between data partitioning in cross-sectional and time series data. In cross-sectional data the partitioning is usually done randomly, with a random set of observations designated as training data and the remainder as test data. However, in time series, a random partition creates two problems:

1. It does not mimic the temporal uncertainty where we use the past and present to forecast the future.

2. It creates two time series with "holes", whereas many standard forecasting methods cannnot handle time series with missing value.

Therefore, time series partitioning into training and test sets is done by taking a training partition from earlier observations and then using a later partition for the test set.

```{r}
knitr::include_graphics("image/partitioning-1.png")
```

One function that can be used to create training and test sets is `subset.ts()`, which returns a subset of a time series where the start and end of the subset are specified using _index_ values. The `gold` time series comprises daily gold prices over 1108 traiding days. Let's use the first 1000 days as a training set. We can also create a test data set of the remaining data.

```{r eval=FALSE}
train <- subset.ts(gold, end = 1000)
test <-  subset.ts(gold, start = 1001, end = length(gold))
```

A similar approach can be used for data where we want to maintain season features. For example, if we use `subset.ts` to take the first 1000 observations this may include half of a year in the training data and the other half in the test data.

Rather, for forecasting methods that include seasonal features (i.e. `snaive`), we prefer to not split a full cycle of seasons (i.e. year, month, week). We can use the `window` function to do this. For example, the `ausbeer` data set has quarterly data from 1956-2010. Here, we take data from 1956 through the 4th quarter of 1995 to be a `training` data set.

```{r}
train2 <- window(ausbeer, end = c(1995, 4))
autoplot(train2)
```

The `window` function is also particularly useful when we want to take data from a defined period. For example, if we decide that data from 1956-1969 is not appropriate due to regulatory changes then we can create a training data set from the 1st qtr of 1970 through the 4th qtr of 1995.

```{r}
train3 <-  window(ausbeer, start = c(1970, 1), end = c(1995, 4))
train3 %>% autoplot()
```

### Evaluating forecast accuracy {#TS_BM_Eval}

For evaluating predictive performance, several measures are commonly used to assess the predictive accuracy of a forecasting method. In all cases, the _measures are based on the test data set_, which services as a more objective basis than the training period to assess predictive accuracy. Given a forecast and its given errors ($e_t$), the commonly used accuracy measures are listed below.


Definition 

- Observation: $y_t$
- Forecast: $\hat{y_t}$
- Forecast error: $e_t = y_t - \hat{y_t}$

Accuracy Measure:
- Mean absolute error: MAE = $average(|e_t|)$ 
- Mean squared error: MSE = $average(e_t^2)$
- Mean absolute percentage error: $MAPE = 100*average(\frac{e_t}{y_t})$
- Mean absolute scaled error: $MASE = MAE/Q$

Note that each measure has its strengths and weaknesses. For example, if you want to compare forecast accuracy between two series on very different scales you canÅft compare the MAE or MSE for these forecast as these measures depend on the scale of the time series data. MAPE is often better for comparisons but only if our data are all positive and have no zeros or small values. It also assumes a natural zero so it canÅft be used for temperature forecasts as these are based on arbitrary zero scales. MASE is similar to MAE but is scaled so that it can be compared across different data series. You can read more about each of these measures [here](https://www.otexts.org/fpp/2/5); however, for now just keep in mind that for all these measures a smaller value signifies a better forecast.

#### Evaluating forecast accuracy of no-seasonal methods

We can compute all of these measures by using the `accuracy` function. The accuracy measures provided also include root mean squared error (RMSE) which is the square root of the mean squared error (MSE). Minimizing RMSE, which corresponds with increasing accuracy, is the same as minimizing MSE. In addition, other accuracy measures not illustrated above are also provided (i.e. ACF1 - autocorrelation at lag 1; TheilÅfs U). The output of `accuracy` allows us to compare these accuracy measures for the residuals of the training data set against the forecast errors of the test data. However, our main concern is how well different forecasting methods improve the predictive accuracy on the test data.

Using the training data we created from the `gold` data set (`train`), letÅfs create two different forecasts:

1. A `naive` forecast and
2. a forecast equal to the mean of all observations

Here, I use `h = 108` to predict for the next 108 days (note that the `gold` data set has 1108 observations, the `train`ing data set has 1000, so we want to predict the next 108 observations and compare that to the test data)

```{r eval=FALSE}
# create training data
train <- subset.ts(gold, end = 1000)

# compute naive forecasts and save to naive_fc
naive_fc <- naive(train, h = 108)

# compute mean forecasts and save to mean_fc
accuracy(naive_fc, gold)
##                       ME     RMSE       MAE         MPE     MAPE MASE
## Training set  0.09161392  6.33977  3.158386  0.01662141 0.794523    1
## Test set     -6.53834951 15.84236 13.638350 -1.74622688 3.428789   NA
##                    ACF1 Theil's U
## Training set -0.3098928        NA
## Test set      0.9793153  5.335899
accuracy(mean_fc, gold)
##                         ME     RMSE      MAE       MPE      MAPE     MASE
## Training set -4.239671e-15 59.17809 53.63397 -2.390227 14.230224 16.98145
## Test set      1.319363e+01 19.55255 15.66875  3.138577  3.783133       NA
##                   ACF1 Theil's U
## Training set 0.9907254        NA
## Test set     0.9793153  6.123788
```

What `accuracy` is doning is taking the model output from the training data (`naive_fc` & `means_fc`) and computing the accuracy measures of the 108 forecasted values to those values in `gold` that are not included in the model (aka test data). This means we do not need to directly feed it a test data set, although we do have the option as illustrated as below: 

```{r eval=FALSE}
train <- subset.ts(gold, end = 1000)
test <- subset.ts(gold, start = 1001, end = length(gold))
naive_fc <- naive(train, h = 108)
accuracy(naive_fc, test)
##                       ME     RMSE       MAE         MPE     MAPE MASE
## Training set  0.09161392  6.33977  3.158386  0.01662141 0.794523    1
## Test set     -6.53834951 15.84236 13.638350 -1.74622688 3.428789   NA
##                    ACF1 Theil's U
## Training set -0.3098928        NA
## Test set      0.9793153  5.335899
```

If we compare the test set accuracy measures for both models we see that the naive approach has lower scores across all measures indicating better forecasting accuracy.

#### Evaluating forecast accuracy of seasonal methods

We can use a similar approach to evaluate the accuracy of seasonal forecasting models. The primary difference is we want to use the `window` function for creating our `train`ing
data so that we appropriately capture seasonal cycles.

Here, I illustrate with the `ausbeer` data set. We see the snaive model produces lower scores across all measures indicating better forecasting accuracy.

```{r}
# create training data
train2 <- window(ausbeer, end = c(1995, 4))

# create specific test data of interest
test <- window(ausbeer, start = c(1996, 1), end = c(2004, 4))

# Compute snaive forecasts and save to snaive_fc
snaive_fc <- snaive(train2, h = length(test))

# Compute mean forecasts and save to mean_fc
mean_fc <- meanf(train2, h = length(test))

# Use accuracy() to compute forecast accuracy
accuracy(snaive_fc, test)
##                     ME     RMSE      MAE       MPE     MAPE      MASE
## Training set  4.730769 20.60589 16.51282  1.284697 3.957622 1.0000000
## Test set     -8.527778 18.06854 13.08333 -2.011727 3.038430 0.7923137
##                     ACF1 Theil's U
## Training set  0.01783674        NA
## Test set     -0.39101873 0.2890478
accuracy(mean_fc, test)
##                         ME     RMSE      MAE       MPE      MAPE     MASE
## Training set -2.279427e-14 96.37835 78.64437 -6.646984 21.996740 4.762625
## Test set      2.393472e+01 49.38745 34.07500  4.640789  7.243654 2.063548
##                     ACF1 Theil's U
## Training set  0.72972747        NA
## Test set     -0.09479634 0.8164006
```

### Time series cross-validation {#TS_BM_CV}

A more sophisticted version of training/test sets is cross validation. You can see how CV works for cross-sectional data [here](http://uc-r.github.io/resampling_methods). For time series data, the procedure is similar but the training set consists only of observations that occurred prior to the observation that forms the test set. So in traditional time series partitioning, we select a certain point in time where everything before that point (blue) is the training data and everything after that point (red) is the test data.

```{r}
knitr::include_graphics("image/ts_validation.png")
```
However, assuming we want to perform 1-step forecast (predicting the next value in the series), time series cross-validation will:

1. Select the observation at, and prior to, time k (blue dots).
2. Select the observation at $k+1$ for the test data (red dots).
3. Discard the observation $k+2,k+3, ..., k+n$ (white dots) 
4. Compute the error on the forecast for time $k+1$.
5. Repeat steps 1-4 above for $i=1,2,...,T-k$ where $T$ is the total number of observations.
6. Compute the forecast accuracy measures based on the errors obtained. 

This procedure is sometimes known as a "rolling forecast origin" because the "origin" $(k+i-1)$ at which the forecast is based rolls foward in times as displayed by each row in the above illustration.

With time series forecasting, one-step forecasts may not be as relevant as multi-step forecasts.  In this case, the cross-validation procedure based on a rolling forecasting origin can be modified to allow multi-step errors to be used. Suppose we are interested in models that produce good h-step-ahead forecasts. Here, we simply adjust the above algorithm so that we select the observation at time $k+h+i-1$ for the test set, use the observations at time $1,2,..., k+i-1$ to estimate the forecasting model, compute the $h-$step error on the forecast for time $k+h+i-1$, rinse & repeat until we can compute the forecasting accuracy for all errors caluclated. For a 2-step-ahead forecast this looks like 

```{r}
knitr::include_graphics("image/two_step_cv.png")
```

This seems tedious, however, there is a simple function that implements this procedure. The `tsCV` function applies a forecasting model to a sequence of training sets from a give time series and provides the errors as the output. However, we need to compute our own accuracy measures with these errors. 

As an example, let's perform a cross-validation approach for a 1-step ahead (`h=1`) `naive` model with the `goog` data. We can then compute the MSS which is 79.59.

```{r}
errors <- tsCV(goog, forecastfunction = naive, h = 1)
mean(errors^2, na.rm = TRUE)
## [1] 79.58582
```

We can compute and compare the MSE for different forecast horizons (1-10) to see if certain forecasting horizons perform better than others. Here, we see that as the forecasting horizon extends the predictive accuracy becomes poorer.

```{r}
# create empty vector to hold MSE values
MSE <- vector("numeric", 10)
for(h in 1:10) {
  errors <- tsCV(goog, forecastfunction = naive, h = h)
  MSE[h] <- mean(errors^2, na.rm = TRUE)
}

MSE
##  [1]  79.58582 167.14427 253.92996 330.20539 398.47090 464.83012 531.09256
##  [8] 596.40191 654.25322 712.70988
```


## Moving Average

Smoothing methods are a family of forecasting methods that average values over multiple periods in order to reduce the noise and uncoiver patterns in the data. 

Moving averages are one such smoothing method. Moving averages is a smoothing approach that averages  values from a window of consecutive time periods, thereby generating a series of averages. The moving average approaches primarily differ based on the number of values averaged, how the average is computed, and how many times averaging is performed. This tutorial will walk you through the basics of performing moving averages.

### tl;dr

1. [Replication Requirements](#TS_MA_RR): What youÅfll need to replicate the analysis in this tutorial
2. [Centered Moving Averages](#TS_CMA): Technique for calculating and displaying a simple moving average
3. [Moving Averages of Moving Averages](#TS_MAMA): Using the concept of simple moving averages to perform multi-step smoothing
4. [Weighted Moving Averages](#TS_WMA): Smoothing by use of weights specifically chosen for their mathematical properties

### Replication requirements {#TS_MA_RR}

There are four packages outside of the base set of functions that will be used in the tutorial:

```{r}
library(tidyverse)      # data manipulation and visualization
library(lubridate)      # easily work with dates and times
library(fpp2)           # working with time series data
library(zoo)            # working with time series data
```

### Centered Moving Averages {#TS_CMA}

The most straightforward method is called a simple moving average. For this method, we choose a number of nearby points and average them to estimate the trend. When calculating a simple moving average, it is beneficial to use an odd number of points so that the calculation is symmetric. For example, to calculate a 5 point moving average, the formula is:

$$
\hat{y_t} = \frac{y_{t-2}+y_{t-1}+y_t+y_{y+1}+y_{t+2}}{5}
$$

where t is the time step that you are smoothing at and 5 is the number of points being used to calculate the average (which moving forward will be denoted as `k`). To compute moving averages on our data we can leverage the `rollmean` function from the `zoo` package. Here, we focus on the personal savings rate (`psavert`) variable in the economics data frame. Using mutate and `rollmean`, I compute the 13, 25, Åc, 121 month moving average values and add this data back to the data frame. Note that we need to explicitly state to fill any years that cannot be computed (due to lack of data) with NA.

```{r}
savings <- economics %>% 
  select(date, srate = psavert) %>% 
  mutate(srate_ma01 = rollmean(srate, k = 13, fill = NA),
         srate_ma02 = rollmean(srate, k = 25, fill = NA),
         srate_ma03 = rollmean(srate, k = 37, fill = NA),
         srate_ma05 = rollmean(srate, k = 61, fill = NA),
         srate_ma10 = rollmean(srate, k = 121, fill = NA))
```

Now we can go ahead and plot these values and compare the actual data to the different moving average smoothers.

```{r}
savings %>% 
  gather(metric, value, srate:srate_ma10) %>% 
  ggplot(aes(date, value, color = metric))+
  geom_line()
```

You may notice that as the number of points used for the average increases, the curve becomes smoother. Choosing a value for $k$ is a balance between eliminating noise while still capturing the data's true structure. For this set, the 10 year moving average ($k=121$) eliminates most of the pattern and is probabily little more than just looking at the data itself. We can see this by zooming into the 2000-2015 time range:

```{r}
savings %>%
  gather(metric, value, srate:srate_ma10) %>%
  ggplot(aes(date, value, color = metric)) +
  geom_line() +
  coord_cartesian(xlim = c(date("2000-01-01"), date("2015-04-01")), ylim = c(0, 11))
```

To understand how thse different moving averages compare we can compute the [MSE and MAPE](http://uc-r.github.io/ts_benchmarking#accuracy). Both of these error rates will increase as you choose a larger $k$ to average over; however, if you or your leadership are indifferent between a 6-9% error rate then you may want to illustrate trends with a 3 year moving average rather than a 1 year moving average. 

```{r}
savings %>% 
  gather(metric, value, srate_ma01:srate_ma10) %>% 
  group_by(metric) %>% 
  summarise(
    MSE = mean((srate - value)^2, na.rm=TRUE),
    MAPE = mean(abs((srate - value)/srate), na.rm=TRUE))
```

#### Using the fpp2 package 

A simple moving average can also be plotted by using `autoplot()` contained in the `fpp2` package. This is helpful if your data is already in time series data object. For example, if our savings rate data were already converted to a time series object as hereÅc

```{r}
savings.ts <- economics %>% 
  select(srate = psavert) %>% 
  ts(start = c(1967, 7), frequency = 12)

head(savings.ts, 30)
```

we can plot this data with `autoplot`. Here, the data is plotted in line 1 of the following code, while the moving average (calculated using the `ma()` function) is plotted in the second layer.

```{r}
autoplot(savings.ts, series = "Data")+
  autolayer(ma(savings.ts, 13), series = "1 year MA")+
  autolayer(ma(savings.ts, 61), series = "5 year MA")+
  autolayer(ma(savings.ts, 121), series = "10 year MA")+
  xlab("Date")+
  ylab("Saving Rate")
```

#### Trailing Moving Averages for Forecasting

Centered moving averages are computed by averaging across data both in the past and future of a given time point. In that sense they cannot be used for forecasting because at the time of forecasting, the future is typically unknown. Hence, for purposes of forecasting, we use trailing moving averages, where the window of k periods is placed over the most recent available k values of the series. For example, if we have data up to time period t, we can predict the value for t+1 by averaging over k periods prior to t+1. If we want to use the 5 most recent time periods to predict for t+1 then our function looks like:

$$
\hat{y_{t+1}} = \frac{y_{t-4}+y_{t-3}+y_{t-2}+y_{t-1}+y_t}{5}
$$

So, if we wanted to predict the next month's savings rate based on the previous year's average, we can use `rollmean` with the `align = "right"` argument to compute the training moving average. We can see that if we wanted to predict what the saving rates would be for 2015-05-01 based on the last 12 months,  our prediction would be 5.06% (the 12-month average for 2015-04-01). This is now similar to using a naive forecast but with an averaged value rather than the last actual value.

```{r}
savings_tma <- economics %>% 
  select(date, srate = psavert) %>% 
  mutate(
    srate_tma = rollmean(srate, k = 12, fill = NA, align = "right")
  )

tail(savings_tma,12)
```

We can visualize how the 12-month trailing moving average predicts future savings rates with the following plot. ItÅfs easy to see that trailing moving averages have a delayed reaction to changes in patterns and trends.

```{r}
savings_tma %>% 
  gather(metric, value, -date) %>% 
  ggplot(aes(date, value, color = metric))+
  geom_line()
```

### Moving averages of moving averages {#TS_MAMA}

The concept of simple moving averages can be extended to taking moving averages of moving averages. This technique is often employed with an even number of data points so that the final product is symmetric around each point. For example, letÅfs look at the built-in data set elecsales provided by the fpp2 package. For our first example we convert to a data frame. This data frame is even numbered with 20 rows.

```{r}
# convert to data frame 
elecsales.df <- data.frame(year = time(elecsales),
                           sales = elecsales)

elecsales.df
```

An even-numbered moving average is unbalanced, and for our purposes, the unbalancing will be in factor of more recent observations. For example, to calcuate a 4-MA, the equation is as follows:

$$
\hat{y_{t+1}} = \frac{y_{t-1}+y_{t}+y_{t+1}+y_{t+2}}{4}
$$

To make the moving averages symmetric (and therefore more accurate), we then take a 2-M of the 4-MA to create __2 x 4-MA__. For the 2-MA step, we average the current and previous moving averages, thus resulting in an overall estimate of:

$$
\hat{y_{t+1}} = \frac{1}{8} y_{t-2}+
\frac{1}{4} y_{t-1}+\frac{1}{4} y_{t}+ 
\frac{1}{4} y_{t+1}+
\frac{1}{8}y_{t+2}
$$
This two-step process can be performed easily with the `ma` function by setting `order = 4` and `center = TRUE`. 

```{r}
elecsales.df %>%
  mutate(ma4 = ma(sales, order = 4, centre = TRUE)) %>%
  head()
##   year   sales      ma4
## 1 1989 2354.34       NA
## 2 1990 2379.71       NA
## 3 1991 2318.52 2384.359
## 4 1992 2468.99 2412.047
## 5 1993 2386.09 2467.918
## 6 1994 2569.47 2536.784
```

To compuare this moving average to a regular moving average we can plot the two outputs:

```{r}
# Compute 2 and 2*4 moving averages

elecsales.df %>% 
  mutate(ma2 = rollmean(sales, k=2, fill = NA),
         ma2x4 = ma(sales, order = 4, centre = TRUE)) %>% 
  gather(ma, value, ma2:ma2x4) %>% 
  ggplot(aes(x = year))+
  geom_point(aes(y = sales))+
  geom_line(aes(y = value, color = ma))
```

This 2 x 4-MA process produces the best fit yet. It massages out some of the noise while maintaining the overall trend of the data. Other combinations of moving averages are possible, such as 3 x 3-MA. To maintain symmetry, if your first moving average is an even number of points, the follow-up MA should also contain an even number. Likewise, if your first MA uses an odd number of points, the follow-up should use an odd number of points. Just keep in mind that moving averages of moving averages will lose information as you do not retain as many data points.

#### Using fpp2 package

If your data is already in time series data object, then you can apply the `ma` function directly to that object with `order = 4` and `centre = TRUE`. For example, the built-in `elecsales` data set is a time series object:

```{r}
class(elecsales)
```

Wecan compute the ma2x4 moving average directly:

```{r}
ma(elecsales, order = 4, centre = TRUE)
```

And we can use `autoplot` to plot the 2x4 moving average against the raw data:

```{r}
autoplot(elecsales, series = "Data")+
  autolayer(ma(elecsales, order = 4, centre = TRUE), series = "2x4-MA")+
  labs(x = "Year", y = "Sales")+
  ggtitle("Annual electricity sales: South Australia")
```

### Weighted moving averages {##TS_WMA}

A moving average of a movingaverage can be thought of as a symmetric MA that has different weights on each nearby observation. For example, the 2x4-MA discussed above is equivalent to a weighted 5-MA with weights given by $[\frac{1}{8}, \frac{1}{4}, \frac{1}{4},\frac{1}{4},\frac{1}{8}]$. In general, a weighted m-MA can be written as:

$$
\hat{T_t} = \Sigma_{j=-k}^k a_jy_{t+j}
$$

where k=(m-1)/2 and the weights are given by $[a_{-k}, ...,a_k]$. It is important that the weights all sum to one and that they are symmetric so that $a_j=a_{-j}$. This simple m-MA is a special case where all the weights are equal to $\frac{1}{m}$. This simple m-MA is a special case where all the weights are equal to $\frac{1}{m}$  A major advantage of weighted moving averages is that they yield a smoother estimate of the trend-cycle. Instead of observations entering and leaving the calculation at full weight, their weights are slowly increased and then slowly decreased resulting in a smoother curve. Some specific sets of weights are widely used such as the following:

For example, the `AirPassengers` data contains an entry for every month in a 12 year span,  so a time period would consist of 12 time units. A 2 x 12-MA set-up is the preferred method for such data. The observation itself, as well as the 5 observations immediately before and after it, receives weight $\frac{1}{12}=0.083$, while the data point for that month last year and that month the following year both receive weight $\frac{1}{24}=0.042$. 

We can produce this weighted moving average using the `ma` function as we did in the last section.

```{r}
ma(AirPassengers, order = 12, centre = TRUE)
```

And to compare this moving average to the actual time series:

```{r}
autoplot(AirPassengers, series = "Data") + 
  autolayer(ma(AirPassengers, order = 12, centre = T), series = "2x12-MA") +
  ggtitle("Monthly Airline Passengers (1949-60)") +
  labs(x = NULL, y = "Passengers")
```

You can see weÅfve smoothed out the seasonality but have captured the overall trend.

Exercises
Using the economics data set provided by the ggplot2 package:

1. Compute and plot the 1, 3, and 5 year moving average for the personal consumption expenditures.
2. Compute the mean square error of these moving averages.
3. Forecast the personal consumption expenditure for 2015-05-01 using a 1, 3, and 5 year trailing moving average.
4. Compute and plot a 2x12 weighted smoothing average.

## Exponential smoothing

Expontential smoothing  another smoothing method and has been around since the 1950s. Where niave forecasting places 100% weight on the most recent observation and moving averages place equal weight on k values, exponential smoothing allows for weighted averages where greater weight can be placed on recent observations and lesser weight on older observations. Exponential smoothing methods are intuitive, computationally efficient, and generally applicable to a wide range of time series. Consequently, exponentially smoothing is a great forecasting tool to have and this tutorial will walk you through the basics.

### tl;dr

1. [Replication requirements](#ES_RR):  What youÅfll need to reproduce the analysis in this tutorial.
2. [Simple Exponential Smoothing](#ES_Simple): Technique for data with no trend or seasonality.
3. [HoltÅfs Method](#ES_Holt): Technique for data with trend but no seasonality.
4. [Holt-Winters Seasonal Method](#ES_Holt-Winters): Technique for data with trend and seasonality.
5. [Damped Trend Methods](#ES_Dampled_Trend): Technique for trends that are believed to become more conservative or Ågflat-lineÅh over time.
6. Exercises: Practice what youÅfve learned 

### Replication requirements {#ES_RR}

This tutorial primarily uses the fpp2 package. fpp2 will automatically load the forecast package (among others), which provides many of the key forecasting functions used throughout.

```{r}
library(tidyverse)
library(fpp2)          
```

Furthermore, weÅfll use a couple data sets to illustrate. The goog and qcement data are provided by the fpp2 package. LetÅfs go ahead and set up training and validation sets:

```{r}
# create training and validation of the Google stock data
goog.train <- window(goog, end = 900)
goog.test <- window(goog, start = 901)

# create training and validation of the AirPassengers data
qcement.train <- window(qcement, end = c(2012, 4))
qcement.test <- window(qcement, start = c(2013, 1))
```

### Simple exponential smoothing {#ES_Simple}

The simplest of the exponentially smoothing methods is called "simple exponential smooothing" (SES). The key point to remmember is that SES is suitable for data with no trend or seasonal pattern. This section will illustrate why.

For exponential smoothing, we weight the recent observation more heavily than older observations. The weight of each observation is determined through the use of _smoothing parameter_, which we will denote $\alpha$. For a data set with $T$ observations, we calculate our predicted value $\hat{y}_{t+1}$, which will be based on $y_1$ through $y_t$ as follows.

$$
\hat{y}_{t+1} = \alpha y_t + \alpha(1-\alpha)y_{t-1}+...
\alpha(1-\alpha)^{t-1} y_1
$$

where $0<\alpha<1$. It is also common to come to use the _component form_ of this model, which uses the following set of equations.

$$
\hat{y}_{t+1} = l_t \\
l_t = \alpha y_t + (1-\alpha)l_{t-1}
$$

In both equations, we can see that the most weights is placed on the most recent observation. In practice, $\alpha$ equals to 0.1-0.2 tends to perform quite well but we will demonstrate shortly how to tune this parameter. When $\alpha$ is close to 1 we consider this fast learning because the algorithm gives more historical data more weight. When $\alpha$ is closer to 1, we consider the fast learning because the algorithms give more weight to the most recent observation; therefore, recent changes in the data will have a bigger impact on forecasted values. The following table illustrates how weighting changes based on the $\alpha$ parameters.

LetÅfs go ahead and apply SES to the Google data using the ses function. We manually set the $\alpha=.2$ for our initial model and forecast foward 100 steps with $h=100$. We see that our forecast proejcts a flatlined estimate into the future, which does not capture the positive trend in the data. This is why SES should not be used on data with a trend or seasonal component.

```{r}
ses.goog <- ses(goog.train, alpha=.2, h=100)
autoplot(ses.goog)
```

One approach to correct for this is to difference our data to remove the trend. Now, `goog.dif` represents the change in stock price from the previous day.

```{r}
goog.dif <- diff(goog.train)
autoplot(goog.dif)
```


Once we have differenced we have effectively removed the trend from our data and can reapply the SES model.

```{r}
ses.goog.dif <- ses(goog.dif, alpha = .2, h = 100)
autoplot(ses.goog.dif)
```

To understand how well the model predicts we can compare our forecasts to our validation data set. But first we need to create a differenced validation set since our training data was built on differenced data. We see that performance measures are smaller on the test set than the training so we are not overfitting our model.

```{r}
goog.dif.test <- diff(goog.test)

# accuracy calculation
forecast::accuracy(ses.goog.dif, goog.dif.test) %>% 
  broom::tidy()
```

In our model, we used the standard $/alpha=.2$; however, we can tune our alpha parameter to identify the value that reduces our forecasting error. Here, we loop through alpha values from 0.01-0.99 and identify the level that minimizes our test RMSE. Turns our that $\alpha=.05$ minimize our prediction error.

```{r}
# identify optimal alpha parameter
alpha <- seq(.01, .99, by=.01)
RMSE <- NA

for (i in seq_along(alpha)){
  fit <- ses(goog.dif, alpha = alpha[i], h=100)
  RMSE[i] <- accuracy(fit, goog.dif.test)[2,2]
}

# convert to a data frame and idenitify min alpha value
alpha.fit <- data_frame(alpha, RMSE)
alpha.min <- filter(alpha.fit, RMSE == min(RMSE))

# plot RMSE vs. alpha
ggplot(alpha.fit, aes(alpha, RMSE)) +
  geom_line() +
  geom_point(data = alpha.min, aes(alpha, RMSE), size = 2, color = "blue")  
```

Now we can re-fit our SES with $\alpha=0.05$. Our performance metrics are not significantly different from our model, where $\alpha=0.20$; however, you will notice that the predicted confidence intervals are narrower (left chart). And when we zoom into the predicted versus actuals (right chart) you see that for most observations, our predicted confidence intervals did well.

```{r}
# refit model with alpha = .05
ses.goog.opt <- ses(goog.dif, 
                    alpha = .05,
                    h = 100)

# performance eval
accuracy(ses.goog.opt,
         goog.dif.test)

# plotting results
p1 <- autoplot(ses.goog.opt)+
  theme(legend.position = "bottom")
p2 <- autoplot(goog.dif.test)+
  autolayer(ses.goog.opt, alpha=.5)+
  ggtitle("Predicted vs actuals for the test data set")

gridExtra::grid.arrange(p1, p2, nrow=1)
```

### Holt's Method {#ES_Holt}

As mentionedd and observed in the previous section, SES does not perform well with data that has a long-term trend. In the last section, we will illustrate how you can remove the rend with differencing and then perform SES. An alternative method to apply exponential smoothing while capturing trend in the data is to use _HoltÅfs Method_.

HoltÅfs Method makes predictions for data with a trend using two smoothing parameters, $\alpha$ and $\beta$, which correspond to the level and trend components, respectively. For HoltÅfs method, the prediction will be a line of some non-zero slope that extends from the time step after the last collected data point onwards.

The methodology for predictions using data with a trend (HoltÅfs Method) uses the following equation with $T$ observations. The k-step-ahead forecast is given by combining the level estimate at time t($L_t$) and the trend estimate (which in this example is assumed additive) at time t ($T_t$). 

$$
\hat{y}_{T+1} = L_t + kT_k
$$

The level ($L_t$) and trend ($T_t$) are updated through a pair of updating equations, which is where you see the presence of the two smoothing parameters:

$$ 
L_t = \alpha y_t + \alpha(1-\alpha)(L_t + T_{t-1}) \\
T_t = \beta(L_t - L_{t-1}) + (1-\beta)T_{t-1}
$$

In these equations, the first means that the level at time t is a weighted average of the actual value at time t and the level in the previous period, adjusted for trend. The second equation means that the trend at time t is a weighted average of the trend in the previous period and the more recent information on the change in the level. Similar to SES, $\alpha$ and $\beta$ are constrained to 0-1 with higher values giving faster learning and lower values providing slower learning. 

To capture a __multiplicative__ (exponential) trend we make a minor adjustment in the above equations.

$$
\hat{y}_{T+1} = L_t * kT_k \\
L_t = \alpha y_t + \alpha(1-\alpha)(L_t * T_{t-1}) \\
T_t = \beta(L_t / L_{t-1}) + (1-\beta)T_{t-1}
$$

HoltÅfs method also has the alternative Component Form operations. In this case these represent the additive trend component form:

$$
\hat{y}_{t+h} = l_t * hb_t \\
l_t = \alpha y_t + \alpha(1-\alpha)(l_t + b_{t-1}) =
\beta (l_t - l_{t-1}) + (1-\beta)b_{t-1}
$$

If we go back to our Google stock data, we can apply Holt's method in the following manner. Her, we will not manually set the $\alpha$ and $\beta$ for our initial model and forecast foward 100 step with h=100. We see that our forecast now does a better job capturing the positive trend in the data.

```{r}
holt.goog <- holt(goog.train, h = 100)
autoplot(holt.goog)
```

Within `holt` you cna manually set the $\alpha$ and $\beta$ parameters; however, if you leave those parameters at NULL, the `holt` function will actually identify the optimal model parameters. It does by ,minimizing AIC and BIC values. We can see the model selected by `holt`. In this case, $\alpha=0.9967$ meaning fast learning in the daty-today movements and $\beta=0.0001$ which means slow learning for the trend.

```{r}
holt.goog$model
```

LetÅfs check the predictive accuracy of our model. According to our MAPE we have about a 2% error rate.

```{r}
accuracy(holt.goog, goog.test)
```

Similr to SES, we can tune the $\beta$ parameter to see if we can improve our predictive accuracy. The `holt` function identified an optimal $\beta=0.0001$; however, this optimal value is based on minimizing errors on the training set, not minimizing predicton errors on the test set. Let's assess a tradespace of $\beta$ values and see if we gain some predictive accuracy. Here, we loop through a series of $/beta$ values starting at 0.0001 all the way up to 0.5. We see that there is a dip in our RMSE at 0.0601.

```{r}
# identify optimal alpha parameter
beta <- seq(.0001, .5, by=0.001)
RMSE <- NA

for (i in seq_along(beta)){
  fit <- holt(goog.train, beta = beta[i], h=100)
  RMSE[i] <- accuracy(fit, goog.test)[2,2]
}

# convert to a data frame and identify min alpha value
beta.fit <- data_frame(beta, RMSE)
beta.min <- filter(beta.fit, RMSE == min(RMSE))

# plot RMSE vs alpha
ggplot(beta.fit, aes(beta, RMSE))+
  geom_line()+
  geom_point(data=beta.min, aes(beta,RMSE), size=2, color="blue")

```

Now let's refit our model with this optimal $\beta$ value and compare our predictive accuracy to our original model. We see that our new model reduces our error rate (MAPE) down to 1.78%.

```{r}
# new model with optimal beta
holt.goog.opt <- holt(goog.train, 
                      h = 100,
                      beta = 0.0601)

# accuracy of first model
accuracy(holt.goog, goog.test)
##                        ME      RMSE       MAE         MPE     MAPE
## Training set -0.003922069  8.938319  5.974921 -0.01188414 1.004290
## Test set     -8.945342029 21.099897 16.268344 -1.15371296 2.039945
##                   MASE       ACF1 Theil's U
## Training set 0.9997514 0.03746482        NA
## Test set     2.7220944 0.89540070  2.481272

# accuracy of new optimal model
accuracy(holt.goog.opt, goog.test)
##                       ME      RMSE       MAE         MPE     MAPE     MASE
## Training set -0.01098347  9.109332  6.218278 -0.00502392 1.043517 1.040471
## Test set     -0.33180592 18.536622 14.287508 -0.09452438 1.780052 2.390652
##                    ACF1 Theil's U
## Training set 0.01293146        NA
## Test set     0.88797970  2.156113
```

If we plot our original versus more recent optimal model we will notice a couple things. 

- First, our predicted values for the optimal model are more conservative; in other words, they are assuming a more gradual slope.
- Second, the confidence intervals are much more extreme. So although our predictions were more accuracte, our uncertainty increases. 

The reasons for this is that by increasing our $/beta$ value we are assuming faster learning from more recent observations. And since there some quite a bit of turbulence in the recent time period, this is causing greater variance to be incorporated into our prediction intervals. This requires a more indepth discussion than this tutorial will go into, but the important thing to keep in mind is that although we increase our prediction accuracy with parameter tuning, there are additional side effects that can occur, which may be harder to explain to decision-makers.

```{r}

p1 <- autoplot(holt.goog)+
  ggtitle("Original Holt's Model")+
  coord_cartesian(ylim = c(400, 1000))

p2 <- autoplot(holt.goog.opt)+
  ggtitle("Optimal Holt's Model")+
  coord_cartesian(ylim = c(400, 1000))

gridExtra::grid.arrange(p1, p2, nrow=1)

```

### Holt-Winters Seasonal Method {#ES_Holt-Winters}

To make predictions using data with a trend and seasonality, we turn to the Holt Winters Seasonal Method. This method can be implemented with an "Additive" structure or "Multiplicative" structure, where the choice of method depends on the data set. The Additive model is best used when the seasonal trend is of the same magnitude throughout the data set, while the multiplicative model is preferred when the magnitude of seasonality changes as time increase. 

Since the Google data does not have seasonality, weÅfll use the `qcement` data that we set up in the Replication section to demonstrate. This data has seasonality and trend; however, it is unclear if seasonality is additive or multiplicative. WeÅfll use the Holt-Winters method to identify the best fit model.

```{r}
autoplot(decompose(qcement))
```

#### Additive 
For the additive model, the regular equation form is:

$$
\hat{y}_{t+1} = L_t + kT_k + S_{t+k-m}
$$

The level ($l_t$), trend ($T_t$) and season ($S_t$) are updated through a pair of updating equations, which is where you see the presence of the three smoothing parameters:

$$
L_t = \alpha(y_t - S_{t-m} + (1-\alpha)(L_{t-1}+T_{t-1})) \\
T_t = \beta(L_t - L_{t-1}) + (1-\beta)T_{t-1} \\
S_t = \gamma(y_t - L_t) + (1-\gamma)S_{t-m}
\\
$$

where $\alpha, \beta and \gamma$ are the three smoothing parameters to deal with the level pattern, the trend, and the seasonality, respectively. Similar to SES and Holt's method, all three parameters are constrained to 0-1. The component equations are as follows. 

$$
\hat{y}_{t+1} = l_t + ht_k + s_{t+k-m} \\
l_t = \alpha(y_t - s_{t-m} + (1-\alpha)(l_{t-1}+b_{t-1})) \\
b_t = \beta(l_t - l_{t-1}) + (1-\beta)b_{t-1} \\
s_t = \gamma(y_t - l_t) + (1-\gamma)s_{t-m}
\\
$$

To apply the Holt-Winters method weÅfll introduce a new function, `ets` which stands for error, trend, and seasonality. The important thing to understand about the `ets` model is how to select the `model =` parameter. In total you have 36 model options to choose from. The parameter settings in the below code (`model = "AAA"`) stands for a model with additive error, additive trend, and additve seasonality.

```{r}
qcement.hw <- ets(qcement.train, model="AAA")
autoplot(forecast(qcement.hw))
```

So when specificying the model type you always specificy the error, trend, then seasonality (hence ÅgetsÅh). The options you can specify for each component is as follows:

- error: additive (ÅgAÅh), multiplicative (ÅgMÅh), unknown (ÅgZÅh)
- trend: none (ÅgNÅh), additive (ÅgAÅh), multiplicative (ÅgMÅh), unknown (ÅgZÅh)
- seasonality: none (ÅgNÅh), additive (ÅgAÅh), multiplicative (ÅgMÅh), unknown (ÅgZÅh)

Consequently, if you wanted to apply a HoltÅfs model where the error and trend were additive and no seasonality exists you would select `model = "AAN"`. If you want to apply a Holt-Winters model where there is additive error, an exponential (multiplicative) trend, and additive seasonality you would select model = "AMA". If you are uncertain of the type of component then you use ÅgZÅh. So if you were uncertain of the components or if you want the model to select the best option, you could use model = "ZZZ" and the ÅgoptimalÅh model will be selected.

If we assess our additive model we can see that $\alpha=0.6208$, $/beta=0.0001$ and $\gamma=0.1913$.

```{r}
summary(qcement.hw)
```

If we check our residuals, we see that residuals grow larger over time. This may suggest that a multiplicative error rate may be more appropriate.

```{r}
checkresiduals(qcement.hw)
```

If we check the predictive accuracy we see that our prediction accuracy is about 2.9% (according to the MAPE).

```{r}
#forecast the next 5 quarter
qcement.f1 <- forecast(qcement.hw, h = 5)
autoplot(qcement.f1)

# check accuracy
accuracy(qcement.f1, qcement.test)
```

#### Multiplicative

As previously stated, we may have multiplicative features for our Holt-Winters method. If we have multiplicative seasonality, then our equation from changes to:

$$
\hat{y}_{t+1} = (L_t + kT_k) S_{t+k-m}
$$

The level ($l_t$), trend ($T_t$) and season ($S_t$) are updated through a pair of updating equations, which is where you see the presence of the three smoothing parameters:

$$
L_t = \alpha y_t / S_{t-m} + (1-\alpha)(L_{t-1}+T_{t-1})) \\
T_t = \beta(L_t - L_{t-1}) + (1-\beta)T_{t-1} \\
S_t = \gamma(y_t / L_t) + (1-\gamma)S_{t-m}
\\
$$

If we apply a multiplicative seasonality `model` then our model parameter becomes `model = "MAM"` (here, we are actually applying a multiplicative error and seasonality model). We see that are residuals illustrate less change in magnitude over time. We still have an issue with autocorrelation with errors but weÅfll address that in later tutorials.

```{r}
qcement.hw2 <- ets(qcement.train, model = "MAM")
checkresiduals(qcement.hw2)
```

To compare the predictive accuracy of our models letÅfs compare four different models. We see that the first model (additive error, trend and seasonality) results in the lowest RMSE and MAPE on our test data set.

```{r}
# additive error, trend and seasonality
qcement.hw1 <- ets(qcement.train, model = "AAA")
qcement.f1 <- forecast(qcement.hw1, h = 5)
accuracy(qcement.f1, qcement.test)

# multiplicative error, additive trend and seasonality
qcement.hw2 <- ets(qcement.train, model = "MAA")
qcement.f2 <- forecast(qcement.hw2, h=5)
accuracy(qcement.f2, qcement.test)

# additive error and trend and multiplicative seasonality
qcement.hw3 <- ets(qcement.train, model = "AAM", restrict = FALSE)
qcement.f3 <- forecast(qcement.hw3, h = 5)
accuracy(qcement.f3, qcement.test)
##                       ME       RMSE       MAE       MPE     MAPE      MASE
## Training set 0.003403387 0.07762833 0.0558983 0.1487162 3.693408 0.5530085
## Test set     0.027648352 0.08382395 0.0777908 0.8568631 3.304030 0.7695936
##                     ACF1 Theil's U
## Training set -0.02009013        NA
## Test set     -0.06590551 0.2161475

# multiplicative error, additive trend, and multiplicative seasonality
qcement.hw4 <- ets(qcement.train, model = "MAM")
qcement.f4 <- forecast(qcement.hw4, h = 5)
accuracy(qcement.f4, qcement.test)
```

If we were to compare this to an unspecified model where we let `ets` select the optimal model, we see that ets selects a model specification of multiplicative error, additive trend, and multiplicative seasonality (ÅgMAMÅh). This is equivalent to our fourth model above. This model is assumed ÅgoptimalÅh because it minimizes RMSE, AIC, and BIC on the training data set, but does not necessarily minimize prediction errors on the test set.

```{r}
qcement.hw5 <- ets(qcement.train, model = "ZZZ")
summary(qcement.hw5)
```

As we did in the SES and HoltÅfs method section, we can optimize the $\gamma$  parameter in our Holt-Winters model. Here, we use the additive error, trend and seasonality model that minimized our prediction errors above and identify the $\gamma$ parameter that minimizes forecast errors. In this case we see that $\gamma=0.21$  minimizes the error rate.

```{r}
gamma <- seq(0.01, 0.85, 0.01)
RMSE <- NA

for (i in seq_along(gamma)){
  hw.expo <- ets(qcement.train, "AAA", gamma = gamma[i])
  future <- forecast(hw.expo, h = 5)
  RMSE[i] = accuracy(future, qcement.test)[2,2]
}

error <- data_frame(gamma, RMSE)
minimum <- filter(error, RMSE == min(RMSE))
ggplot(error, aes(gamma, RMSE)) +
  geom_line() +
  geom_point(data = minimum, color = "blue", size = 2) +
  ggtitle("gamma's impact on forecast errors",
          subtitle = "gamma = 0.21 minimizes RMSE")
```

If we update our model with this ÅgoptimalÅh $\gamma$ parameter we see that we bring our forecasting error rate down from 2.88% to 2.76%. This is a small improvement, but often small improvements can have large business implications.

```{r}
# previous model with additive error, trend and seasonality
accuracy(qcement.f1, qcement.test)
##                         ME       RMSE        MAE        MPE     MAPE
## Training set -0.0004297096 0.08375033 0.05912561 -0.1834617 3.830243
## Test set      0.0307206913 0.07102359 0.06762179  1.0818090 2.882741
##                   MASE        ACF1 Theil's U
## Training set 0.5849367  0.04576105        NA
## Test set     0.6689904 -0.30067586 0.2084824

# new model with optimal gamma parameter
qcement.hw6 <- ets(qcement.train, model = "AAA", gamma = 0.21)
qcement.f6 <- forecast(qcement.hw6, h = 5)
accuracy(qcement.f6, qcement.test)
##                        ME       RMSE        MAE       MPE     MAPE
## Training set -0.001206215 0.08425184 0.05975069 -0.154495 3.974104
## Test set      0.029358296 0.06911376 0.06466821  1.038366 2.761642
##                   MASE        ACF1 Theil's U
## Training set 0.5911206  0.03804556        NA
## Test set     0.6397703 -0.36727559 0.2081361
```

With this new optimal model we can get our predicted values:
```{r}
qcement.f6
```

and also visualize these predicted values:

```{r}
autoplot(qcement.f6)
```

### Damping methods {#ES_Dampled_Trend}

One last item to discuss is the idea of "damping" your forecast. Damped forecasts use a damping coefficients denoted $\phi$ to more conservatively estimate the predicted trend. 

Basically, if you belive that your additive or multiplicative trend is or will be slowing down ("flat-lining) in the near future then you are assuming it will dampen.

The equation form for an additive model with a damping coefficient is 

$$
\hat{h}_{t+h} = L_t + ( \phi+\phi^2+...+\phi^h)\beta_t \\
+_t = \alpha y_t + \alpha(1-\alpha)(L_{t^1}+\phi\beta_{t-1}), \\
\beta_t = \beta(L_t - L_{t-1}) + (1-\beta)\phi \beta_{t-1} 
$$

where, $0<\phi<1$. When $\phi=1$ the method is the same as Holt's additive model. As $\phi$ gets closer to 0, the trend becomes more conservative and flat-lines to a constant in the nearer future. The end result of this method is flat that short-run forecasts are still trendeded while the long-run forecasts are constant. 

To illustrate the effect of damped forecast we will use the `fpp2::ausair` data set. Here, we create several models (additive, additive+damped, multiplicative, multiplicative+damped). In the plot you can see that the dampled models (dashped lines) have more conservative trend lines and if we forecasted these far enough into the future we would see this trend flat-line.

```{r}
# holt's linear (additive) model
fit1 <- ets(ausair, model = "ZAN", alpha = 0.8, beta = 0.2)
pred1 <- forecast(fit1, h = 5)

# holt's linear (additive) model
fit2 <- ets(ausair, model = "ZAN", damped = TRUE, 
            alpha = 0.8, beta = 0.2)
pred2 <- forecast(fit2, h = 5)

# holt's exponential (multiplicative) model
fit3 <- ets(ausair, model = "ZMN" , alpha = 0.8, beta = 0.2)
pred3 <- forecast(fit3, h = 5)

# holt's exponential multiplicative model damped
fit4 <- ets(ausair, model = "ZMN", damped  = TRUE, alpha = 0.8,
            beta = 0.2)
pred4 <- forecast(fit4, h = 5)

autoplot(ausair)+
  autolayer(pred1$mean, color = "blue")+
  autolayer(pred2$mean, color = "blue", linetype = "dashed")+
  autolayer(pred3$mean, color = "red")+
  autolayer(pred4$mean, color = "red", linetype = "dashed")

```

The above models were for illustrative purposes only. You would apply the same process as you saw in earlier sections to identify if a damped model predicts more accurately than a non-dampped model. You can even apply the approaches you saw earlier for tuning this parameter to identify the optimal $\phi$ coefficient.




























